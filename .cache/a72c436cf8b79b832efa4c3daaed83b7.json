{"dependencies":[{"name":"/Users/chris/Desktop/因特尔杯/tfjs-models/posenet/demos/.babelrc","includedInParent":true,"mtime":1539397829000},{"name":"/Users/chris/Desktop/因特尔杯/tfjs-models/posenet/demos/package.json","includedInParent":true,"mtime":1539397829000},{"name":"babel-runtime/core-js/promise"},{"name":"@tensorflow-models/posenet","loc":{"line":17,"column":25}},{"name":"@tensorflow/tfjs","loc":{"line":18,"column":20}},{"name":"dat.gui","loc":{"line":19,"column":16}},{"name":"./demo_util","loc":{"line":29,"column":7}}],"generated":{"js":"'use strict';\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\n\nvar _promise = require('babel-runtime/core-js/promise');\n\nvar _promise2 = _interopRequireDefault(_promise);\n\nexports.bindPage = bindPage;\n\nvar _posenet = require('@tensorflow-models/posenet');\n\nvar posenet = _interopRequireWildcard(_posenet);\n\nvar _tfjs = require('@tensorflow/tfjs');\n\nvar tf = _interopRequireWildcard(_tfjs);\n\nvar _dat = require('dat.gui');\n\nvar _dat2 = _interopRequireDefault(_dat);\n\nvar _demo_util = require('./demo_util');\n\nfunction _interopRequireWildcard(obj) { if (obj && obj.__esModule) { return obj; } else { var newObj = {}; if (obj != null) { for (var key in obj) { if (Object.prototype.hasOwnProperty.call(obj, key)) newObj[key] = obj[key]; } } newObj.default = obj; return newObj; } }\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\n// clang-format on\n\n\n/**\n * @license\n * Copyright 2018 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nconst images = ['frisbee.jpg', 'frisbee_2.jpg', 'backpackman.jpg', 'boy_doughnut.jpg', 'soccer.png', 'with_computer.jpg', 'snowboard.jpg', 'person_bench.jpg', 'skiing.jpg', 'fire_hydrant.jpg', 'kyte.jpg', 'looking_at_computer.jpg', 'tennis.jpg', 'tennis_standing.jpg', 'truck.jpg', 'on_bus.jpg', 'tie_with_beer.jpg', 'baseball.jpg', 'multi_skiing.jpg', 'riding_elephant.jpg', 'skate_park_venice.jpg', 'skate_park.jpg', 'tennis_in_crowd.jpg', 'two_on_bench.jpg'];\n\n// clang-format off\n\n\nconst { partIds, poseChain } = posenet;\n\n/**\n * Draws a pose if it passes a minimum confidence onto a canvas.\n * Only the pose's keypoints that pass a minPartConfidence are drawn.\n */\nfunction drawResults(canvas, poses, minPartConfidence, minPoseConfidence) {\n  (0, _demo_util.renderImageToCanvas)(image, [513, 513], canvas);\n  poses.forEach(pose => {\n    if (pose.score >= minPoseConfidence) {\n      if (guiState.showKeypoints) {\n        (0, _demo_util.drawKeypoints)(pose.keypoints, minPartConfidence, canvas.getContext('2d'));\n      }\n\n      if (guiState.showSkeleton) {\n        (0, _demo_util.drawSkeleton)(pose.keypoints, minPartConfidence, canvas.getContext('2d'));\n      }\n\n      if (guiState.showBoundingBox) {\n        (0, _demo_util.drawBoundingBox)(pose.keypoints, canvas.getContext('2d'));\n      }\n    }\n  });\n}\n\nconst imageBucket = 'https://storage.googleapis.com/tfjs-models/assets/posenet/';\n\nasync function loadImage(imagePath) {\n  const image = new Image();\n  const promise = new _promise2.default((resolve, reject) => {\n    image.crossOrigin = '';\n    image.onload = () => {\n      resolve(image);\n    };\n  });\n\n  image.src = `${imageBucket}${imagePath}`;\n  return promise;\n}\n\nfunction singlePersonCanvas() {\n  return document.querySelector('#single canvas');\n}\n\nfunction multiPersonCanvas() {\n  return document.querySelector('#multi canvas');\n}\n\n/**\n * Draw the results from the single-pose estimation on to a canvas\n */\nfunction drawSinglePoseResults(pose) {\n  const canvas = singlePersonCanvas();\n  drawResults(canvas, [pose], guiState.singlePoseDetection.minPartConfidence, guiState.singlePoseDetection.minPoseConfidence);\n\n  const { part, showHeatmap, showOffsets } = guiState.visualizeOutputs;\n  // displacements not used for single pose decoding\n  const showDisplacements = false;\n  const partId = +part;\n\n  visualizeOutputs(partId, showHeatmap, showOffsets, showDisplacements, canvas.getContext('2d'));\n}\n\n/**\n * Draw the results from the multi-pose estimation on to a canvas\n */\nfunction drawMultiplePosesResults(poses) {\n  const canvas = multiPersonCanvas();\n  drawResults(canvas, poses, guiState.multiPoseDetection.minPartConfidence, guiState.multiPoseDetection.minPoseConfidence);\n\n  const { part, showHeatmap, showOffsets, showDisplacements } = guiState.visualizeOutputs;\n  const partId = +part;\n\n  visualizeOutputs(partId, showHeatmap, showOffsets, showDisplacements, canvas.getContext('2d'));\n}\n\n/**\n * Define the skeleton by part id. This is used in multi-pose estimation. This\n *defines the parent->child relationships of our tree. Arbitrarily this defines\n *the nose as the root of the tree.\n **/\nconst parentChildrenTuples = poseChain.map(([parentJoinName, childJoinName]) => [partIds[parentJoinName], partIds[childJoinName]]);\n\n/**\n * Parent to child edges from the skeleton indexed by part id.  Indexes the edge\n * ids by the part ids.\n */\nconst parentToChildEdges = parentChildrenTuples.reduce((result, [partId], i) => {\n  if (result[partId]) {\n    result[partId] = [...result[partId], i];\n  } else {\n    result[partId] = [i];\n  }\n\n  return result;\n}, {});\n\n/**\n * Child to parent edges from the skeleton indexed by part id.  Indexes the edge\n * ids by the part ids.\n */\nconst childToParentEdges = parentChildrenTuples.reduce((result, [, partId], i) => {\n  if (result[partId]) {\n    result[partId] = [...result[partId], i];\n  } else {\n    result[partId] = [i];\n  }\n\n  return result;\n}, {});\n\nfunction drawOffsetVector(ctx, y, x, outputStride, offsetsVectorY, offsetsVectorX) {\n  (0, _demo_util.drawSegment)([y * outputStride, x * outputStride], [y * outputStride + offsetsVectorY, x * outputStride + offsetsVectorX], 'red', 1., ctx);\n}\n\nfunction drawDisplacementEdgesFrom(ctx, partId, displacements, outputStride, edges, y, x, offsetsVectorY, offsetsVectorX) {\n  const numEdges = displacements.shape[2] / 2;\n\n  const offsetX = x * outputStride + offsetsVectorX;\n  const offsetY = y * outputStride + offsetsVectorY;\n\n  const edgeIds = edges[partId] || [];\n\n  if (edgeIds.length > 0) {\n    edgeIds.forEach(edgeId => {\n      const displacementY = displacements.get(y, x, edgeId);\n      const displacementX = displacements.get(y, x, edgeId + numEdges);\n\n      (0, _demo_util.drawSegment)([offsetY, offsetX], [offsetY + displacementY, offsetX + displacementX], 'blue', 1., ctx);\n    });\n  }\n}\n\n/**\n * Visualizes the outputs from the model which are used for decoding poses.\n * Limited to visualizing the outputs for a single part.\n *\n * @param partId The id of the part to visualize\n *\n */\nfunction visualizeOutputs(partId, drawHeatmaps, drawOffsetVectors, drawDisplacements, ctx) {\n  const { heatmapScores, offsets, displacementFwd, displacementBwd } = modelOutputs;\n  const outputStride = +guiState.outputStride;\n\n  const [height, width] = heatmapScores.shape;\n\n  ctx.globalAlpha = 0;\n  for (let y = 0; y < height; y++) {\n    for (let x = 0; x < width; x++) {\n      const score = heatmapScores.get(y, x, partId);\n\n      // to save on performance, don't draw anything with a low score.\n      if (score < 0.05) continue;\n\n      // set opacity of drawn elements based on the score\n      ctx.globalAlpha = score;\n\n      if (drawHeatmaps) {\n        (0, _demo_util.drawPoint)(ctx, y * outputStride, x * outputStride, 2, 'yellow');\n      }\n\n      const offsetsVectorY = offsets.get(y, x, partId);\n      const offsetsVectorX = offsets.get(y, x, partId + 17);\n\n      if (drawOffsetVectors) {\n        drawOffsetVector(ctx, y, x, outputStride, offsetsVectorY, offsetsVectorX);\n      }\n\n      if (drawDisplacements) {\n        // exponentially affect the alpha of the displacements;\n        ctx.globalAlpha *= score;\n\n        drawDisplacementEdgesFrom(ctx, partId, displacementFwd, outputStride, parentToChildEdges, y, x, offsetsVectorY, offsetsVectorX);\n\n        drawDisplacementEdgesFrom(ctx, partId, displacementBwd, outputStride, childToParentEdges, y, x, offsetsVectorY, offsetsVectorX);\n      }\n    }\n\n    ctx.globalAlpha = 1;\n  }\n}\n\n/**\n * Converts the raw model output results into single-pose estimation results\n */\nasync function decodeSinglePoseAndDrawResults() {\n  if (!modelOutputs) {\n    return;\n  }\n\n  const pose = await posenet.decodeSinglePose(modelOutputs.heatmapScores, modelOutputs.offsets, guiState.outputStride);\n\n  drawSinglePoseResults(pose);\n}\n\n/**\n * Converts the raw model output results into multi-pose estimation results\n */\nasync function decodeMultiplePosesAndDrawResults() {\n  if (!modelOutputs) {\n    return;\n  }\n\n  const poses = await posenet.decodeMultiplePoses(modelOutputs.heatmapScores, modelOutputs.offsets, modelOutputs.displacementFwd, modelOutputs.displacementBwd, guiState.outputStride, guiState.multiPoseDetection.maxDetections, guiState.multiPoseDetection);\n\n  drawMultiplePosesResults(poses);\n}\n\nfunction decodeSingleAndMultiplePoses() {\n  decodeSinglePoseAndDrawResults();\n  decodeMultiplePosesAndDrawResults();\n}\n\nfunction setStatusText(text) {\n  const resultElement = document.getElementById('status');\n  resultElement.innerText = text;\n}\n\nlet image = null;\nlet modelOutputs = null;\n\n/**\n * Purges variables and frees up GPU memory using dispose() method\n */\nfunction disposeModelOutputs() {\n  if (modelOutputs) {\n    modelOutputs.heatmapScores.dispose();\n    modelOutputs.offsets.dispose();\n    modelOutputs.displacementFwd.dispose();\n    modelOutputs.displacementBwd.dispose();\n  }\n}\n\n/**\n * Loads an image, feeds it into posenet the posenet model, and\n * calculates poses based on the model outputs\n */\nasync function testImageAndEstimatePoses(net) {\n  setStatusText('Predicting...');\n  document.getElementById('results').style.display = 'none';\n\n  // Purge prevoius variables and free up GPU memory\n  disposeModelOutputs();\n\n  // Load an example image\n  image = await loadImage(guiState.image);\n\n  // Creates a tensor from an image\n  const input = tf.fromPixels(image);\n\n  // Stores the raw model outputs from both single- and multi-pose results can\n  // be decoded.\n  // Normally you would call estimateSinglePose or estimateMultiplePoses,\n  // but by calling this method we can previous the outputs of the model and\n  // visualize them.\n  modelOutputs = await net.predictForMultiPose(input, guiState.outputStride);\n\n  // Process the model outputs to convert into poses\n  await decodeSingleAndMultiplePoses();\n\n  setStatusText('');\n  document.getElementById('results').style.display = 'block';\n  input.dispose();\n}\n\nlet guiState;\n\nfunction setupGui(net) {\n  guiState = {\n    outputStride: 16,\n    image: 'tennis_in_crowd.jpg',\n    detectPoseButton: () => {\n      testImageAndEstimatePoses(net);\n    },\n    singlePoseDetection: {\n      minPartConfidence: 0.5,\n      minPoseConfidence: 0.5\n    },\n    multiPoseDetection: {\n      minPartConfidence: 0.5,\n      minPoseConfidence: 0.5,\n      scoreThreshold: 0.5,\n      nmsRadius: 20.0,\n      maxDetections: 15\n    },\n    showKeypoints: true,\n    showSkeleton: true,\n    showBoundingBox: false,\n    visualizeOutputs: {\n      part: 0,\n      showHeatmap: false,\n      showOffsets: false,\n      showDisplacements: false\n    }\n  };\n\n  const gui = new _dat2.default.GUI();\n  // Output stride:  Internally, this parameter affects the height and width of\n  // the layers in the neural network. The lower the value of the output stride\n  // the higher the accuracy but slower the speed, the higher the value the\n  // faster the speed but lower the accuracy.\n  gui.add(guiState, 'outputStride', [8, 16, 32]).onChange(outputStride => {\n    guiState.outputStride = +outputStride;\n    testImageAndEstimatePoses(net);\n  });\n  gui.add(guiState, 'image', images).onChange(() => testImageAndEstimatePoses(net));\n\n  // Pose confidence: the overall confidence in the estimation of a person's\n  // pose (i.e. a person detected in a frame)\n  // Min part confidence: the confidence that a particular estimated keypoint\n  // position is accurate (i.e. the elbow's position)\n\n  const multiPoseDetection = gui.addFolder('Multi Pose Estimation');\n  multiPoseDetection.open();\n  multiPoseDetection.add(guiState.multiPoseDetection, 'minPartConfidence', 0.0, 1.0).onChange(decodeMultiplePosesAndDrawResults);\n  multiPoseDetection.add(guiState.multiPoseDetection, 'minPoseConfidence', 0.0, 1.0).onChange(decodeMultiplePosesAndDrawResults);\n\n  // nms Radius: controls the minimum distance between poses that are returned\n  // defaults to 20, which is probably fine for most use cases\n  multiPoseDetection.add(guiState.multiPoseDetection, 'nmsRadius', 0.0, 40.0).onChange(decodeMultiplePosesAndDrawResults);\n  multiPoseDetection.add(guiState.multiPoseDetection, 'maxDetections').min(1).max(20).step(1).onChange(decodeMultiplePosesAndDrawResults);\n\n  const singlePoseDetection = gui.addFolder('Single Pose Estimation');\n  singlePoseDetection.add(guiState.singlePoseDetection, 'minPartConfidence', 0.0, 1.0).onChange(decodeSinglePoseAndDrawResults);\n  singlePoseDetection.add(guiState.singlePoseDetection, 'minPoseConfidence', 0.0, 1.0).onChange(decodeSinglePoseAndDrawResults);\n  singlePoseDetection.open();\n\n  gui.add(guiState, 'showKeypoints').onChange(decodeSingleAndMultiplePoses);\n  gui.add(guiState, 'showSkeleton').onChange(decodeSingleAndMultiplePoses);\n  gui.add(guiState, 'showBoundingBox').onChange(decodeSingleAndMultiplePoses);\n\n  const visualizeOutputs = gui.addFolder('Visualize Outputs');\n\n  visualizeOutputs.add(guiState.visualizeOutputs, 'part', posenet.partIds).onChange(decodeSingleAndMultiplePoses);\n  visualizeOutputs.add(guiState.visualizeOutputs, 'showHeatmap').onChange(decodeSingleAndMultiplePoses);\n  visualizeOutputs.add(guiState.visualizeOutputs, 'showOffsets').onChange(decodeSingleAndMultiplePoses);\n  visualizeOutputs.add(guiState.visualizeOutputs, 'showDisplacements').onChange(decodeSingleAndMultiplePoses);\n\n  visualizeOutputs.open();\n}\n\n/**\n * Kicks off the demo by loading the posenet model and estimating\n * poses on a default image\n */\nasync function bindPage() {\n  const net = await posenet.load();\n\n  setupGui(net);\n\n  await testImageAndEstimatePoses(net);\n  document.getElementById('loading').style.display = 'none';\n  document.getElementById('main').style.display = 'block';\n}\n\nbindPage();"},"hash":"16e565a449f2209a2dc6ec9713e69edc","cacheData":{"env":{}}}