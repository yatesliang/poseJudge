{"dependencies":[{"name":"/Users/chris/Desktop/因特尔杯/TryIt/package.json","includedInParent":true,"mtime":1539397829000},{"name":"/Users/chris/Desktop/因特尔杯/TryIt/.babelrc","includedInParent":true,"mtime":1539397829000},{"name":"/Users/chris/Desktop/因特尔杯/TryIt/node_modules/@tensorflow/tfjs-layers/package.json","includedInParent":true,"mtime":1541754030179},{"name":"@tensorflow/tfjs-core","loc":{"line":17,"column":691}}],"generated":{"js":"\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.version_layers = exports.LayerVariable = exports.Sequential = exports.RNN = exports.sequential = exports.registerCallbackConstructor = exports.model = exports.loadModel = exports.input = exports.Model = exports.SymbolicTensor = exports.Callback = exports.History = exports.CustomCallback = exports.CallbackList = exports.regularizers = exports.metrics = exports.layers = exports.initializers = exports.constraints = undefined;\n\nvar _tfjsCore = require(\"@tensorflow/tfjs-core\");\n\nvar extendStatics = Object.setPrototypeOf || { __proto__: [] } instanceof Array && function (e, t) {\n  e.__proto__ = t;\n} || function (e, t) {\n  for (var n in t) t.hasOwnProperty(n) && (e[n] = t[n]);\n}; /**\n    * @license\n    * Copyright 2018 Google LLC. All Rights Reserved.\n    * Licensed under the Apache License, Version 2.0 (the \"License\");\n    * you may not use this file except in compliance with the License.\n    * You may obtain a copy of the License at\n    *\n    * http://www.apache.org/licenses/LICENSE-2.0\n    *\n    * Unless required by applicable law or agreed to in writing, software\n    * distributed under the License is distributed on an \"AS IS\" BASIS,\n    * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    * See the License for the specific language governing permissions and\n    * limitations under the License.\n    * =============================================================================\n    */\nfunction __extends(e, t) {\n  function n() {\n    this.constructor = e;\n  }extendStatics(e, t), e.prototype = null === t ? Object.create(t) : (n.prototype = t.prototype, new n());\n}var __assign = Object.assign || function (e) {\n  for (var t, n = 1, r = arguments.length; n < r; n++) for (var i in t = arguments[n]) Object.prototype.hasOwnProperty.call(t, i) && (e[i] = t[i]);return e;\n};function __awaiter(e, t, n, r) {\n  return new (n || (n = Promise))(function (i, a) {\n    function o(e) {\n      try {\n        l(r.next(e));\n      } catch (e) {\n        a(e);\n      }\n    }function s(e) {\n      try {\n        l(r.throw(e));\n      } catch (e) {\n        a(e);\n      }\n    }function l(e) {\n      e.done ? i(e.value) : new n(function (t) {\n        t(e.value);\n      }).then(o, s);\n    }l((r = r.apply(e, t || [])).next());\n  });\n}function __generator(e, t) {\n  var n,\n      r,\n      i,\n      a,\n      o = { label: 0, sent: function () {\n      if (1 & i[0]) throw i[1];return i[1];\n    }, trys: [], ops: [] };return a = { next: s(0), throw: s(1), return: s(2) }, \"function\" == typeof Symbol && (a[Symbol.iterator] = function () {\n    return this;\n  }), a;function s(a) {\n    return function (s) {\n      return function (a) {\n        if (n) throw new TypeError(\"Generator is already executing.\");for (; o;) try {\n          if (n = 1, r && (i = r[2 & a[0] ? \"return\" : a[0] ? \"throw\" : \"next\"]) && !(i = i.call(r, a[1])).done) return i;switch (r = 0, i && (a = [0, i.value]), a[0]) {case 0:case 1:\n              i = a;break;case 4:\n              return o.label++, { value: a[1], done: !1 };case 5:\n              o.label++, r = a[1], a = [0];continue;case 7:\n              a = o.ops.pop(), o.trys.pop();continue;default:\n              if (!(i = (i = o.trys).length > 0 && i[i.length - 1]) && (6 === a[0] || 2 === a[0])) {\n                o = 0;continue;\n              }if (3 === a[0] && (!i || a[1] > i[0] && a[1] < i[3])) {\n                o.label = a[1];break;\n              }if (6 === a[0] && o.label < i[1]) {\n                o.label = i[1], i = a;break;\n              }if (i && o.label < i[2]) {\n                o.label = i[2], o.ops.push(a);break;\n              }i[2] && o.ops.pop(), o.trys.pop();continue;}a = t.call(e, o);\n        } catch (e) {\n          a = [6, e], r = 0;\n        } finally {\n          n = i = 0;\n        }if (5 & a[0]) throw a[1];return { value: a[0] ? a[1] : void 0, done: !0 };\n      }([a, s]);\n    };\n  }\n}var _epsilon = _tfjsCore.ENV.get(\"EPSILON\");function epsilon() {\n  return _epsilon;\n}function imageDataFormat() {\n  return \"channelsLast\";\n}var _nextUniqueTensorId = 0;function getNextUniqueTensorId() {\n  return _nextUniqueTensorId++;\n}var _uidPrefixes = {};function getUid(e) {\n  return void 0 === e && (e = \"\"), e in _uidPrefixes || (_uidPrefixes[e] = 0), _uidPrefixes[e] += 1, e + _uidPrefixes[e].toString();\n}var scalarCache = { float32: {}, int32: {} },\n    DEFAULT_DTYPE = \"float32\";function getScalar(e, t) {\n  return void 0 === t && (t = DEFAULT_DTYPE), null == scalarCache[t][e] && (scalarCache[t][e] = (0, _tfjsCore.scalar)(e, t), (0, _tfjsCore.keep)(scalarCache[t][e])), scalarCache[t][e];\n}var AttributeError = function (e) {\n  function t(n) {\n    var r = e.call(this, n) || this;return Object.setPrototypeOf(r, t.prototype), r;\n  }return __extends(t, e), t;\n}(Error),\n    RuntimeError = function (e) {\n  function t(n) {\n    var r = e.call(this, n) || this;return Object.setPrototypeOf(r, t.prototype), r;\n  }return __extends(t, e), t;\n}(Error),\n    ValueError = function (e) {\n  function t(n) {\n    var r = e.call(this, n) || this;return Object.setPrototypeOf(r, t.prototype), r;\n  }return __extends(t, e), t;\n}(Error),\n    NotImplementedError = function (e) {\n  function t(n) {\n    var r = e.call(this, n) || this;return Object.setPrototypeOf(r, t.prototype), r;\n  }return __extends(t, e), t;\n}(Error),\n    AssertionError = function (e) {\n  function t(n) {\n    var r = e.call(this, n) || this;return Object.setPrototypeOf(r, t.prototype), r;\n  }return __extends(t, e), t;\n}(Error),\n    IndexError = function (e) {\n  function t(n) {\n    var r = e.call(this, n) || this;return Object.setPrototypeOf(r, t.prototype), r;\n  }return __extends(t, e), t;\n}(Error);function pyListRepeat(e, t) {\n  if (Array.isArray(e)) {\n    for (var n = [], r = 0; r < t; r++) n = n.concat(e);return n;\n  }return (n = new Array(t)).fill(e), n;\n}function assert(e, t) {\n  if (!e) throw new AssertionError(t);\n}function count(e, t) {\n  for (var n = 0, r = 0, i = e; r < i.length; r++) {\n    i[r] === t && n++;\n  }return n;\n}function singletonOrArray(e) {\n  return 1 === e.length ? e[0] : e;\n}function toList(e) {\n  return Array.isArray(e) ? e : [e];\n}function toSnakeCase(e) {\n  var t = e.replace(/(.)([A-Z][a-z0-9]+)/g, \"$1_$2\").replace(/([a-z])([A-Z])/g, \"$1_$2\").toLowerCase();return \"_\" !== t[0] ? t : \"private\" + t;\n}function toCamelCase(e) {\n  return e.length <= 1 ? e : -1 === e.indexOf(\"_\") ? e : e.replace(/[_]+(\\w|$)/g, function (e, t) {\n    return t.toUpperCase();\n  });\n}var _GLOBAL_CUSTOM_OBJECTS = {};function serializeKerasObject(e) {\n  return null === e || void 0 === e ? null : { className: e.getClassName(), config: e.getConfig() };\n}function deserializeKerasObject(e, t, n, r) {\n  if (void 0 === t && (t = {}), void 0 === n && (n = {}), void 0 === r && (r = \"object\"), \"string\" == typeof e) {\n    var i = e,\n        a = void 0;if (i in n) a = n[i];else if (i in _GLOBAL_CUSTOM_OBJECTS) a = _GLOBAL_CUSTOM_OBJECTS[i];else if (null == (a = t[i])) throw new ValueError(\"Unknown \" + r + \": \" + e + \". This may be due to one of the following reasons:\\n1. The \" + r + \" is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.\\n2. The custom \" + r + \" is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().\");return a;\n  }var o = e;if (null == o.className || null == o.config) throw new ValueError(r + \": Improper config format: \" + JSON.stringify(o) + \".\\n'className' and 'config' must set.\");var s = o.className,\n      l = void 0,\n      u = void 0;if (s in n ? (l = (S = n.get(s))[0], u = S[1]) : s in _GLOBAL_CUSTOM_OBJECTS ? (l = (A = _GLOBAL_CUSTOM_OBJECTS.className)[0], u = A[1]) : s in t && (l = (I = t[s])[0], u = I[1]), null == l) throw new ValueError(\"Unknown \" + r + \": \" + s + \". This may be due to one of the following reasons:\\n1. The \" + r + \" is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.\\n2. The custom \" + r + \" is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().\");if (null != u) {\n    for (var c = {}, p = 0, h = Object.keys(_GLOBAL_CUSTOM_OBJECTS); p < h.length; p++) {\n      c[v = h[p]] = _GLOBAL_CUSTOM_OBJECTS[v];\n    }for (var d = 0, g = Object.keys(n); d < g.length; d++) {\n      c[v = g[d]] = n[v];\n    }o.config.customObjects = c;for (var f = __assign({}, _GLOBAL_CUSTOM_OBJECTS), m = 0, y = Object.keys(n); m < y.length; m++) {\n      var v = y[m];_GLOBAL_CUSTOM_OBJECTS[v] = n[v];\n    }var b = u(l, o.config);return _GLOBAL_CUSTOM_OBJECTS = __assign({}, f), b;\n  }f = __assign({}, _GLOBAL_CUSTOM_OBJECTS);for (var w = 0, z = Object.keys(n); w < z.length; w++) {\n    v = z[w];_GLOBAL_CUSTOM_OBJECTS[v] = n[v];\n  }var S, A, I;b = new l(o.config);return _GLOBAL_CUSTOM_OBJECTS = __assign({}, f), b;\n}function numberCompare(e, t) {\n  return e < t ? -1 : e > t ? 1 : 0;\n}function reverseNumberCompare(e, t) {\n  return -1 * numberCompare(e, t);\n}function stringToDType(e) {\n  switch (e) {case \"float32\":\n      return \"float32\";default:\n      throw new ValueError(\"Invalid dtype: \" + e);}\n}function unique(e) {\n  if (null == e) return e;for (var t = [], n = 0, r = e; n < r.length; n++) {\n    var i = r[n];-1 === t.indexOf(i) && t.push(i);\n  }return t;\n}function isObjectEmpty(e) {\n  if (null == e) throw new ValueError(\"Invalid value in obj: \" + JSON.stringify(e));for (var t in e) if (e.hasOwnProperty(t)) return !1;return !0;\n}function checkStringTypeUnionValue(e, t, n) {\n  if (null != n && e.indexOf(n) < 0) throw new ValueError(n + \" is not a valid \" + t + \".  Valid values are \" + e + \" or null/undefined.\");\n}function checkArrayTypeAndLength(e, t, n, r) {\n  return void 0 === n && (n = 0), void 0 === r && (r = 1 / 0), assert(n >= 0), assert(r >= n), Array.isArray(e) && e.length >= n && e.length <= r && e.every(function (e) {\n    return typeof e === t;\n  });\n}function calcL2Norms(e, t) {\n  return (0, _tfjsCore.tidy)(function () {\n    return (0, _tfjsCore.sqrt)((0, _tfjsCore.sum)((0, _tfjsCore.mulStrict)(e, e), t, !0));\n  });\n}var Constraint = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }return __extends(t, e), t.prototype.getConfig = function () {\n    return {};\n  }, t;\n}(_tfjsCore.serialization.Serializable),\n    MaxNorm = function (e) {\n  function t(t) {\n    var n = e.call(this) || this;return n.defaultMaxValue = 2, n.defaultAxis = 0, n.maxValue = null != t.maxValue ? t.maxValue : n.defaultMaxValue, n.axis = null != t.axis ? t.axis : n.defaultAxis, n;\n  }return __extends(t, e), t.prototype.apply = function (e) {\n    var t = this;return (0, _tfjsCore.tidy)(function () {\n      var n = calcL2Norms(e, t.axis),\n          r = (0, _tfjsCore.clipByValue)(n, 0, t.maxValue);return (0, _tfjsCore.mul)(e, (0, _tfjsCore.div)(r, (0, _tfjsCore.add)(getScalar(epsilon()), n)));\n    });\n  }, t.prototype.getConfig = function () {\n    return { maxValue: this.maxValue, axis: this.axis };\n  }, t.className = \"MaxNorm\", t;\n}(Constraint);_tfjsCore.serialization.registerClass(MaxNorm);var UnitNorm = function (e) {\n  function t(t) {\n    var n = e.call(this) || this;return n.defaultAxis = 0, n.axis = null != t.axis ? t.axis : n.defaultAxis, n;\n  }return __extends(t, e), t.prototype.apply = function (e) {\n    var t = this;return (0, _tfjsCore.tidy)(function () {\n      return (0, _tfjsCore.div)(e, (0, _tfjsCore.add)(getScalar(epsilon()), calcL2Norms(e, t.axis)));\n    });\n  }, t.prototype.getConfig = function () {\n    return { axis: this.axis };\n  }, t.className = \"UnitNorm\", t;\n}(Constraint);_tfjsCore.serialization.registerClass(UnitNorm);var NonNeg = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }return __extends(t, e), t.prototype.apply = function (e) {\n    return (0, _tfjsCore.relu)(e);\n  }, t.className = \"NonNeg\", t;\n}(Constraint);_tfjsCore.serialization.registerClass(NonNeg);var MinMaxNorm = function (e) {\n  function t(t) {\n    var n = e.call(this) || this;return n.defaultMinValue = 0, n.defaultMaxValue = 1, n.defaultRate = 1, n.defaultAxis = 0, n.minValue = null != t.minValue ? t.minValue : n.defaultMinValue, n.maxValue = null != t.maxValue ? t.maxValue : n.defaultMaxValue, n.rate = null != t.rate ? t.rate : n.defaultRate, n.axis = null != t.axis ? t.axis : n.defaultAxis, n;\n  }return __extends(t, e), t.prototype.apply = function (e) {\n    var t = this;return (0, _tfjsCore.tidy)(function () {\n      var n = calcL2Norms(e, t.axis),\n          r = (0, _tfjsCore.add)((0, _tfjsCore.mul)(getScalar(t.rate), (0, _tfjsCore.clipByValue)(n, t.minValue, t.maxValue)), (0, _tfjsCore.mul)(getScalar(1 - t.rate), n));return (0, _tfjsCore.mul)(e, (0, _tfjsCore.div)(r, (0, _tfjsCore.add)(getScalar(epsilon()), n)));\n    });\n  }, t.prototype.getConfig = function () {\n    return { minValue: this.minValue, maxValue: this.maxValue, rate: this.rate, axis: this.axis };\n  }, t.className = \"MinMaxNorm\", t;\n}(Constraint);_tfjsCore.serialization.registerClass(MinMaxNorm);var CONSTRAINT_IDENTIFIER_REGISTRY_SYMBOL_MAP = { maxNorm: \"MaxNorm\", minMaxNorm: \"MinMaxNorm\", nonNeg: \"NonNeg\", unitNorm: \"UnitNorm\" };function serializeConstraint(e) {\n  return serializeKerasObject(e);\n}function deserializeConstraint(e, t) {\n  return void 0 === t && (t = {}), deserializeKerasObject(e, _tfjsCore.serialization.SerializationMap.getMap().classNameMap, t, \"constraint\");\n}function getConstraint(e) {\n  return null == e ? null : \"string\" == typeof e ? deserializeConstraint({ className: e in CONSTRAINT_IDENTIFIER_REGISTRY_SYMBOL_MAP ? CONSTRAINT_IDENTIFIER_REGISTRY_SYMBOL_MAP[e] : e, config: {} }) : e instanceof Constraint ? e : deserializeConstraint(e);\n}function maxNorm(e) {\n  return new MaxNorm(e);\n}function unitNorm(e) {\n  return new UnitNorm(e);\n}function nonNeg() {\n  return new NonNeg();\n}function minMaxNorm(e) {\n  return new MinMaxNorm(e);\n}var exports_constraints = Object.freeze({ maxNorm: maxNorm, unitNorm: unitNorm, nonNeg: nonNeg, minMaxNorm: minMaxNorm }),\n    nameMap = new Map(),\n    VALID_DATA_FORMAT_VALUES = [\"channelsFirst\", \"channelsLast\"];function checkDataFormat(e) {\n  checkStringTypeUnionValue(VALID_DATA_FORMAT_VALUES, \"DataFormat\", e);\n}var VALID_PADDING_MODE_VALUES = [\"valid\", \"same\", \"causal\"];function checkPaddingMode(e) {\n  checkStringTypeUnionValue(VALID_PADDING_MODE_VALUES, \"PaddingMode\", e);\n}var VALID_POOL_MODE_VALUES = [\"max\", \"avg\"];function checkPoolMode(e) {\n  checkStringTypeUnionValue(VALID_POOL_MODE_VALUES, \"PoolMode\", e);\n}var _nameScopeStack = [],\n    _nameScopeDivider = \"/\";function nameScope(e, t) {\n  _nameScopeStack.push(e);try {\n    var n = t();return _nameScopeStack.pop(), n;\n  } catch (e) {\n    throw _nameScopeStack.pop(), e;\n  }\n}function currentNameScopePrefix() {\n  return 0 === _nameScopeStack.length ? \"\" : _nameScopeStack.join(_nameScopeDivider) + _nameScopeDivider;\n}function getScopedTensorName(e) {\n  if (!isValidTensorName(e)) throw new Error(\"Not a valid tensor name: '\" + e + \"'\");return currentNameScopePrefix() + e;\n}function getUniqueTensorName(e) {\n  if (!isValidTensorName(e)) throw new Error(\"Not a valid tensor name: '\" + e + \"'\");nameMap.has(e) || nameMap.set(e, 0);var t = nameMap.get(e);if (nameMap.set(e, nameMap.get(e) + 1), t > 0) {\n    var n = e + \"_\" + t;return nameMap.set(n, 1), n;\n  }return e;\n}var tensorNameRegex = new RegExp(/^[A-Za-z][A-Za-z0-9\\._\\/]*$/);function isValidTensorName(e) {\n  return !!e.match(tensorNameRegex);\n}function isInteger(e) {\n  return e === parseInt(e.toString(), 10);\n}function arrayProd(e, t, n) {\n  null == t && (t = 0), null == n && (n = e.length);for (var r = 1, i = t; i < n; ++i) r *= e[i];return r;\n}function toArray1D(e) {\n  return e = Array.isArray(e) ? new Float32Array(e) : e, (0, _tfjsCore.tensor1d)(e);\n}function min$1(e) {\n  return (0, _tfjsCore.min)(toArray1D(e)).dataSync()[0];\n}function max$1(e) {\n  return (0, _tfjsCore.max)(toArray1D(e)).dataSync()[0];\n}function range(e, t) {\n  if (t < e) throw new ValueError(\"end (\" + t + \") < begin (\" + e + \") is forbidden.\");for (var n = [], r = e; r < t; ++r) n.push(r);return n;\n}function cast$1(e, t) {\n  return e.asType(t);\n}function expandDims(e, t) {\n  void 0 === t && (t = -1);var n = e.shape.slice();return t < 0 && (t = n.length + t + 1), n.splice(t, 0, 1), e.reshape(n);\n}function repeat(e, t) {\n  return (0, _tfjsCore.tidy)(function () {\n    if (2 !== e.shape.length) throw new ValueError(\"repeat() expects a rank-2 tensor, but received a rank-\" + e.shape.length + \" tensor.\");return tile$1(expandDims(e, 1), [1, t, 1]);\n  });\n}function flatten(e) {\n  var t = [arrayProd(e.shape)];return e.reshape(t);\n}function batchFlatten(e) {\n  if (e.rank <= 1) throw new ValueError(\"batchFlatten requires a minimum rank of 2. Got rank: \" + e.rank + \".\");var t = [e.shape[0], arrayProd(e.shape, 1)];return e.reshape(t);\n}function sliceAlongFirstAxis(e, t, n) {\n  return (0, _tfjsCore.tidy)(function () {\n    switch (e.rank) {case 1:\n        return (0, _tfjsCore.slice1d)(e, t, n);case 2:\n        return (0, _tfjsCore.slice2d)(e, [t, 0], [n, e.shape[1]]);case 3:\n        return (0, _tfjsCore.slice3d)(e, [t, 0, 0], [n, e.shape[1], e.shape[2]]);case 4:\n        return (0, _tfjsCore.slice4d)(e, [t, 0, 0, 0], [n, e.shape[1], e.shape[2], e.shape[3]]);default:\n        throw new ValueError(\"sliceAlongFirstAxis() received an unsupported tensor rank: \" + e.rank);}\n  });\n}function sliceAlongLastAxis(e, t, n) {\n  return (0, _tfjsCore.tidy)(function () {\n    switch (e.rank) {case 1:\n        return (0, _tfjsCore.slice1d)(e, t, n);case 2:\n        return (0, _tfjsCore.slice2d)(e, [0, t], [e.shape[0], n]);case 3:\n        return (0, _tfjsCore.slice3d)(e, [0, 0, t], [e.shape[0], e.shape[1], n]);case 4:\n        return (0, _tfjsCore.slice4d)(e, [0, 0, 0, t], [e.shape[0], e.shape[1], e.shape[2], n]);default:\n        throw new ValueError(\"sliceAlongLastAxis() received an unsupported tensor rank: \" + e.rank);}\n  });\n}function sliceAlongAxis(e, t, n, r) {\n  return (0, _tfjsCore.tidy)(function () {\n    switch (e.rank) {case 1:\n        return (0, _tfjsCore.slice1d)(e, t, n);case 2:\n        switch (r) {case 1:\n            return sliceAlongFirstAxis(e, t, n);case 2:\n            return sliceAlongLastAxis(e, t, n);default:\n            throw new ValueError(\"The axis is not within the rank of the tensor \" + r);}case 3:\n        switch (r) {case 1:\n            return sliceAlongFirstAxis(e, t, n);case 2:\n            return (0, _tfjsCore.slice3d)(e, [0, t, 0], [e.shape[0], n, e.shape[2]]);case 3:\n            return sliceAlongLastAxis(e, t, n);default:\n            throw new ValueError(\"The axis is not within the rank of the tensor \" + r);}case 4:\n        switch (r) {case 1:\n            return sliceAlongFirstAxis(e, t, n);case 2:\n            return (0, _tfjsCore.slice4d)(e, [0, t, 0, 0], [e.shape[0], n, e.shape[2], e.shape[3]]);case 3:\n            return (0, _tfjsCore.slice4d)(e, [0, 0, t, 0], [e.shape[0], e.shape[1], n, e.shape[3]]);case 4:\n            return sliceAlongLastAxis(e, t, n);default:\n            throw new ValueError(\"The axis is not within the rank of the tensor \" + r);}default:\n        throw new ValueError(\"sliceAlongLastAxis() received an unsupported tensor rank: \" + e.rank);}\n  });\n}function concatenate(e, t) {\n  var n;return void 0 === t && (t = -1), t < 0 && (t = 0 !== (n = e[0].rank) ? n : 0), t === e[0].rank && (t = -1), (0, _tfjsCore.concat)(e, t);\n}function concatAlongFirstAxis(e, t) {\n  switch (e.rank) {case 1:\n      return (0, _tfjsCore.concat1d)([e, t]);case 2:\n      return (0, _tfjsCore.concat2d)([e, t], 0);case 3:\n      return (0, _tfjsCore.concat3d)([e, t], 0);case 4:\n      return (0, _tfjsCore.concat4d)([e, t], 0);default:\n      throw new ValueError(\"concatAlongFirstAxis() received an unsupported tensor rank: \" + e.rank);}\n}function tile$1(e, t) {\n  if (Array.isArray(t) || (t = [t]), e.rank !== t.length) throw new ValueError(\"The length of input n (\" + t.length + \") does not match the number of dimensions in input x (\" + e.rank + \")\");return (0, _tfjsCore.tile)(e, t);\n}function randomNormal$1(e, t, n, r, i) {\n  return void 0 === t && (t = 0), void 0 === n && (n = 1), (0, _tfjsCore.randomNormal)(e, t, n, r, i);\n}function dot(e, t) {\n  if (e.rank < 2 || t.rank < 2) throw new NotImplementedError(\"dot requires both inputs to be rank >= 2 but got x shape = \" + e.shape + \" and y shape = \" + t.shape);if (t.rank >= 3 && (r = e.shape.slice(-1)[0]) !== (o = t.shape.slice(-2)[0])) throw new NotImplementedError(\"If rank y >= 3, then the second last dim of y must equal the last dim of x but got x shape = \" + e.shape + \" and  y shape = \" + t.shape);if (2 === e.rank && 2 === t.rank) return (0, _tfjsCore.matMul)(e, t);var n = e.shape.slice(),\n      r = n.pop();e = e.reshape([-1, r]);var i = t.shape.slice(),\n      a = i.pop(),\n      o = i.pop(),\n      s = i.concat([a]),\n      l = Array.from({ length: t.rank }, function (e, n) {\n    return 0 === n ? t.rank - 2 : n <= t.rank - 2 ? n - 1 : n;\n  });t = t.transpose(l).reshape([o, -1]);var u = n.concat(s);return (0, _tfjsCore.matMul)(e, t).reshape(u);\n}function gather$1(e, t, n) {\n  return (0, _tfjsCore.tidy)(function () {\n    return t = Array.isArray(t) ? (0, _tfjsCore.tensor1d)(t, \"int32\") : t.toInt(), (0, _tfjsCore.gather)(e, t, n);\n  });\n}function square(e) {\n  return (0, _tfjsCore.mulStrict)(e, e);\n}function biasAdd(e, t, n) {\n  return (0, _tfjsCore.tidy)(function () {\n    if (null == n && (n = imageDataFormat()), checkDataFormat(n), 1 !== t.rank && t.rank !== e.rank) throw new ValueError(\"Unexpected bias dimensions: \" + t.rank + \"; expected it to be 1 or \" + e.rank);var r,\n        i = t.shape;if (5 === e.rank) \"channelsFirst\" === n ? r = 1 === i.length ? e.add(t.reshape([1, i[0], 1, 1, 1])) : e.add(t.reshape([1, i[3], i[0], i[1], i[2]])) : \"channelsLast\" === n && (r = 1 === i.length ? e.add(t.reshape([1, 1, 1, 1, i[0]])) : e.add(t.reshape([1].concat(i))));else if (4 === e.rank) \"channelsFirst\" === n ? r = 1 === i.length ? e.add(t.reshape([1, i[0], 1, 1])) : e.add(t.reshape([1, i[2], i[0], i[1]])) : \"channelsLast\" === n && (r = 1 === i.length ? e.add(t.reshape([1, 1, 1, i[0]])) : e.add(t.reshape([1].concat(i))));else if (3 === e.rank) \"channelsFirst\" === n ? r = 1 === i.length ? e.add(t.reshape([1, i[0], 1])) : e.add(t.reshape([1, i[1], i[0]])) : \"channelsLast\" === n && (r = 1 === i.length ? e.add(t.reshape([1, 1, i[0]])) : e.add(t.reshape([1].concat(i))));else {\n      if (!(e.rank < 3)) throw new ValueError(\"Unsupported input rank by biasAdd: \" + e.rank);r = e.add(t);\n    }return r;\n  });\n}function elu$1(e, t) {\n  if (void 0 === t && (t = 1), 1 !== t) throw new NotImplementedError(\"Support for alpha values other than 1 (\" + t + \") is not implemented yet.\");return (0, _tfjsCore.elu)(e);\n}function softsign(e) {\n  return (0, _tfjsCore.tidy)(function () {\n    return (0, _tfjsCore.div)(e, (0, _tfjsCore.add)(getScalar(1), (0, _tfjsCore.abs)(e)));\n  });\n}function dropout(e, t, n, r) {\n  return (0, _tfjsCore.tidy)(function () {\n    if (null != n && !_tfjsCore.util.arraysEqual(e.shape, n)) throw new NotImplementedError(\"Non-default noise shape is not implemented yet: \" + JSON.stringify(n));if (null != r) throw new NotImplementedError(\"seed is not implemented for dropout yet.\");var i = (0, _tfjsCore.step)((0, _tfjsCore.add)((0, _tfjsCore.neg)(t), (0, _tfjsCore.randomUniform)(e.shape, 0, 1, \"float32\")));return i = (0, _tfjsCore.mul)((0, _tfjsCore.div)(getScalar(1), (0, _tfjsCore.sub)(getScalar(1), t)), i), (0, _tfjsCore.mul)(e, i);\n  });\n}function hardSigmoid(e) {\n  return (0, _tfjsCore.tidy)(function () {\n    var t = (0, _tfjsCore.add)(getScalar(.5), (0, _tfjsCore.mul)(getScalar(.2), e));return (0, _tfjsCore.clipByValue)(t, 0, 1);\n  });\n}function inTrainPhase(e, t, n) {\n  return void 0 === n && (n = !1), n ? e() : t();\n}var VALID_FAN_MODE_VALUES = [\"fanIn\", \"fanOut\", \"fanAvg\"];function checkFanMode(e) {\n  checkStringTypeUnionValue(VALID_FAN_MODE_VALUES, \"FanMode\", e);\n}var VALID_DISTRIBUTION_VALUES = [\"normal\", \"uniform\"];function checkDistribution(e) {\n  checkStringTypeUnionValue(VALID_DISTRIBUTION_VALUES, \"Distribution\", e);\n}var Initializer = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }return __extends(t, e), t.prototype.fromConfigUsesCustomObjects = function () {\n    return !1;\n  }, t.prototype.getConfig = function () {\n    return {};\n  }, t;\n}(_tfjsCore.serialization.Serializable),\n    Zeros = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }return __extends(t, e), t.prototype.apply = function (e, t) {\n    return (0, _tfjsCore.zeros)(e, t);\n  }, t.className = \"Zeros\", t;\n}(Initializer);_tfjsCore.serialization.registerClass(Zeros);var Ones = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }return __extends(t, e), t.prototype.apply = function (e, t) {\n    return (0, _tfjsCore.ones)(e, t);\n  }, t.className = \"Ones\", t;\n}(Initializer);_tfjsCore.serialization.registerClass(Ones);var Constant = function (e) {\n  function t(t) {\n    var n = e.call(this) || this;if (\"object\" != typeof t) throw new ValueError(\"Expected argument of type ConstantConfig but got \" + t);if (void 0 === t.value) throw new ValueError(\"config must have value set but got \" + t);return n.value = t.value, n;\n  }return __extends(t, e), t.prototype.apply = function (e, t) {\n    var n = this;return (0, _tfjsCore.tidy)(function () {\n      return (0, _tfjsCore.mul)((0, _tfjsCore.scalar)(n.value), (0, _tfjsCore.ones)(e, t));\n    });\n  }, t.prototype.getConfig = function () {\n    return { value: this.value };\n  }, t.className = \"Constant\", t;\n}(Initializer);_tfjsCore.serialization.registerClass(Constant);var RandomUniform = function (e) {\n  function t(t) {\n    var n = e.call(this) || this;return n.DEFAULT_MINVAL = -.05, n.DEFAULT_MAXVAL = .05, n.minval = t.minval || n.DEFAULT_MINVAL, n.maxval = t.maxval || n.DEFAULT_MAXVAL, n.seed = t.seed, n;\n  }return __extends(t, e), t.prototype.apply = function (e, t) {\n    return (0, _tfjsCore.randomUniform)(e, this.minval, this.maxval, t);\n  }, t.prototype.getConfig = function () {\n    return { minval: this.minval, maxval: this.maxval, seed: this.seed };\n  }, t.className = \"RandomUniform\", t;\n}(Initializer);_tfjsCore.serialization.registerClass(RandomUniform);var RandomNormal = function (e) {\n  function t(t) {\n    var n = e.call(this) || this;return n.DEFAULT_MEAN = 0, n.DEFAULT_STDDEV = .05, n.mean = t.mean || n.DEFAULT_MEAN, n.stddev = t.stddev || n.DEFAULT_STDDEV, n.seed = t.seed, n;\n  }return __extends(t, e), t.prototype.apply = function (e, t) {\n    if (\"float32\" !== (t = t || \"float32\") && \"int32\" !== t) throw new NotImplementedError(\"randomNormal does not support dType \" + t + \".\");return randomNormal$1(e, this.mean, this.stddev, t, this.seed);\n  }, t.prototype.getConfig = function () {\n    return { mean: this.mean, stddev: this.stddev, seed: this.seed };\n  }, t.className = \"RandomNormal\", t;\n}(Initializer);_tfjsCore.serialization.registerClass(RandomNormal);var TruncatedNormal = function (e) {\n  function t(t) {\n    var n = e.call(this) || this;return n.DEFAULT_MEAN = 0, n.DEFAULT_STDDEV = .05, n.mean = t.mean || n.DEFAULT_MEAN, n.stddev = t.stddev || n.DEFAULT_STDDEV, n.seed = t.seed, n;\n  }return __extends(t, e), t.prototype.apply = function (e, t) {\n    if (\"float32\" !== (t = t || \"float32\") && \"int32\" !== t) throw new NotImplementedError(\"truncatedNormal does not support dType \" + t + \".\");return (0, _tfjsCore.truncatedNormal)(e, this.mean, this.stddev, t, this.seed);\n  }, t.prototype.getConfig = function () {\n    return { mean: this.mean, stddev: this.stddev, seed: this.seed };\n  }, t.className = \"TruncatedNormal\", t;\n}(Initializer);_tfjsCore.serialization.registerClass(TruncatedNormal);var Identity = function (e) {\n  function t(t) {\n    var n = e.call(this) || this;return n.gain = null != t.gain ? (0, _tfjsCore.scalar)(t.gain) : getScalar(1), n;\n  }return __extends(t, e), t.prototype.apply = function (e, t) {\n    var n = this;return (0, _tfjsCore.tidy)(function () {\n      if (2 !== e.length || e[0] !== e[1]) throw new ValueError(\"Identity matrix initializer can only be used for 2D square matrices.\");return (0, _tfjsCore.mul)(n.gain, (0, _tfjsCore.eye)(e[0]));\n    });\n  }, t.prototype.getConfig = function () {\n    return { gain: this.gain.get() };\n  }, t.className = \"Identity\", t;\n}(Initializer);function computeFans(e, t) {\n  var n, r;if (void 0 === t && (t = \"channelsLast\"), checkDataFormat(t), 2 === e.length) n = e[0], r = e[1];else if (-1 !== [3, 4, 5].indexOf(e.length)) {\n    if (\"channelsFirst\" === t) {\n      var i = arrayProd(e, 2);n = e[1] * i, r = e[0] * i;\n    } else if (\"channelsLast\" === t) {\n      i = arrayProd(e, 0, e.length - 2);n = e[e.length - 2] * i, r = e[e.length - 1] * i;\n    }\n  } else {\n    var a = arrayProd(e);n = Math.sqrt(a), r = Math.sqrt(a);\n  }return [n, r];\n}_tfjsCore.serialization.registerClass(Identity);var VarianceScaling = function (e) {\n  function t(t) {\n    var n = e.call(this) || this;if (t.scale < 0) throw new ValueError(\"scale must be a positive float. Got: \" + t.scale);return n.scale = null == t.scale ? 1 : t.scale, n.mode = t.mode, checkFanMode(n.mode), n.distribution = t.distribution, checkDistribution(n.distribution), n.seed = t.seed, n;\n  }return __extends(t, e), t.prototype.apply = function (e, t) {\n    var n = computeFans(e),\n        r = n[0],\n        i = n[1],\n        a = this.scale;if (\"fanIn\" === this.mode ? a /= Math.max(1, r) : \"fanOut\" === this.mode ? a /= Math.max(1, i) : a /= Math.max(1, (r + i) / 2), \"normal\" === this.distribution) {\n      var o = Math.sqrt(a);if (\"float32\" !== (t = t || \"float32\") && \"int32\" !== t) throw new NotImplementedError(this.getClassName() + \" does not support dType \" + t + \".\");return (0, _tfjsCore.truncatedNormal)(e, 0, o, t, this.seed);\n    }var s = Math.sqrt(3 * a);return (0, _tfjsCore.randomUniform)(e, -s, s, t);\n  }, t.prototype.getConfig = function () {\n    return { scale: this.scale, mode: this.mode, distribution: this.distribution, seed: this.seed };\n  }, t.className = \"VarianceScaling\", t;\n}(Initializer);_tfjsCore.serialization.registerClass(VarianceScaling);var GlorotUniform = function (e) {\n  function t(t) {\n    return e.call(this, { scale: 1, mode: \"fanAvg\", distribution: \"uniform\", seed: null == t ? null : t.seed }) || this;\n  }return __extends(t, e), t.prototype.getClassName = function () {\n    return VarianceScaling.className;\n  }, t;\n}(VarianceScaling),\n    GlorotNormal = function (e) {\n  function t(t) {\n    return e.call(this, { scale: 1, mode: \"fanAvg\", distribution: \"normal\", seed: null == t ? null : t.seed }) || this;\n  }return __extends(t, e), t.prototype.getClassName = function () {\n    return VarianceScaling.className;\n  }, t;\n}(VarianceScaling),\n    HeNormal = function (e) {\n  function t(t) {\n    return e.call(this, { scale: 2, mode: \"fanIn\", distribution: \"normal\", seed: null == t ? null : t.seed }) || this;\n  }return __extends(t, e), t.prototype.getClassName = function () {\n    return VarianceScaling.className;\n  }, t;\n}(VarianceScaling),\n    LeCunNormal = function (e) {\n  function t(t) {\n    return e.call(this, { scale: 1, mode: \"fanIn\", distribution: \"normal\", seed: null == t ? null : t.seed }) || this;\n  }return __extends(t, e), t.prototype.getClassName = function () {\n    return VarianceScaling.className;\n  }, t;\n}(VarianceScaling),\n    Orthogonal = function (e) {\n  function t(t) {\n    var n = e.call(this) || this;if (n.DEFAULT_GAIN = 1, n.gain = null == t.gain ? n.DEFAULT_GAIN : t.gain, n.seed = t.seed, null != n.seed) throw new NotImplementedError(\"Random seed is not implemented for Orthogonal Initializer yet.\");return n;\n  }return __extends(t, e), t.prototype.apply = function (e, t) {\n    var n = this;return (0, _tfjsCore.tidy)(function () {\n      if (2 !== e.length) throw new NotImplementedError(\"The Orthogonal Initializer does not support non-2D shapes yet.\");e[0] * e[1] > 2e3 && console.warn(\"Orthogonal initializer is being called on a matrix with more than 2000 (\" + e[0] * e[1] + \") elements: Slowness may result.\");var t = randomNormal$1(e[0] > e[1] ? [e[1], e[0]] : e, 0, 1, \"float32\"),\n          r = _tfjsCore.linalg.gramSchmidt(t);return e[0] > e[1] && (r = r.transpose()), (0, _tfjsCore.mul)(getScalar(n.gain), r);\n    });\n  }, t.prototype.getConfig = function () {\n    return { gain: this.gain, seed: this.seed };\n  }, t.className = \"Orthogonal\", t;\n}(Initializer);_tfjsCore.serialization.registerClass(Orthogonal);var INITIALIZER_IDENTIFIER_REGISTRY_SYMBOL_MAP = { constant: \"Constant\", glorotNormal: \"GlorotNormal\", glorotUniform: \"GlorotUniform\", heNormal: \"HeNormal\", identity: \"Identity\", leCunNormal: \"LeCunNormal\", ones: \"Ones\", orthogonal: \"Orthogonal\", randomNormal: \"RandomNormal\", randomUniform: \"RandomUniform\", truncatedNormal: \"TruncatedNormal\", varianceScaling: \"VarianceScaling\", zeros: \"Zeros\" };function deserializeInitializer(e, t) {\n  return void 0 === t && (t = {}), deserializeKerasObject(e, _tfjsCore.serialization.SerializationMap.getMap().classNameMap, t, \"initializer\");\n}function serializeInitializer(e) {\n  return serializeKerasObject(e);\n}function getInitializer(e) {\n  if (\"string\" == typeof e) {\n    var t = e in INITIALIZER_IDENTIFIER_REGISTRY_SYMBOL_MAP ? INITIALIZER_IDENTIFIER_REGISTRY_SYMBOL_MAP[e] : e;return \"GlorotUniform\" === t ? new GlorotUniform() : \"GlorotNormal\" === t ? new GlorotNormal() : \"HeNormal\" === t ? new HeNormal() : \"LeCunNormal\" === t ? new LeCunNormal() : deserializeInitializer({ className: t, config: {} });\n  }return e instanceof Initializer ? e : deserializeInitializer(e);\n}function zeros$1() {\n  return new Zeros();\n}function ones$1() {\n  return new Ones();\n}function constant(e) {\n  return new Constant(e);\n}function randomUniform$1(e) {\n  return new RandomUniform(e);\n}function randomNormal$2(e) {\n  return new RandomNormal(e);\n}function truncatedNormal$1(e) {\n  return new TruncatedNormal(e);\n}function identity(e) {\n  return new Identity(e);\n}function varianceScaling(e) {\n  return new VarianceScaling(e);\n}function glorotUniform(e) {\n  return new GlorotUniform(e);\n}function glorotNormal(e) {\n  return new GlorotNormal(e);\n}function heNormal(e) {\n  return new HeNormal(e);\n}function leCunNormal(e) {\n  return new LeCunNormal(e);\n}function orthogonal(e) {\n  return new Orthogonal(e);\n}var exports_initializers = Object.freeze({ zeros: zeros$1, ones: ones$1, constant: constant, randomUniform: randomUniform$1, randomNormal: randomNormal$2, truncatedNormal: truncatedNormal$1, identity: identity, varianceScaling: varianceScaling, glorotUniform: glorotUniform, glorotNormal: glorotNormal, heNormal: heNormal, leCunNormal: leCunNormal, orthogonal: orthogonal });function isArrayOfShapes(e) {\n  return Array.isArray(e) && Array.isArray(e[0]);\n}function normalizeShapeList(e) {\n  return 0 === e.length ? [] : Array.isArray(e[0]) ? e : [e];\n}function getExactlyOneTensor(e) {\n  var t;if (Array.isArray(e)) {\n    if (1 !== e.length) throw new ValueError(\"Expected Tensor length to be 1; got \" + e.length);t = e[0];\n  } else t = e;return t;\n}function getExactlyOneShape(e) {\n  if (Array.isArray(e) && Array.isArray(e[0])) {\n    if (1 === e.length) return (e = e)[0];throw new ValueError(\"Expected exactly 1 Shape; got \" + e.length);\n  }return e;\n}function countParamsInWeights(e) {\n  for (var t = 0, n = 0, r = e; n < r.length; n++) {\n    var i = r[n];0 === i.shape.length ? t += 1 : t += i.shape.reduce(function (e, t) {\n      return e * t;\n    });\n  }return t;\n}var DEFAULT_VARIABLE_NAME_PREFIX = \"Variable\",\n    LayerVariable = function () {\n  function e(e, t, n, r, i) {\n    void 0 === t && (t = \"float32\"), void 0 === n && (n = DEFAULT_VARIABLE_NAME_PREFIX), void 0 === r && (r = !0), void 0 === i && (i = null), this.dtype = null == t ? \"float32\" : t, this.shape = e.shape, this.id = getNextUniqueTensorId(), n = null == n ? DEFAULT_VARIABLE_NAME_PREFIX : n, this.originalName = getScopedTensorName(n), this.name = getUniqueTensorName(this.originalName), this.trainable = r, this.constraint = i, this.val = (0, _tfjsCore.variable)(e, this.trainable, this.name, this.dtype);\n  }return e.prototype.read = function () {\n    return this.assertNotDisposed(), this.val;\n  }, e.prototype.write = function (e) {\n    return this.assertNotDisposed(), checkShapesMatch(this.val, e), this.val.id !== e.id && (this.val.assign(e), null != this.constraint && this.val.assign(this.constraint.apply(this.val))), this;\n  }, e.prototype.dispose = function () {\n    this.assertNotDisposed(), this.val.dispose();\n  }, e.prototype.assertNotDisposed = function () {\n    if (this.val.isDisposed) throw new Error(\"LayersVariable \" + this.name + \" is already disposed.\");\n  }, e;\n}();function checkShapesMatch(e, t) {\n  if (e.shape.toString() !== t.shape.toString()) throw new Error(\"Shape mismatch: \" + JSON.stringify(e.shape) + \" vs. \" + JSON.stringify(t.shape));\n}function batchGetValue(e) {\n  return e.map(function (e) {\n    return e.read();\n  });\n}function batchSetValue(e) {\n  e.map(function (e) {\n    e[0].write(e[1]);\n  });\n}var InputSpec = function () {\n  return function (e) {\n    this.dtype = e.dtype, this.shape = e.shape, null != e.shape ? this.ndim = e.shape.length : this.ndim = e.ndim, this.maxNDim = e.maxNDim, this.minNDim = e.minNDim, this.axes = e.axes || {};\n  };\n}(),\n    SymbolicTensor = function () {\n  return function (e, t, n, r, i, a, o) {\n    this.dtype = e, this.shape = t, this.sourceLayer = n, this.inputs = r, this.callArgs = i, this.outputTensorIndex = o, this.id = getNextUniqueTensorId(), null != a && (this.originalName = getScopedTensorName(a), this.name = getUniqueTensorName(this.originalName)), this.rank = t.length;\n  };\n}(),\n    _nextNodeID = 0,\n    Node = function () {\n  function e(e, t) {\n    this.callArgs = t, this.id = _nextNodeID++, this.outboundLayer = e.outboundLayer, this.inboundLayers = e.inboundLayers, this.nodeIndices = e.nodeIndices, this.tensorIndices = e.tensorIndices, this.inputTensors = e.inputTensors, this.outputTensors = e.outputTensors, this.inputMasks = e.inputMasks, this.outputMasks = e.outputMasks, this.inputShapes = e.inputShapes, this.outputShapes = e.outputShapes;for (var n = 0, r = e.inboundLayers; n < r.length; n++) {\n      var i = r[n];null != i && i.outboundNodes.push(this);\n    }e.outboundLayer.inboundNodes.push(this);\n  }return e.prototype.getConfig = function () {\n    for (var e = [], t = 0, n = this.inboundLayers; t < n.length; t++) {\n      var r = n[t];null != r ? e.push(r.name) : e.push(null);\n    }return { outboundLayer: this.outboundLayer ? this.outboundLayer.name : null, inboundLayers: e, nodeIndices: this.nodeIndices, tensorIndices: this.tensorIndices };\n  }, e;\n}(),\n    _nextLayerID = 0,\n    Layer = function (e) {\n  function t(t) {\n    var n = e.call(this) || this;n._callHook = null, n._addedWeightNames = [], n._stateful = !1, n.id = _nextLayerID++, n.activityRegularizer = null, n.inputSpec = null, n.supportsMasking = !1, n._trainableWeights = [], n._nonTrainableWeights = [], n._losses = [], n._updates = [], n._built = !1, n.inboundNodes = [], n.outboundNodes = [];var r = t.name;if (!r) {\n      var i = n.getClassName();r = toSnakeCase(i) + \"_\" + getUid(i);\n    }if (n.name = r, n.trainable = null == t.trainable || t.trainable, n.updatable = null == t.updatable || t.updatable, null != t.inputShape || null != t.batchInputShape) {\n      var a = void 0;if (null != t.batchInputShape) a = t.batchInputShape;else if (null != t.inputShape) {\n        var o = null;null != t.batchSize && (o = t.batchSize), a = [o].concat(t.inputShape);\n      }n.batchInputShape = a;var s = t.dtype;null == s && (s = t.inputDType), null == s && (s = \"float32\"), n.dtype = s;\n    }return null != t.weights ? n.initialWeights = t.weights : n.initialWeights = null, n._refCount = null, n;\n  }return __extends(t, e), t.nodeKey = function (e, t) {\n    return e.name + \"_ib-\" + t.toString();\n  }, t.prototype.getNodeAtIndex = function (e, t) {\n    if (0 === this.inboundNodes.length) throw new RuntimeError(\"The layer has never been called and thus has no defined \" + t + \".\");if (this.inboundNodes.length <= e) throw new ValueError(\"Asked to get \" + t + \" at node \" + e + \", but the layer has only \" + this.inboundNodes.length + \" inbound nodes.\");return this.inboundNodes[e];\n  }, t.prototype.getInputAt = function (e) {\n    return singletonOrArray(this.getNodeAtIndex(e, \"input\").inputTensors);\n  }, t.prototype.getOutputAt = function (e) {\n    return singletonOrArray(this.getNodeAtIndex(e, \"output\").outputTensors);\n  }, Object.defineProperty(t.prototype, \"input\", { get: function () {\n      if (this.inboundNodes.length > 1) throw new AttributeError(\"Layer \" + this.name + ' has multiple inbound nodes, hence the notion of \"layer input\" is ill-defined. Use `getInputAt(nodeIndex)` instead.');if (0 === this.inboundNodes.length) throw new AttributeError(\"Layer \" + this.name + \" is not connected, no input to return.\");return singletonOrArray(this.getNodeAtIndex(0, \"input\").inputTensors);\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"output\", { get: function () {\n      if (0 === this.inboundNodes.length) throw new AttributeError(\"Layer \" + this.name + \" has no inbound nodes.\");if (this.inboundNodes.length > 1) throw new AttributeError(\"Layer \" + this.name + ' has multiple inbound nodes, hence the notion of \"layer output\" is ill-defined. Use `getOutputAt(nodeIndex)` instead.');return singletonOrArray(this.getNodeAtIndex(0, \"output\").outputTensors);\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"losses\", { get: function () {\n      return this._losses;\n    }, enumerable: !0, configurable: !0 }), t.prototype.calculateLosses = function () {\n    return this.losses.map(function (e) {\n      return e();\n    });\n  }, Object.defineProperty(t.prototype, \"updates\", { get: function () {\n      return this._updates;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"built\", { get: function () {\n      return this._built;\n    }, set: function (e) {\n      this._built = e;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"trainableWeights\", { get: function () {\n      return this.trainable ? this._trainableWeights : [];\n    }, set: function (e) {\n      this._trainableWeights = e;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"nonTrainableWeights\", { get: function () {\n      return this.trainable ? this._nonTrainableWeights : this._trainableWeights.concat(this._nonTrainableWeights);\n    }, set: function (e) {\n      this._nonTrainableWeights = e;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"weights\", { get: function () {\n      return this.trainableWeights.concat(this.nonTrainableWeights);\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"stateful\", { get: function () {\n      return this._stateful;\n    }, enumerable: !0, configurable: !0 }), t.prototype.resetStates = function () {\n    if (!this.stateful) throw new Error(\"Cannot call the resetStates() method of a non-stateful Layer object.\");\n  }, t.prototype.assertInputCompatibility = function (e) {\n    if (e = toList(e), null != this.inputSpec && 0 !== this.inputSpec.length) {\n      var t = toList(this.inputSpec);if (e.length !== t.length) throw new ValueError(\"Layer \" + this.name + \" expects \" + t.length + \" inputs, but it received \" + e.length + \" input tensors. Input received: \" + e);for (var n = 0; n < e.length; n++) {\n        var r = e[n],\n            i = t[n];if (null != i) {\n          var a = r.rank;if (null != i.ndim && a !== i.ndim) throw new ValueError(\"Input \" + n + \" is incompatible with layer \" + this.name + \": expected ndim=\" + i.ndim + \", found ndim=\" + a);if (null != i.maxNDim && a > i.maxNDim) throw new ValueError(\"Input \" + n + \" is incompatible with layer \" + this.name + \": expected max_ndim=\" + i.maxNDim + \", found ndim=\" + a);if (null != i.minNDim && a < i.minNDim) throw new ValueError(\"Input \" + n + \" is incompatible with layer \" + this.name + \": expected min_ndim=\" + i.minNDim + \", found ndim=\" + a + \".\");if (null != i.dtype && r.dtype !== i.dtype) throw new ValueError(\"Input \" + n + \" is incompatible with layer \" + this.name + \" : expected dtype=\" + i.dtype + \", found dtype=\" + r.dtype + \".\");if (i.axes) {\n            var o = r.shape;for (var s in i.axes) {\n              var l = Number(s),\n                  u = i.axes[s],\n                  c = l >= 0 ? o[l] : o[o.length + l];if (null != u && -1 === [u, null].indexOf(c)) throw new ValueError(\"Input \" + n + \" is incompatible with layer \" + this.name + \": expected axis \" + l + \" of input shape to have value \" + u + \" but got shape \" + o + \".\");\n            }\n          }if (null != i.shape) for (var p = 0; p < i.shape.length; ++p) {\n            var h = i.shape[p],\n                d = r.shape[p];if (null != h && null != d && h !== d) throw new ValueError(\"Input \" + n + \" is incompatible with layer \" + this.name + \": expected shape=\" + i.shape + \", found shape=${xShape}.\");\n          }\n        }\n      }\n    }\n  }, t.prototype.call = function (e, t) {\n    return e;\n  }, t.prototype.invokeCallHook = function (e, t) {\n    null != this._callHook && this._callHook(e, t);\n  }, t.prototype.setCallHook = function (e) {\n    this._callHook = e;\n  }, t.prototype.clearCallHook = function () {\n    this._callHook = null;\n  }, t.prototype.apply = function (e, t) {\n    var n = this;t = t || {}, this.assertNotDisposed();for (var r = toList(e), i = !0, a = 0, o = r; a < o.length; a++) {\n      if (!(o[a] instanceof SymbolicTensor)) {\n        i = !1;break;\n      }\n    }for (var s = !0, l = 0, u = r; l < u.length; l++) {\n      if (u[l] instanceof SymbolicTensor) {\n        s = !1;break;\n      }\n    }if (i === s) throw new ValueError(\"Arguments to apply() must be all SymbolicTensors or all Tensors\");return nameScope(this.name, function () {\n      if (!n.built) {\n        n.assertInputCompatibility(e);for (var i = [], a = 0, o = toList(e); a < o.length; a++) {\n          var l = o[a];i.push(l.shape);\n        }n.build(singletonOrArray(i)), n.built = !0, n.initialWeights && n.setWeights(n.initialWeights), null === n._refCount && s && (n._refCount = 1);\n      }if (n.assertInputCompatibility(e), s) {\n        for (var u = [], c = 0, p = toList(f = n.call(e, t)); c < p.length; c++) {\n          var h = p[c];-1 !== r.indexOf(h) && (h = h.clone()), u.push(h);\n        }if (f = singletonOrArray(u), null != n.activityRegularizer) throw new NotImplementedError(\"Layer invocation in the presence of activity regularizer(s) is not supported yet.\");return f;\n      }var d = collectInputShape(e),\n          g = n.computeOutputShape(d),\n          f = void 0,\n          m = guessOutputDType(e);if (n.warnOnIncompatibleInputShape(Array.isArray(e) ? d[0] : d), f = null != g && g.length > 0 && Array.isArray(g[0]) ? g.map(function (r, i) {\n        return new SymbolicTensor(m, r, n, toList(e), t, n.name, i);\n      }) : new SymbolicTensor(m, g, n, toList(e), t, n.name), n.addInboundNode(e, f, null, null, d, g, t), n._refCount++, null != n.activityRegularizer) throw new NotImplementedError(\"Layer invocation in the presence of activity regularizer(s) is not supported yet.\");return f;\n    });\n  }, t.prototype.warnOnIncompatibleInputShape = function (e) {\n    if (null != this.batchInputShape) if (e.length !== this.batchInputShape.length) console.warn(\"The rank of the input tensor provided (shape: \" + JSON.stringify(e) + \") does not match that of the batchInputShape (\" + JSON.stringify(this.batchInputShape) + \") of the layer \" + this.name);else {\n      var t = !1;this.batchInputShape.forEach(function (n, r) {\n        null != n && null != e[r] && e[r] !== n && (t = !0);\n      }), t && console.warn(\"The shape of the input tensor (\" + JSON.stringify(e) + \") does not match the expectation of layer \" + this.name + \": \" + JSON.stringify(this.batchInputShape));\n    }\n  }, Object.defineProperty(t.prototype, \"outputShape\", { get: function () {\n      if (null == this.inboundNodes || 0 === this.inboundNodes.length) throw new AttributeError(\"The layer \" + this.name + \" has never been called and thus has no defined output shape.\");for (var e = [], t = 0, n = this.inboundNodes; t < n.length; t++) {\n        var r = n[t],\n            i = JSON.stringify(r.outputShapes);-1 === e.indexOf(i) && e.push(i);\n      }if (1 === e.length) {\n        var a = this.inboundNodes[0].outputShapes;return Array.isArray(a) && Array.isArray(a[0]) && 1 === a.length ? a[0] : a;\n      }throw new AttributeError(\"The layer \" + this.name + ' has multiple inbound nodes with different output shapes. Hence the notion of \"outut shape\" is ill-defined for the layer.');\n    }, enumerable: !0, configurable: !0 }), t.prototype.countParams = function () {\n    if (!this.built) throw new RuntimeError(\"You tried to call countParams() on \" + this.name + \", but the layer is not built yet. Build it first by calling build(batchInputShape).\");return countParamsInWeights(this.weights);\n  }, t.prototype.build = function (e) {\n    this.built = !0;\n  }, t.prototype.getWeights = function (e) {\n    return void 0 === e && (e = !1), batchGetValue(e ? this.trainableWeights : this.weights);\n  }, t.prototype.setWeights = function (e) {\n    var t = this;(0, _tfjsCore.tidy)(function () {\n      var n = t.weights;if (n.length !== e.length) throw new ValueError('You called setWeights(weights) on layer \"' + t.name + '\" with a weight list of length ' + e.length + \", but the layer was expecting \" + n.length + \" weights. Provided weights: \" + e + \"...\");if (0 !== n.length) {\n        for (var r = [], i = batchGetValue(n), a = 0; a < i.length; ++a) {\n          var o = i[a],\n              s = n[a],\n              l = e[a];if (!_tfjsCore.util.arraysEqual(o.shape, l.shape)) throw new ValueError(\"Layer weight shape \" + o.shape + \" not compatible with provided weight shape \" + l.shape);r.push([s, l]);\n        }batchSetValue(r);\n      }\n    });\n  }, t.prototype.addWeight = function (e, t, n, r, i, a, o) {\n    if (-1 !== this._addedWeightNames.indexOf(e)) throw new ValueError(\"Duplicate weight name \" + e + \" for layer \" + this.name);this._addedWeightNames.push(e), null == n && (n = \"float32\");var s = new LayerVariable(r.apply(t, n), n, e, a, o);return null != i && this.addLoss(function () {\n      return i.apply(s.read());\n    }), null == a && (a = !0), a ? this._trainableWeights.push(s) : this._nonTrainableWeights.push(s), s;\n  }, t.prototype.addLoss = function (e) {\n    var t;null == e || Array.isArray(e) && 0 === e.length || (e = toList(e), void 0 !== this._losses && null !== this._losses && (t = this.losses).push.apply(t, e));\n  }, t.prototype.computeOutputShape = function (e) {\n    return e;\n  }, t.prototype.computeMask = function (e, t) {\n    var n = this;if (!this.supportsMasking) {\n      if (null != t) {\n        if (!Array.isArray(t)) throw new TypeError(\"Layer \" + this.name + \" does not support masking,but was passed an inputMask.\");t.forEach(function (e) {\n          if (null != e) throw new TypeError(\"Layer \" + n.name + \" does not support masking,but was passed an inputMask.\");\n        });\n      }return null;\n    }return t;\n  }, t.prototype.addInboundNode = function (e, t, n, r, i, a, o) {\n    void 0 === o && (o = null);var s = toList(e);t = toList(t), n = toList(n), r = toList(r), i = normalizeShapeList(i), a = normalizeShapeList(a);for (var l = [], u = [], c = [], p = 0, h = s; p < h.length; p++) {\n      var d = h[p];l.push(d.sourceLayer), u.push(d.nodeIndex), c.push(d.tensorIndex);\n    }new Node({ outboundLayer: this, inboundLayers: l, nodeIndices: u, tensorIndices: c, inputTensors: s, outputTensors: t, inputMasks: n, outputMasks: r, inputShapes: i, outputShapes: a }, o);for (var g = 0; g < t.length; g++) t[g].sourceLayer = this, t[g].nodeIndex = this.inboundNodes.length - 1, t[g].tensorIndex = g;\n  }, t.prototype.getConfig = function () {\n    var e = { name: this.name, trainable: this.trainable };return null != this.batchInputShape && (e.batchInputShape = this.batchInputShape), null != this.dtype && (e.dtype = this.dtype), e;\n  }, t.prototype.disposeWeights = function () {\n    return this.weights.forEach(function (e) {\n      return e.dispose();\n    }), this.weights.length;\n  }, t.prototype.assertNotDisposed = function () {\n    if (0 === this._refCount) throw new Error(\"Layer '\" + this.name + \"' is already disposed.\");\n  }, t.prototype.dispose = function () {\n    if (!this.built) throw new Error(\"Cannot dispose Layer \" + this.name + \" because it has not been built yet.\");if (null === this._refCount) throw new Error(\"Cannot dispose Layer \" + this.name + \" because it has not been used yet.\");this.assertNotDisposed();var e = 0;return 0 == --this._refCount && (e = this.disposeWeights()), { refCountAfterDispose: this._refCount, numDisposedVariables: e };\n  }, t;\n}(_tfjsCore.serialization.Serializable);function collectInputShape(e) {\n  for (var t = [], n = 0, r = e = toList(e); n < r.length; n++) {\n    var i = r[n];t.push(i.shape);\n  }return singletonOrArray(t);\n}function guessOutputDType(e) {\n  return \"float32\";\n}function getSourceInputs(e, t, n) {\n  if ((null == t || null != n && n > 0) && (t = e.sourceLayer, n = e.nodeIndex), 0 === t.inboundNodes.length) return [e];var r = t.inboundNodes[n];if (0 === r.inboundLayers.length) return r.inputTensors;for (var i = [], a = 0; a < r.inboundLayers.length; a++) for (var o = 0, s = getSourceInputs(r.inputTensors[a], r.inboundLayers[a], r.nodeIndices[a]); o < s.length; o++) {\n    var l = s[o];-1 === i.indexOf(l) && i.push(l);\n  }return i;\n}var InputLayer = function (e) {\n  function t(t) {\n    var n = e.call(this, { dtype: t.dtype, name: null != t.name ? t.name : getUid(\"input\").toString() }) || this;if (null == t.batchSize && (t.batchSize = null), null == t.sparse && (t.sparse = !1), n.trainable = !1, n.built = !0, n.sparse = t.sparse, null != t.inputShape && null != t.batchInputShape) throw new ValueError(\"Only provide the inputShape OR batchInputShape argument to inputLayer, not both at the same time.\");var r = t.batchInputShape;if (null == r) {\n      if (null == t.inputShape) throw new ValueError(\"An InputLayer should be passed either a `batchInputShape` or an `inputShape`.\");r = [t.batchSize].concat(t.inputShape);\n    } else if (null != t.batchSize) throw new ValueError(\"Cannot specify batchSize if batchInputShape isspecified when creating an InputLayer.\");var i = t.dtype || \"float32\";n.batchInputShape = r, n.dtype = i, n.inputSpec = [{ shape: r }];var a = new SymbolicTensor(n.dtype, n.batchInputShape, n, [], {}, n.name);return a.nodeIndex = 0, a.tensorIndex = 0, new Node({ outboundLayer: n, inboundLayers: [], nodeIndices: [], tensorIndices: [], inputTensors: [a], outputTensors: [a], inputMasks: [null], outputMasks: [null], inputShapes: [r], outputShapes: [r] }), n;\n  }return __extends(t, e), t.prototype.apply = function (e, t) {\n    throw new ValueError(\"Cannot pass any input to an InputLayer's apply() method. InputLayer name: \" + this.name);\n  }, t.prototype.dispose = function () {\n    return { refCountAfterDispose: this._refCount, numDisposedVariables: 0 };\n  }, t.prototype.getConfig = function () {\n    return { batchInputShape: this.batchInputShape, dtype: this.dtype, sparse: this.sparse, name: this.name };\n  }, t.className = \"InputLayer\", t;\n}(Layer);function Input(e) {\n  if (null == e.batchShape && null == e.shape) throw new Error(\"Please provide to Input either a `shape` or a `batchShape` argument. Note that `shape` does not include the batch dimension.\");if (null != e.batchShape && null != e.shape) throw new ValueError(\"Please provide either a `shape` or `batchShape` argument to Input, but not both.\");var t = e.batchShape;null != e.shape && null == t && (t = [null].concat(e.shape));var n = e.dtype;return null == n && (n = \"float32\"), new InputLayer({ batchInputShape: t, name: e.name, dtype: n, sparse: e.sparse }).inboundNodes[0].outputTensors[0];\n}function resolveScalarsInLogs(e) {\n  return __awaiter(this, void 0, void 0, function () {\n    var t, n, r, i, a, o, s, l;return __generator(this, function (u) {\n      switch (u.label) {case 0:\n          if (null == e) return [2];for (i in t = [], n = [], r = [], e) \"number\" != typeof (a = e[i]) && (o = a, t.push(o.data()), n.push(i), r.push(o));return [4, Promise.all(t)];case 1:\n          for (s = u.sent(), l = 0; l < s.length; ++l) e[n[l]] = s[l][0];return (0, _tfjsCore.dispose)(r), [2];}\n    });\n  });\n}function disposeTensorsInLogs(e) {\n  if (null != e) for (var t in e) {\n    var n = e[t];\"number\" != typeof n && n.dispose();\n  }\n}_tfjsCore.serialization.registerClass(InputLayer);var BaseCallback = function () {\n  function e() {\n    this.validationData = null;\n  }return e.prototype.setParams = function (e) {\n    this.params = e;\n  }, e.prototype.onEpochBegin = function (e, t) {\n    return __awaiter(this, void 0, void 0, function () {\n      return __generator(this, function (e) {\n        return [2];\n      });\n    });\n  }, e.prototype.onEpochEnd = function (e, t) {\n    return __awaiter(this, void 0, void 0, function () {\n      return __generator(this, function (e) {\n        return [2];\n      });\n    });\n  }, e.prototype.onBatchBegin = function (e, t) {\n    return __awaiter(this, void 0, void 0, function () {\n      return __generator(this, function (e) {\n        return [2];\n      });\n    });\n  }, e.prototype.onBatchEnd = function (e, t) {\n    return __awaiter(this, void 0, void 0, function () {\n      return __generator(this, function (e) {\n        return [2];\n      });\n    });\n  }, e.prototype.onTrainBegin = function (e) {\n    return __awaiter(this, void 0, void 0, function () {\n      return __generator(this, function (e) {\n        return [2];\n      });\n    });\n  }, e.prototype.onTrainEnd = function (e) {\n    return __awaiter(this, void 0, void 0, function () {\n      return __generator(this, function (e) {\n        return [2];\n      });\n    });\n  }, e.prototype.setModel = function (e) {}, e;\n}(),\n    CallbackList = function () {\n  function e(e, t) {\n    void 0 === t && (t = 10), null == e && (e = []), this.callbacks = e, this.queueLength = t;\n  }return e.prototype.append = function (e) {\n    this.callbacks.push(e);\n  }, e.prototype.setParams = function (e) {\n    for (var t = 0, n = this.callbacks; t < n.length; t++) {\n      n[t].setParams(e);\n    }\n  }, e.prototype.setModel = function (e) {\n    for (var t = 0, n = this.callbacks; t < n.length; t++) {\n      n[t].setModel(e);\n    }\n  }, e.prototype.onEpochBegin = function (e, t) {\n    return __awaiter(this, void 0, void 0, function () {\n      var n, r;return __generator(this, function (i) {\n        switch (i.label) {case 0:\n            null == t && (t = {}), n = 0, r = this.callbacks, i.label = 1;case 1:\n            return n < r.length ? [4, r[n].onEpochBegin(e, t)] : [3, 4];case 2:\n            i.sent(), i.label = 3;case 3:\n            return n++, [3, 1];case 4:\n            return [2];}\n      });\n    });\n  }, e.prototype.onEpochEnd = function (e, t) {\n    return __awaiter(this, void 0, void 0, function () {\n      var n, r;return __generator(this, function (i) {\n        switch (i.label) {case 0:\n            null == t && (t = {}), n = 0, r = this.callbacks, i.label = 1;case 1:\n            return n < r.length ? [4, r[n].onEpochEnd(e, t)] : [3, 4];case 2:\n            i.sent(), i.label = 3;case 3:\n            return n++, [3, 1];case 4:\n            return [2];}\n      });\n    });\n  }, e.prototype.onBatchBegin = function (e, t) {\n    return __awaiter(this, void 0, void 0, function () {\n      var n, r;return __generator(this, function (i) {\n        switch (i.label) {case 0:\n            null == t && (t = {}), n = 0, r = this.callbacks, i.label = 1;case 1:\n            return n < r.length ? [4, r[n].onBatchBegin(e, t)] : [3, 4];case 2:\n            i.sent(), i.label = 3;case 3:\n            return n++, [3, 1];case 4:\n            return [2];}\n      });\n    });\n  }, e.prototype.onBatchEnd = function (e, t) {\n    return __awaiter(this, void 0, void 0, function () {\n      var n, r;return __generator(this, function (i) {\n        switch (i.label) {case 0:\n            null == t && (t = {}), n = 0, r = this.callbacks, i.label = 1;case 1:\n            return n < r.length ? [4, r[n].onBatchEnd(e, t)] : [3, 4];case 2:\n            i.sent(), i.label = 3;case 3:\n            return n++, [3, 1];case 4:\n            return [2];}\n      });\n    });\n  }, e.prototype.onTrainBegin = function (e) {\n    return __awaiter(this, void 0, void 0, function () {\n      var t, n;return __generator(this, function (r) {\n        switch (r.label) {case 0:\n            null == e && (e = {}), t = 0, n = this.callbacks, r.label = 1;case 1:\n            return t < n.length ? [4, n[t].onTrainBegin(e)] : [3, 4];case 2:\n            r.sent(), r.label = 3;case 3:\n            return t++, [3, 1];case 4:\n            return [2];}\n      });\n    });\n  }, e.prototype.onTrainEnd = function (e) {\n    return __awaiter(this, void 0, void 0, function () {\n      var t, n;return __generator(this, function (r) {\n        switch (r.label) {case 0:\n            null == e && (e = {}), t = 0, n = this.callbacks, r.label = 1;case 1:\n            return t < n.length ? [4, n[t].onTrainEnd(e)] : [3, 4];case 2:\n            r.sent(), r.label = 3;case 3:\n            return t++, [3, 1];case 4:\n            return [2];}\n      });\n    });\n  }, e;\n}(),\n    ModelTrainingYielder = function () {\n  function e(e) {\n    this.yieldEvery = e, this.batchCount = 0, this.batchDurationsMillis = [], this.autoYieldEveryBatches = null, this.batchStartMillis = _tfjsCore.util.now();\n  }return e.prototype.resolveOneTensorInLogs = function (e) {\n    return __awaiter(this, void 0, void 0, function () {\n      var t, n, r, i, a;return __generator(this, function (o) {\n        switch (o.label) {case 0:\n            for (n in t = [], e) t.push(n);r = 0, o.label = 1;case 1:\n            return r < t.length ? (i = t[r], \"number\" == typeof (a = e[i]) ? [3, 3] : [4, a.data()]) : [3, 4];case 2:\n            return o.sent(), [3, 4];case 3:\n            return r++, [3, 1];case 4:\n            return [2];}\n      });\n    });\n  }, e.prototype.maybeYieldOnBatch = function (t) {\n    return __awaiter(this, void 0, void 0, function () {\n      var n, r;return __generator(this, function (i) {\n        switch (i.label) {case 0:\n            return \"auto\" !== this.yieldEvery ? [3, 7] : (this.batchCount++, null != this.autoYieldEveryBatches ? [3, 3] : [4, this.resolveOneTensorInLogs(t)]);case 1:\n            return i.sent(), n = _tfjsCore.util.now(), [4, (0, _tfjsCore.nextFrame)()];case 2:\n            return i.sent(), this.batchCount > e.SKIP_FIRST_BATCHES && (this.batchDurationsMillis.push(n - this.batchStartMillis), this.batchDurationsMillis.length >= e.DECISION_BATCH_COUNT && (r = this.batchDurationsMillis.reduce(function (e, t) {\n              return e + t;\n            }) / this.batchDurationsMillis.length, this.autoYieldEveryBatches = Math.round(e.THRESHOLD_MILLIS / r), this.autoYieldEveryBatches < 1 && (this.autoYieldEveryBatches = 1))), this.batchStartMillis = _tfjsCore.util.now(), this.lastYieldBatchCount = this.batchCount, [3, 6];case 3:\n            return this.batchCount - this.lastYieldBatchCount >= this.autoYieldEveryBatches ? [4, (0, _tfjsCore.nextFrame)()] : [3, 6];case 4:\n            return i.sent(), [4, this.resolveOneTensorInLogs(t)];case 5:\n            i.sent(), this.lastYieldBatchCount = this.batchCount, i.label = 6;case 6:\n            return [3, 9];case 7:\n            return \"batch\" !== this.yieldEvery ? [3, 9] : [4, (0, _tfjsCore.nextFrame)()];case 8:\n            i.sent(), i.label = 9;case 9:\n            return [2];}\n      });\n    });\n  }, e.prototype.maybeYieldOnEpoch = function () {\n    return __awaiter(this, void 0, void 0, function () {\n      return __generator(this, function (e) {\n        switch (e.label) {case 0:\n            return \"epoch\" !== this.yieldEvery ? [3, 2] : [4, (0, _tfjsCore.nextFrame)()];case 1:\n            e.sent(), e.label = 2;case 2:\n            return [2];}\n      });\n    });\n  }, e.SKIP_FIRST_BATCHES = 1, e.DECISION_BATCH_COUNT = 2, e.THRESHOLD_MILLIS = 16, e;\n}(),\n    BaseLogger = function (e) {\n  function t(t) {\n    var n = e.call(this) || this;return n.yieldEvery = t || \"auto\", n;\n  }return __extends(t, e), t.prototype.onTrainBegin = function (e) {\n    return __awaiter(this, void 0, void 0, function () {\n      return __generator(this, function (e) {\n        return this.autoYielder = new ModelTrainingYielder(this.yieldEvery), [2];\n      });\n    });\n  }, t.prototype.onEpochBegin = function (e) {\n    return __awaiter(this, void 0, void 0, function () {\n      return __generator(this, function (e) {\n        return this.seen = 0, this.totals = {}, [2];\n      });\n    });\n  }, t.prototype.onBatchEnd = function (e, t) {\n    return __awaiter(this, void 0, void 0, function () {\n      var e,\n          n,\n          r,\n          i,\n          a = this;return __generator(this, function (o) {\n        switch (o.label) {case 0:\n            return [4, this.autoYielder.maybeYieldOnBatch(t)];case 1:\n            for (i in o.sent(), null == t && (t = {}), e = null == t.size ? 0 : t.size, this.seen += e, n = function (n) {\n              var i = t[n];if (\"number\" == typeof i) r.totals.hasOwnProperty(n) || (r.totals[n] = 0), r.totals[n] = r.totals[n] + i * e;else {\n                var o = void 0;n in r.totals ? o = r.totals[n] : r.totals[n] = getScalar(0), r.totals[n] = (0, _tfjsCore.tidy)(function () {\n                  return (0, _tfjsCore.add)(a.totals[n], (0, _tfjsCore.mul)(i, getScalar(e)));\n                }), null != o && o.dispose();\n              }\n            }, r = this, t) n(i);return [2];}\n      });\n    });\n  }, t.prototype.onEpochEnd = function (e, t) {\n    return __awaiter(this, void 0, void 0, function () {\n      var e,\n          n,\n          r,\n          i,\n          a,\n          o = this;return __generator(this, function (s) {\n        switch (s.label) {case 0:\n            return [4, this.autoYielder.maybeYieldOnEpoch()];case 1:\n            if (s.sent(), null != t) for (e = function (e) {\n              if (null == n.totals[e]) return \"continue\";\"number\" == typeof n.totals[e] ? t[e] = n.totals[e] / n.seen : (0, _tfjsCore.tidy)(function () {\n                t[e] = (0, _tfjsCore.mul)((0, _tfjsCore.div)(getScalar(1), getScalar(o.seen)), o.totals[e]), o.totals[e].dispose(), (0, _tfjsCore.keep)(t[e]);\n              });\n            }, n = this, r = 0, i = this.params.metrics; r < i.length; r++) a = i[r], e(a);return [2];}\n      });\n    });\n  }, t;\n}(BaseCallback),\n    History = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }return __extends(t, e), t.prototype.onTrainBegin = function (e) {\n    return __awaiter(this, void 0, void 0, function () {\n      return __generator(this, function (e) {\n        return this.epoch = [], this.history = {}, [2];\n      });\n    });\n  }, t.prototype.onEpochEnd = function (e, t) {\n    return __awaiter(this, void 0, void 0, function () {\n      var n;return __generator(this, function (r) {\n        for (n in null == t && (t = {}), this.epoch.push(e), t) null == this.history[n] && (this.history[n] = []), this.history[n].push(t[n]);return [2];\n      });\n    });\n  }, t.prototype.syncData = function () {\n    return __awaiter(this, void 0, void 0, function () {\n      var e, t, n, r, i, a, o, s, l;return __generator(this, function (u) {\n        switch (u.label) {case 0:\n            for (r in e = [], t = [], n = [], this.history) for (i = this.history[r], a = 0; a < i.length; ++a) \"number\" != typeof i[a] && (o = i[a], e.push(o.data()), t.push(r), n.push(a));return [4, Promise.all(e)];case 1:\n            for (s = u.sent(), l = 0; l < s.length; ++l) this.history[t[l]][n[l]].dispose(), this.history[t[l]][n[l]] = s[l][0];return [2];}\n      });\n    });\n  }, t;\n}(BaseCallback),\n    CustomCallback = function (e) {\n  function t(t) {\n    var n = e.call(this) || this;return n.trainBegin = t.onTrainBegin, n.trainEnd = t.onTrainEnd, n.epochBegin = t.onEpochBegin, n.epochEnd = t.onEpochEnd, n.batchBegin = t.onBatchBegin, n.batchEnd = t.onBatchEnd, n;\n  }return __extends(t, e), t.prototype.onEpochBegin = function (e, t) {\n    return __awaiter(this, void 0, void 0, function () {\n      return __generator(this, function (n) {\n        switch (n.label) {case 0:\n            return null == this.epochBegin ? [3, 3] : [4, resolveScalarsInLogs(t)];case 1:\n            return n.sent(), [4, this.epochBegin(e, t)];case 2:\n            n.sent(), n.label = 3;case 3:\n            return [2];}\n      });\n    });\n  }, t.prototype.onEpochEnd = function (e, t) {\n    return __awaiter(this, void 0, void 0, function () {\n      return __generator(this, function (n) {\n        switch (n.label) {case 0:\n            return null == this.epochEnd ? [3, 3] : [4, resolveScalarsInLogs(t)];case 1:\n            return n.sent(), [4, this.epochEnd(e, t)];case 2:\n            n.sent(), n.label = 3;case 3:\n            return [2];}\n      });\n    });\n  }, t.prototype.onBatchBegin = function (e, t) {\n    return __awaiter(this, void 0, void 0, function () {\n      return __generator(this, function (n) {\n        switch (n.label) {case 0:\n            return null == this.batchBegin ? [3, 3] : [4, resolveScalarsInLogs(t)];case 1:\n            return n.sent(), [4, this.batchBegin(e, t)];case 2:\n            n.sent(), n.label = 3;case 3:\n            return [2];}\n      });\n    });\n  }, t.prototype.onBatchEnd = function (e, t) {\n    return __awaiter(this, void 0, void 0, function () {\n      return __generator(this, function (n) {\n        switch (n.label) {case 0:\n            return null == this.batchEnd ? [3, 3] : [4, resolveScalarsInLogs(t)];case 1:\n            return n.sent(), [4, this.batchEnd(e, t)];case 2:\n            n.sent(), n.label = 3;case 3:\n            return [2];}\n      });\n    });\n  }, t.prototype.onTrainBegin = function (e) {\n    return __awaiter(this, void 0, void 0, function () {\n      return __generator(this, function (t) {\n        switch (t.label) {case 0:\n            return null == this.trainBegin ? [3, 3] : [4, resolveScalarsInLogs(e)];case 1:\n            return t.sent(), [4, this.trainBegin(e)];case 2:\n            t.sent(), t.label = 3;case 3:\n            return [2];}\n      });\n    });\n  }, t.prototype.onTrainEnd = function (e) {\n    return __awaiter(this, void 0, void 0, function () {\n      return __generator(this, function (t) {\n        switch (t.label) {case 0:\n            return null == this.trainEnd ? [3, 3] : [4, resolveScalarsInLogs(e)];case 1:\n            return t.sent(), [4, this.trainEnd(e)];case 2:\n            t.sent(), t.label = 3;case 3:\n            return [2];}\n      });\n    });\n  }, t;\n}(BaseCallback);function standardizeCallbacks(e) {\n  return null == e ? null : e instanceof BaseCallback ? [e] : Array.isArray(e) && e[0] instanceof BaseCallback ? e : toList(e).map(function (e) {\n    return new CustomCallback(e);\n  });\n}var CallbackConstructorRegistry = function () {\n  function e() {}return e.registerCallbackConstructor = function (t, n) {\n    _tfjsCore.util.assert(t >= 0 && Number.isInteger(t), \"Verbosity level is expected to be an integer >= 0, but got \" + t), e.checkForDuplicate(n), null == e.constructors[t] && (e.constructors[t] = []), e.constructors[t].push(n);\n  }, e.checkForDuplicate = function (t) {\n    for (var n in e.constructors) {\n      e.constructors[+n].forEach(function (e) {\n        if (e === t) throw new ValueError(\"Duplicate callback constructor.\");\n      });\n    }\n  }, e.clear = function () {\n    e.constructors = {};\n  }, e.createCallbacks = function (t) {\n    var n = [];for (var r in e.constructors) {\n      var i = +r;t >= i && n.push.apply(n, e.constructors[i]);\n    }return n.map(function (e) {\n      return new e();\n    });\n  }, e;\n}();function l2Normalize(e, t) {\n  return (0, _tfjsCore.tidy)(function () {\n    var n = (0, _tfjsCore.sum)(square(e), t, !0),\n        r = (0, _tfjsCore.mul)((0, _tfjsCore.scalar)(epsilon()), (0, _tfjsCore.onesLike)(e)),\n        i = (0, _tfjsCore.sqrt)((0, _tfjsCore.maximum)(n, r));return (0, _tfjsCore.div)(e, i);\n  });\n}function meanSquaredError(e, t) {\n  return (0, _tfjsCore.tidy)(function () {\n    return (0, _tfjsCore.mean)(square((0, _tfjsCore.sub)(t, e)), -1);\n  });\n}function meanAbsoluteError(e, t) {\n  return (0, _tfjsCore.tidy)(function () {\n    return (0, _tfjsCore.mean)((0, _tfjsCore.abs)((0, _tfjsCore.sub)(t, e)), -1);\n  });\n}function meanAbsolutePercentageError(e, t) {\n  return (0, _tfjsCore.tidy)(function () {\n    var n = (0, _tfjsCore.sub)(e, t),\n        r = (0, _tfjsCore.clipByValue)((0, _tfjsCore.abs)(e), epsilon(), Number.MAX_VALUE),\n        i = (0, _tfjsCore.abs)((0, _tfjsCore.div)(n, r));return (0, _tfjsCore.mul)(getScalar(100), (0, _tfjsCore.mean)(i, -1));\n  });\n}function meanSquaredLogarithmicError(e, t) {\n  return (0, _tfjsCore.tidy)(function () {\n    var n = getScalar(1),\n        r = (0, _tfjsCore.clipByValue)(t, epsilon(), Number.MAX_VALUE),\n        i = (0, _tfjsCore.log)((0, _tfjsCore.add)(n, r)),\n        a = (0, _tfjsCore.clipByValue)(e, epsilon(), Number.MAX_VALUE),\n        o = (0, _tfjsCore.log)((0, _tfjsCore.add)(n, a));return (0, _tfjsCore.mean)(square((0, _tfjsCore.sub)(i, o)), -1);\n  });\n}function squaredHinge(e, t) {\n  return (0, _tfjsCore.tidy)(function () {\n    var n = getScalar(0),\n        r = getScalar(1),\n        i = (0, _tfjsCore.maximum)(n, (0, _tfjsCore.sub)(r, (0, _tfjsCore.mul)(e, t)));return (0, _tfjsCore.mean)(square(i), -1);\n  });\n}function hinge(e, t) {\n  return (0, _tfjsCore.tidy)(function () {\n    var n = getScalar(0),\n        r = getScalar(1),\n        i = (0, _tfjsCore.maximum)(n, (0, _tfjsCore.sub)(r, (0, _tfjsCore.mul)(e, t)));return (0, _tfjsCore.mean)(i, -1);\n  });\n}function categoricalHinge(e, t) {\n  return (0, _tfjsCore.tidy)(function () {\n    var n = getScalar(0),\n        r = getScalar(1),\n        i = (0, _tfjsCore.sum)((0, _tfjsCore.mul)(e, t), -1),\n        a = (0, _tfjsCore.max)((0, _tfjsCore.mul)((0, _tfjsCore.sub)(r, e), t), -1);return (0, _tfjsCore.maximum)(n, (0, _tfjsCore.add)(r, (0, _tfjsCore.sub)(a, i)));\n  });\n}function logcosh(e, t) {\n  return (0, _tfjsCore.tidy)(function () {\n    var n = getScalar(Math.log(2)),\n        r = (0, _tfjsCore.sub)(t, e),\n        i = (0, _tfjsCore.sub)((0, _tfjsCore.add)(r, (0, _tfjsCore.softplus)((0, _tfjsCore.mul)(getScalar(-2), r))), n);return (0, _tfjsCore.mean)(i, -1);\n  });\n}function categoricalCrossentropy(e, t, n) {\n  return void 0 === n && (n = !1), (0, _tfjsCore.tidy)(function () {\n    if (n) t = (0, _tfjsCore.softmax)(t);else {\n      var r = (0, _tfjsCore.sum)(t, t.shape.length - 1, !0);t = (0, _tfjsCore.div)(t, r);\n    }return t = (0, _tfjsCore.clipByValue)(t, epsilon(), 1 - epsilon()), (0, _tfjsCore.neg)((0, _tfjsCore.sum)((0, _tfjsCore.mul)(e.toFloat(), (0, _tfjsCore.log)(t)), t.shape.length - 1));\n  });\n}function sparseCategoricalCrossentropy(e, t, n) {\n  return void 0 === n && (n = !1), (0, _tfjsCore.tidy)(function () {\n    var r = (0, _tfjsCore.floor)(flatten(e)).toInt(),\n        i = t.shape;return categoricalCrossentropy((0, _tfjsCore.oneHot)(r, i[i.length - 1]).reshape(i), t, n);\n  });\n}function sigmoidCrossEntropyWithLogits(e, t) {\n  return (0, _tfjsCore.tidy)(function () {\n    var n = (0, _tfjsCore.maximum)(t, (0, _tfjsCore.zerosLike)(t)),\n        r = (0, _tfjsCore.mul)(t, e),\n        i = (0, _tfjsCore.log)((0, _tfjsCore.add)(getScalar(1), (0, _tfjsCore.exp)((0, _tfjsCore.neg)((0, _tfjsCore.abs)(t)))));return (0, _tfjsCore.add)((0, _tfjsCore.sub)(n, r), i);\n  });\n}function binaryCrossentropy(e, t) {\n  return (0, _tfjsCore.tidy)(function () {\n    var n;return n = (0, _tfjsCore.clipByValue)(t, epsilon(), 1 - epsilon()), n = (0, _tfjsCore.log)((0, _tfjsCore.div)(n, (0, _tfjsCore.sub)((0, _tfjsCore.onesLike)(n), n))), (0, _tfjsCore.mean)(sigmoidCrossEntropyWithLogits(e, n), -1);\n  });\n}function kullbackLeiblerDivergence(e, t) {\n  return (0, _tfjsCore.tidy)(function () {\n    var n = (0, _tfjsCore.clipByValue)(e, epsilon(), 1),\n        r = (0, _tfjsCore.clipByValue)(t, epsilon(), 1);return (0, _tfjsCore.sum)((0, _tfjsCore.mul)(e, (0, _tfjsCore.log)((0, _tfjsCore.div)(n, r))), -1);\n  });\n}function poisson(e, t) {\n  return (0, _tfjsCore.tidy)(function () {\n    var n = (0, _tfjsCore.log)((0, _tfjsCore.add)(getScalar(epsilon()), t));return (0, _tfjsCore.mean)((0, _tfjsCore.sub)(t, (0, _tfjsCore.mul)(e, n)), -1);\n  });\n}function cosineProximity(e, t) {\n  return (0, _tfjsCore.tidy)(function () {\n    var n = l2Normalize(e, -1),\n        r = l2Normalize(t, -1),\n        i = (0, _tfjsCore.mul)(n, r);return (0, _tfjsCore.neg)((0, _tfjsCore.sum)(i, -1));\n  });\n}function get(e) {\n  var t = { meanSquaredError: meanSquaredError, meanAbsoluteError: meanAbsoluteError, meanAbsolutePercentageError: meanAbsolutePercentageError, meanSquaredLogarithmicError: meanSquaredLogarithmicError, squaredHinge: squaredHinge, hinge: hinge, categoricalHinge: categoricalHinge, logcosh: logcosh, categoricalCrossentropy: categoricalCrossentropy, sparseCategoricalCrossentropy: sparseCategoricalCrossentropy, binaryCrossentropy: binaryCrossentropy, kullbackLeiblerDivergence: kullbackLeiblerDivergence, poisson: poisson, cosineProximity: cosineProximity };if (\"string\" == typeof e) {\n    if (e in t) return t[e];var n = \"Unknown loss \" + e;throw e.toLowerCase().includes(\"softmaxcrossentropy\") && (n = \"Unknown loss \" + e + '. Use \"categoricalCrossentropy\" as the string name for tf.losses.softmaxCrossEntropy'), new ValueError(n);\n  }return e;\n}function binaryAccuracy(e, t) {\n  return (0, _tfjsCore.tidy)(function () {\n    var n = (0, _tfjsCore.mul)(getScalar(.5), (0, _tfjsCore.onesLike)(t)),\n        r = cast$1((0, _tfjsCore.greater)(t, n), e.dtype);return (0, _tfjsCore.mean)((0, _tfjsCore.equal)(e, r), -1);\n  });\n}function categoricalAccuracy(e, t) {\n  return (0, _tfjsCore.tidy)(function () {\n    return cast$1((0, _tfjsCore.equal)((0, _tfjsCore.argMax)(e, -1), (0, _tfjsCore.argMax)(t, -1)), \"float32\");\n  });\n}function truePositives(e, t) {\n  return (0, _tfjsCore.tidy)(function () {\n    var n = getScalar(1);return (0, _tfjsCore.logicalAnd)(e.equal(n), t.equal(n)).sum().cast(\"float32\");\n  });\n}function falseNegatives(e, t) {\n  return (0, _tfjsCore.tidy)(function () {\n    var n = getScalar(1),\n        r = getScalar(0);return (0, _tfjsCore.logicalAnd)(e.equal(n), t.equal(r)).sum().cast(\"float32\");\n  });\n}function falsePositives(e, t) {\n  return (0, _tfjsCore.tidy)(function () {\n    var n = getScalar(1),\n        r = getScalar(0);return (0, _tfjsCore.logicalAnd)(e.equal(r), t.equal(n)).sum().cast(\"float32\");\n  });\n}function precision(e, t) {\n  return (0, _tfjsCore.tidy)(function () {\n    var n = getScalar(0),\n        r = truePositives(e, t),\n        i = falsePositives(e, t),\n        a = r.add(i);return (0, _tfjsCore.where)((0, _tfjsCore.greater)(a, n), r.div(a), n).cast(\"float32\");\n  });\n}function recall(e, t) {\n  return (0, _tfjsCore.tidy)(function () {\n    var n = getScalar(0),\n        r = truePositives(e, t),\n        i = falseNegatives(e, t),\n        a = r.add(i);return (0, _tfjsCore.where)((0, _tfjsCore.greater)(a, n), r.div(a), n).cast(\"float32\");\n  });\n}function binaryCrossentropy$1(e, t) {\n  return binaryCrossentropy(e, t);\n}function sparseCategoricalAccuracy(e, t) {\n  throw new NotImplementedError();\n}var mse$1 = meanSquaredError,\n    MSE$1 = meanSquaredError,\n    mae$1 = meanAbsoluteError,\n    MAE$1 = meanAbsoluteError,\n    mape$1 = meanAbsolutePercentageError,\n    MAPE$1 = meanAbsolutePercentageError,\n    categoricalCrossentropy$1 = categoricalCrossentropy,\n    cosine$1 = cosineProximity,\n    sparseCategoricalCrossentropy$1 = sparseCategoricalCrossentropy;function get$1(e) {\n  var t = { binaryAccuracy: binaryAccuracy, categoricalAccuracy: categoricalAccuracy, precision: precision, categoricalCrossentropy: categoricalCrossentropy$1, sparseCategoricalCrossentropy: sparseCategoricalCrossentropy$1, mse: mse$1, MSE: MSE$1, mae: mae$1, MAE: MAE$1, mape: mape$1, MAPE: MAPE$1, cosine: cosine$1 };if (\"string\" == typeof e && e in t) return t[e];if (\"string\" != typeof e && null != e) return e;throw new ValueError(\"Unknown metric \" + e);\n}function getOptimizer(e) {\n  var t = { Adagrad: function () {\n      return _tfjsCore.train.adagrad(.01);\n    }, Adadelta: function () {\n      return _tfjsCore.train.adadelta(1, .95, epsilon());\n    }, Adam: function () {\n      return _tfjsCore.train.adam(.001, .9, .999, epsilon());\n    }, Adamax: function () {\n      return _tfjsCore.train.adamax(.002, .9, .999, epsilon(), 0);\n    }, RMSProp: function () {\n      return _tfjsCore.train.rmsprop(.001, .9, 0, epsilon());\n    }, SGD: function () {\n      return _tfjsCore.train.sgd(.01);\n    } };if (t.adagrad = t.Adagrad, t.adadelta = t.Adadelta, t.adam = t.Adam, t.adamax = t.Adamax, t.rmsprop = t.RMSProp, t.sgd = t.SGD, e in t) return t[e]();throw new ValueError(\"Unknown Optimizer \" + e);\n}function printSummary(e, t, n, r) {\n  void 0 === r && (r = console.log);var i,\n      a = isModelSequentialLike(e),\n      o = [\"Layer (type)\", \"Output shape\", \"Param #\"];if (a ? (t = t || 65, n = n || [.45, .85, 1]) : (t = t || 98, n = n || [.33, .55, .67, 1]), n[n.length - 1] <= 1 && (n = n.map(function (e) {\n    return Math.floor(t * e);\n  })), !a) for (var s in o.push(\"Receives inputs\"), i = [], e.nodesByDepth) i.push.apply(i, e.nodesByDepth[s]);r(\"_\".repeat(t)), printRow(o, n, r), r(\"=\".repeat(t));for (var l = e.layers, u = 0; u < l.length; ++u) a ? printLayerSummary(l[u], n, r) : printLayerSummaryWithConnections(l[u], n, i, r), r((u === l.length - 1 ? \"=\" : \"_\").repeat(t));e.checkTrainableWeightsConsistency();var c = countTrainableParams(e),\n      p = countParamsInWeights(e.nonTrainableWeights);r(\"Total params: \" + (c + p)), r(\"Trainable params: \" + c), r(\"Non-trainable params: \" + p), r(\"_\".repeat(t));\n}function countTrainableParams(e) {\n  return null != e.collectedTrainableWeights ? countParamsInWeights(e.collectedTrainableWeights) : countParamsInWeights(e.trainableWeights);\n}function isModelSequentialLike(e) {\n  var t = !0,\n      n = [],\n      r = [];for (var i in e.nodesByDepth) n.push(e.nodesByDepth[i]);for (var a = 0, o = n; a < o.length; a++) {\n    var s = o[a];if (s.length > 1 || 1 === s.length && s[0].inboundLayers.length > 1) {\n      t = !1;break;\n    }r.push.apply(r, s);\n  }if (t) for (var l = 0, u = e.layers; l < u.length; l++) {\n    for (var c = !1, p = 0, h = u[l].inboundNodes; p < h.length; p++) {\n      var d = h[p];if (-1 !== r.indexOf(d)) {\n        if (c) {\n          t = !1;break;\n        }c = !0;\n      }\n    }if (!t) break;\n  }return t;\n}function printRow(e, t, n) {\n  void 0 === n && (n = console.log);for (var r = \"\", i = 0; i < e.length; ++i) i > 0 && (r = r.slice(0, r.length - 1) + \" \"), r = (r += e[i]).slice(0, t[i]), r += \" \".repeat(t[i] - r.length);n(r);\n}function printLayerSummary(e, t, n) {\n  var r;try {\n    r = JSON.stringify(e.outputShape);\n  } catch (e) {\n    r = \"multiple\";\n  }printRow([e.name + \" (\" + e.getClassName() + \")\", r, e.countParams().toString()], t, n);\n}function printLayerSummaryWithConnections(e, t, n, r) {\n  var i;try {\n    i = JSON.stringify(e.outputShape);\n  } catch (e) {\n    i = \"multiple\";\n  }for (var a = [], o = 0, s = e.inboundNodes; o < s.length; o++) {\n    var l = s[o];if (!(null != n && n.length > 0 && -1 === n.indexOf(l))) for (var u = 0; u < l.inboundLayers.length; ++u) {\n      var c = l.inboundLayers[u].name,\n          p = l.nodeIndices[u],\n          h = l.tensorIndices[u];a.push(c + \"[\" + p + \"][\" + h + \"]\");\n    }\n  }var d = e.name,\n      g = e.getClassName(),\n      f = 0 === a.length ? \"\" : a[0];printRow([d + \" (\" + g + \")\", i, e.countParams().toString(), f], t, r);for (u = 1; u < a.length; ++u) printRow([\"\", \"\", \"\", a[u]], t, r);\n}function deserialize(e, t) {\n  return void 0 === t && (t = {}), deserializeKerasObject(e, _tfjsCore.serialization.SerializationMap.getMap().classNameMap, t, \"layer\");\n}function isArrayItemInputOrOutputName(e, t, n) {\n  return (\"inboundNodes\" === e || \"outputLayers\" === e || \"inputLayers\" === e) && 0 === t && \"string\" == typeof n;\n}function convertPythonicToTs(e, t) {\n  if (null === e) return null;if (\"string\" == typeof e) return toCamelCase(e);if (\"number\" == typeof e || \"boolean\" == typeof e) return e;if (e instanceof Array) {\n    for (var n = [], r = e.length, i = 0; i < r; ++i) {\n      var a = e[i];isArrayItemInputOrOutputName(t, i, a) ? n.push(a) : n.push(convertPythonicToTs(a, t));\n    }return n;\n  }for (var o = {}, s = 0, l = Object.keys(e); s < l.length; s++) {\n    var u = l[s],\n        c = e[u];if (\"name\" === u && \"string\" == typeof c) o[u] = c;else {\n      var p = toCamelCase(u);o[p] = convertPythonicToTs(c, p);\n    }\n  }return o;\n}function convertTsToPythonic(e, t) {\n  if (null === e || void 0 === e) return null;if (\"string\" == typeof e) return toSnakeCase(e);if (\"number\" == typeof e || \"boolean\" == typeof e) return e;if (e instanceof Array) {\n    for (var n = [], r = e.length, i = 0; i < r; ++i) {\n      var a = e[i];isArrayItemInputOrOutputName(t, i, a) ? n.push(a) : n.push(convertTsToPythonic(a, t));\n    }return n;\n  }for (var o = {}, s = 0, l = Object.keys(e); s < l.length; s++) {\n    var u = l[s],\n        c = e[u],\n        p = toSnakeCase(u);o[p] = \"name\" !== u && \"className\" !== u || \"string\" != typeof c ? convertTsToPythonic(c, u) : c;\n  }return o;\n}var version = \"0.8.0\";function preprocessWeightsForLoading(e, t, n, r) {\n  if (!n.startsWith(\"2.\")) throw new ValueError(\"Unsupported Keras version in weights being loaded: \" + n);return t;\n}function loadTensor(e, t, n) {\n  var r = stringToDType(e);return _tfjsCore.Tensor.make(t, { values: 0 === t.length ? n : _tfjsCore.util.flatten(n) }, r);\n}function loadWeightsFromJson(e, t, n) {\n  void 0 === n && (n = !1);for (var r = e.keras_version, i = e.backend, a = t.map(function (e) {\n    return e.name;\n  }), o = {}, s = 0, l = t; s < l.length; s++) {\n    null != (b = l[s]).name && (null == o[b.name] && (o[b.name] = []), o[b.name].push(b));\n  }for (var u = e.weights, c = [], p = 0; p < a.length; ++p) {\n    var h = a[p],\n        d = u[h];null == d && (d = []);for (var g = [], f = 0; f < d.length; ++f) {\n      var m = d[f];g.push(new LayerVariable(loadTensor(m.dtype, m.shape, m.value)));\n    }for (var y = 0, v = o[h]; y < v.length; y++) {\n      var b,\n          w = (b = v[y]).weights;if ((g = preprocessWeightsForLoading(b, g, r, i)).length !== w.length) {\n        if (!n) throw new ValueError(\"Layer #\" + p + ' (named \"' + b.name + '\") expects ' + w.length + \" weight(s), but the saved weights have \" + g.length + \" element(s).\");console.warn(\"Skipping loading of weights of layer \" + b.name + \" due to mismatch in number of weights: (\" + g.length + \" vs \" + w.length + \").\");\n      }for (var z = 0; z < g.length; ++z) !n || _tfjsCore.util.arraysEqual(w[z].shape, g[z].shape) ? c.push([w[z], g[z].read()]) : console.warn(\"Skipping loading of weights for layer \" + b.name + \" due to mismatch in shape (\" + w[z].shape + \" vs \" + g[z].shape + \")\");\n    }\n  }batchSetValue(c);\n}function loadWeightsFromNamedTensorMap(e, t, n) {\n  void 0 === n && (n = !0);for (var r = {}, i = 0, a = 0, o = t; a < o.length; a++) for (var s = 0, l = o[a].weights; s < l.length; s++) {\n    var u = l[s];if (null != r[u.originalName]) throw new ValueError(\"Duplicate weight name: \" + u.originalName);r[u.originalName] = u, i++;\n  }var c = [];for (var p in e) {\n    if (null != r[p]) c.push([r[p], e[p]]);else if (n) throw new ValueError(\"Provided weight data has no target variable: \" + p);delete r[p];\n  }if (n) {\n    var h = [];for (var d in r) h.push(d);if (h.length > 0) throw new ValueError(h.length + \" of \" + i + \" weights are not set: \" + h);\n  }batchSetValue(c);\n}var Container = function (e) {\n  function t(n) {\n    var r = e.call(this, {}) || this;if (r.containerNodes = new Set(), r.name = n.name, null == r.name) {\n      var i = r.getClassName().toLowerCase();r.name = getUid(i);\n    }if (r.supportsMasking = !1, r.trainable = !0, r.updatable = !0, Array.isArray(n.inputs) ? r.inputs = n.inputs.slice() : r.inputs = [n.inputs], Array.isArray(n.outputs) ? r.outputs = n.outputs.slice() : r.outputs = [n.outputs], unique(r.inputs).length !== r.inputs.length) throw new ValueError(\"The list of inputs passed to the model is redundant. All inputs should only appear once. Found: \" + r.inputs.map(function (e) {\n      return e.name;\n    }));unique(r.outputs).length !== r.outputs.length && console.warn(\"The list of outputs passed to the model is redundant. All outputs should only appear once. Found: \" + r.outputs.map(function (e) {\n      return e.name;\n    })), r.inputLayers = [], r.inputLayersNodeIndices = [], r.inputLayersTensorIndices = [], r.outputLayers = [], r.outputLayersNodeIndices = [], r.outputLayersTensorIndices = [], r.layers = [];for (var a = 0, o = r.outputs; a < o.length; a++) {\n      var s = (N = o[a]).sourceLayer,\n          l = N.nodeIndex,\n          u = N.tensorIndex;r.outputLayers.push(s), r.outputLayersNodeIndices.push(l), r.outputLayersTensorIndices.push(u);\n    }for (var c = 0, p = r.inputs; c < p.length; c++) {\n      s = (N = p[c]).sourceLayer, l = N.nodeIndex, u = N.tensorIndex;assert(0 === l, \"input layer has >1 nodes\"), assert(0 === u, \"input layer has >1 tensors\"), r.inputLayers.push(s), r.inputLayersNodeIndices.push(l), r.inputLayersTensorIndices.push(u);\n    }r.inputNames = [], r.outputNames = [], r.feedInputShapes = [], r.feedInputNames = [], r.feedOutputNames = [];for (var h = 0; h < r.inputLayers.length; h++) {\n      if (!((s = r.inputLayers[h]) instanceof InputLayer)) throw new TypeError(\"Input layers to a Model must be InputLayer objects. Received inputs: \" + n.inputs + \". Input \" + h + \" (0-based) originates from layer type \" + s.getClassName() + \".\");r.inputNames.push(s.name), r.feedInputShapes.push(s.batchInputShape), r.feedInputNames.push(s.name);\n    }for (var d = 0, g = r.outputLayers; d < g.length; d++) {\n      s = g[d];r.outputNames.push(s.name);\n    }r.internalInputShapes = r.inputs.map(function (e) {\n      return e.shape;\n    }), r.internalOutputShapes = r.outputs.map(function (e) {\n      return e.shape;\n    });for (var f = {}, m = {}, y = {}, v = {}, b = {}, w = [], z = function (e, n, i, a, o, s) {\n      null != a && null != o && null != s || (a = e.sourceLayer, o = e.nodeIndex, s = e.tensorIndex);var l = a.inboundNodes[o];if (-1 !== i.indexOf(l)) throw new RuntimeError(\"The tensor \" + e.name + ' at layer \"' + a.name + '\" is part of a cycle.');if (-1 === n.indexOf(l)) {\n        r.containerNodes.add(t.nodeKey(a, o)), (a.id in b) || (b[a.id] = Object.keys(b).length), -1 === i.indexOf(l) && i.push(l);for (var u = l.inboundLayers.length, c = 0; c < u; c++) {\n          var p = l.inputTensors[c],\n              h = l.inboundLayers[c],\n              d = l.nodeIndices[c],\n              g = l.tensorIndices[c];z(p, n, i, h, d, g);\n        }for (n.push(l); i.indexOf(l) >= 0;) i.splice(i.indexOf(l), 1);w.push(l);\n      }\n    }, S = [], A = [], I = 0, C = r.outputs; I < C.length; I++) {\n      var N = C[I];z(N, S, A);\n    }for (var E = 0, _ = w.slice().reverse(); E < _.length; E++) {\n      m[(K = _[E]).id] = K, K.id in f || (f[K.id] = 0);var k = f[K.id],\n          L = null == y[K.outboundLayer.id] ? 0 : y[K.outboundLayer.id];k = Math.max(k, L), y[K.outboundLayer.id] = k, v[K.outboundLayer.id] = K.outboundLayer, f[K.id] = k;for (h = 0; h < K.inboundLayers.length; h++) {\n        var x = K.inboundLayers[h],\n            T = (l = K.nodeIndices[h], x.inboundNodes[l]),\n            R = null == f[T.id] ? 0 : f[T.id];f[T.id] = Math.max(k + 1, R), m[T.id] = T;\n      }\n    }var O = {};for (var D in f) {\n      (k = f[D]) in O || (O[k] = []), O[k].push(m[D]);\n    }var M = {};for (var P in y) {\n      (k = y[P]) in M || (M[k] = []), M[k].push(v[P]);\n    }var V = Object.keys(M).map(function (e) {\n      return parseInt(e, 10);\n    }).sort(reverseNumberCompare);r.layers = [];for (var F = 0, B = V; F < B.length; F++) {\n      var U = M[k = B[F]];U.sort(function (e, t) {\n        var n = b[e.id],\n            r = b[t.id];return n < r ? -1 : n > r ? 1 : 0;\n      });for (var j = 0, W = U; j < W.length; j++) {\n        s = W[j];r.layers.push(s);\n      }\n    }r.layersByDepth = M, V = Object.keys(O).map(function (e) {\n      return parseInt(e, 10);\n    }).sort(reverseNumberCompare);for (var $ = r.inputs.slice(), q = [], G = 0, J = V; G < J.length; G++) for (var H = 0, Z = O[k = J[G]]; H < Z.length; H++) {\n      var K;if (null != (s = (K = Z[H]).outboundLayer)) {\n        for (var Y = 0, X = K.inputTensors; Y < X.length; Y++) {\n          N = X[Y];if (-1 === $.indexOf(N)) throw new RuntimeError(\"Graph disconnected: cannot obtain value for tensor \" + N + ' at layer \"' + s.name + '\". The following previous layers were accessed without issue: ' + q);\n        }for (var Q = 0, ee = K.outputTensors; Q < ee.length; Q++) {\n          N = ee[Q];$.push(N);\n        }q.push(s.name);\n      }\n    }r.nodesByDepth = O;for (var te = r.layers.map(function (e) {\n      return e.name;\n    }), ne = function (e) {\n      var t = te.filter(function (t) {\n        return t === e;\n      }).length;if (1 !== t) throw new RuntimeError('The name \"' + e + '\" is used ' + t + \" times in the model. All layer names should be unique. Layer names: \" + JSON.stringify(te));\n    }, re = 0, ie = te; re < ie.length; re++) {\n      ne(ie[re]);\n    }return r.outboundNodes = [], r.inboundNodes = [], new Node({ outboundLayer: r, inboundLayers: [], nodeIndices: [], tensorIndices: [], inputTensors: r.inputs, outputTensors: r.outputs, inputMasks: r.inputs.map(function (e) {\n        return null;\n      }), outputMasks: r.outputs.map(function (e) {\n        return null;\n      }), inputShapes: r.inputs.map(function (e) {\n        return e.shape;\n      }), outputShapes: r.outputs.map(function (e) {\n        return e.shape;\n      }) }), r.built = !0, r._refCount = 1, r;\n  }return __extends(t, e), t.prototype.assertNotDisposed = function () {\n    if (0 === this._refCount) throw new Error(\"Container '\" + this.name + \"' is already disposed.\");\n  }, t.prototype.dispose = function () {\n    this.assertNotDisposed();var e = { refCountAfterDispose: null, numDisposedVariables: 0 };if (0 == --this._refCount) for (var t = 0, n = this.layers; t < n.length; t++) {\n      var r = n[t];e.numDisposedVariables += r.dispose().numDisposedVariables;\n    }return e.refCountAfterDispose = this._refCount, e;\n  }, Object.defineProperty(t.prototype, \"trainableWeights\", { get: function () {\n      if (this._trainableWeights.length > 0) throw new ValueError(\"Container instance unexpectedly contains _trainableWeights.The trainable weights of a Container are a union of the trainable weights of its consituent Layers. Its own _trainableWeights must remain an empty Array.\");if (!this.trainable) return [];for (var e = [], t = 0, n = this.layers; t < n.length; t++) {\n        var r = n[t];e = e.concat(r.trainableWeights);\n      }return e;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"nonTrainableWeights\", { get: function () {\n      for (var e = [], t = 0, n = this.layers; t < n.length; t++) {\n        var r = n[t];e.push.apply(e, r.nonTrainableWeights);\n      }if (!this.trainable) {\n        for (var i = [], a = 0, o = this.layers; a < o.length; a++) {\n          r = o[a];i.push.apply(i, r.trainableWeights);\n        }return i.concat(e);\n      }return e;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"weights\", { get: function () {\n      return this.trainableWeights.concat(this.nonTrainableWeights);\n    }, enumerable: !0, configurable: !0 }), t.prototype.loadWeights = function (e, t, n, r) {\n    void 0 === t && (t = !1), void 0 === n && (n = !1), void 0 === r && (r = !0), n ? loadWeightsFromNamedTensorMap(e, this.layers, r) : loadWeightsFromJson(e, this.layers, t);\n  }, t.prototype.updatedConfig = function () {\n    var e = this.getConfig();return { className: this.getClassName(), config: e, kerasVersion: \"tfjs-layers \" + version, backend: \"TensorFlow.js\" };\n  }, t.prototype.toJSON = function (e, t) {\n    void 0 === t && (t = !0);var n = convertTsToPythonic(this.updatedConfig());return t ? JSON.stringify(n) : n;\n  }, t.prototype.call = function (e, t) {\n    var n = this;return (0, _tfjsCore.tidy)(function () {\n      var r;return e = toList(e), r = \"mask\" in t ? toList(t.mask) : pyListRepeat(null, e.length), n.runInternalGraph(e, r)[0];\n    });\n  }, t.prototype.computeMask = function (e, t) {\n    var n = this;return (0, _tfjsCore.tidy)(function () {\n      var r;return e = toList(e), r = null == t ? pyListRepeat(null, e.length) : toList(t), n.runInternalGraph(e, r)[1];\n    });\n  }, t.prototype.computeOutputShape = function (e) {\n    var t = normalizeShapeList(e);if (t.length !== this.inputLayers.length) throw new ValueError(\"Invalid inputShape argument \" + e + \": model has \" + this.inputLayers.length + \" tensor inputs.\");for (var n = {}, r = 0; r < t.length; r++) {\n      var i = this.inputLayers[r],\n          a = t[r];n[A = i.name + \"_0_0\"] = a;\n    }var o = Object.keys(this.nodesByDepth).map(function (e) {\n      return parseInt(e, 10);\n    }).sort(reverseNumberCompare);if (o.length > 1) for (var s = 0, l = o; s < l.length; s++) for (var u = l[s], c = 0, p = this.nodesByDepth[u]; c < p.length; c++) {\n      var h = p[c];i = h.outboundLayer;if (-1 === this.inputLayers.map(function (e) {\n        return e.id;\n      }).indexOf(i.id)) {\n        for (var d = [], g = 0; g < h.inboundLayers.length; g++) {\n          var f = h.inboundLayers[g],\n              m = h.nodeIndices[g],\n              y = h.tensorIndices[g],\n              v = n[A = f.name + \"_\" + m + \"_\" + y];d.push(v);\n        }var b = normalizeShapeList(i.computeOutputShape(singletonOrArray(d))),\n            w = i.inboundNodes.indexOf(h);for (g = 0; g < b.length; g++) {\n          n[A = i.name + \"_\" + w + \"_\" + g] = b[g];\n        }\n      }\n    }var z = [],\n        S = [];for (r = 0; r < this.outputLayers.length; r++) {\n      i = this.outputLayers[r], w = this.outputLayersNodeIndices[r], y = this.outputLayersTensorIndices[r];var A = i.name + \"_\" + w + \"_\" + y;S.push(A);\n    }for (r = 0; r < S.length; r++) {\n      var I = S[r];assert(I in n), z.push(n[I]);\n    }return singletonOrArray(z);\n  }, t.prototype.runInternalGraph = function (e, t) {\n    null == t && (t = pyListRepeat(null, e.length));for (var n = {}, r = 0; r < this.inputs.length; ++r) {\n      var i = this.inputs[r],\n          a = e[r],\n          o = t[r];n[i.id] = [a, o];\n    }for (var s = 0, l = Object.keys(this.nodesByDepth).map(function (e) {\n      return parseInt(e, 10);\n    }).sort(reverseNumberCompare); s < l.length; s++) for (var u = l[s], c = 0, p = this.nodesByDepth[u]; c < p.length; c++) {\n      for (var h = p[c], d = h.outboundLayer, g = h.inputTensors, f = h.outputTensors, m = new Array(), y = 0, v = g; y < v.length; y++) {\n        (i = v[y]).id in n && m.push(n[i.id]);\n      }if (m.length === g.length) {\n        var b = {},\n            w = void 0,\n            z = void 0,\n            S = void 0,\n            A = void 0;if (null != h.callArgs && (b = h.callArgs), 1 === m.length) {\n          var I = m[0],\n              C = I[0],\n              N = I[1];null == b.mask && (b.mask = N), S = toList(d.call(C, b)), A = toList(d.computeMask(C, N)), w = [C], z = [N];\n        } else w = m.map(function (e) {\n          return e[0];\n        }), z = m.map(function (e) {\n          return e[1];\n        }), null == b.mask && (b.mask = z), S = toList(d.call(w, b)), A = toList(d.computeMask(w, z));if (d.activityRegularizer) throw new NotImplementedError(\"Model invocation with concrete Tensor value(s) in the presence of activity regularizer(s) is not supported yet.\");for (r = 0; r < f.length; ++r) {\n          i = f[r], a = S[r], o = A[r];n[i.id] = [a, o];\n        }\n      }\n    }for (var E = [], _ = [], k = [], L = 0, x = this.outputs; L < x.length; L++) {\n      assert((i = x[L]).id in n, \"Could not compute output \" + i.name + \" : \" + i.id);var T = n[i.id],\n          R = T[0];o = T[1];k.push(R.shape), E.push(R), _.push(o);\n    }return [E, _, k];\n  }, t.prototype.buildNodeConversionMap = function (e) {\n    for (var n, r = {}, i = 0, a = this.layers; i < a.length; i++) {\n      var o = a[i];n = o instanceof t ? 1 : 0;for (var s = 0; s < o.inboundNodes.length; s++) {\n        var l = t.nodeKey(o, s);l in this.containerNodes && (r[l] = n, n += 1);\n      }\n    }return r;\n  }, t.prototype.getLayer = function (e, t) {\n    if (null != t) {\n      if (this.layers.length <= t) throw new ValueError(\"Was asked to retrieve layer at index \" + t + \", but model only has \" + this.layers.length + \" layer(s).\");return this.layers[t];\n    }if (null == e) throw new ValueError(\"Provide either a layer name or layer index\");for (var n = 0, r = this.layers; n < r.length; n++) {\n      var i = r[n];if (i.name === e) return i;\n    }throw new ValueError(\"No such layer: \" + e);\n  }, t.prototype.calculateLosses = function () {\n    var e = this;return (0, _tfjsCore.tidy)(function () {\n      for (var n = [], r = 0, i = e.layers; r < i.length; r++) for (var a = i[r], o = 0; o < a.inboundNodes.length; ++o) {\n        var s = t.nodeKey(a, o);e.containerNodes.has(s) && n.push.apply(n, a.calculateLosses());\n      }return n;\n    });\n  }, t.prototype.getConfig = function () {\n    for (var e = { name: this.name }, n = this.buildNodeConversionMap(this.layers), r = [], i = 0, a = this.layers; i < a.length; i++) {\n      for (var o = (b = a[i]).getClassName(), s = b.getConfig(), l = [], u = 0; u < b.inboundNodes.length; u++) {\n        var c = b.inboundNodes[u],\n            p = t.nodeKey(b, u),\n            h = {};if (this.containerNodes.has(p)) {\n          if (c.callArgs) try {\n            JSON.stringify(c.callArgs), h = c.callArgs;\n          } catch (e) {\n            console.warn(\"Layer \" + b.name + \" was passed non-serializable keyword arguments: \" + c.callArgs + \". They will not be included in the serialized model (and thus will be missing at deserialization time).\"), h = {};\n          }if (c.inboundLayers.length > 0) {\n            for (var d = [], g = 0; g < c.inboundLayers.length; g++) {\n              var f = c.inboundLayers[g],\n                  m = c.nodeIndices[g],\n                  y = c.tensorIndices[g];null !== (z = n[t.nodeKey(f, m)]) && void 0 !== z || (z = 0), d.push([f.name, z, y, h]);\n            }l.push(d);\n          }\n        }\n      }r.push({ name: b.name, className: o, config: s, inboundNodes: l });\n    }e.layers = r;var v = [];for (g = 0; g < this.inputLayers.length; g++) {\n      var b = this.inputLayers[g];m = this.inputLayersNodeIndices[g], p = t.nodeKey(b, m);if (this.containerNodes.has(p)) {\n        null !== (z = n[p]) && void 0 !== z || (z = 0);y = this.inputLayersTensorIndices[g];v.push([b.name, z, y]);\n      }\n    }e.inputLayers = v;var w = [];for (g = 0; g < this.outputLayers.length; g++) {\n      b = this.outputLayers[g], m = this.outputLayersNodeIndices[g], p = t.nodeKey(b, m);if (this.containerNodes.has(p)) {\n        var z;null !== (z = n[p]) && void 0 !== z || (z = 0);y = this.outputLayersTensorIndices[g];w.push([b.name, z, y]);\n      }\n    }return e.outputLayers = w, e;\n  }, t.fromConfig = function (e, t) {\n    var n = {},\n        r = {};function i(e, t) {\n      e.name in r ? r[e.name].push(t) : r[e.name] = [t];\n    }function a(e, t) {\n      for (var r, a = [], o = 0, s = t; o < s.length; o++) {\n        var l = s[o],\n            u = l[0],\n            c = l[1],\n            p = l[2];if (3 === l.length) r = {};else {\n          if (4 !== l.length) throw new ValueError(\"Improperly formatted model config for layer \" + JSON.stringify(e) + \": \" + JSON.stringify(l));r = l[3];\n        }if (!(u in n)) return void i(e, t);var h = n[u];if (h.inboundNodes.length <= c) return void i(e, t);var d = h.inboundNodes[c];a.push(d.outputTensors[p]);\n      }a.length > 0 && e.apply(singletonOrArray(a), r);\n    }function o(e) {\n      var r = e.name,\n          a = deserialize(e, null != t.customObjects ? t.customObjects : {});n[r] = a;for (var o = 0, s = e.inboundNodes; o < s.length; o++) {\n        var l = s[o];if (!(l instanceof Array)) throw new ValueError(\"Corrupted configuration, expected array for nodeData: \" + l);i(a, l);\n      }\n    }for (var s = t.name, l = t.layers, u = 0, c = l; u < c.length; u++) {\n      o(d = c[u]);\n    }for (; !isObjectEmpty(r);) for (var p = 0, h = l; p < h.length; p++) {\n      var d = h[p];if ((N = n[d.name]).name in r) {\n        for (var g = 0, f = r[N.name]; g < f.length; g++) {\n          a(N, f[g]);\n        }delete r[N.name];\n      }\n    }for (var m = [], y = [], v = 0, b = t.inputLayers; v < b.length; v++) {\n      var w = (d = b[v])[0],\n          z = d[1],\n          S = d[2];assert(w in n);var A = (N = n[w]).inboundNodes[z].outputTensors;m.push(A[S]);\n    }for (var I = 0, C = t.outputLayers; I < C.length; I++) {\n      w = (d = C[I])[0], z = d[1], S = d[2];assert(w in n);var N;A = (N = n[w]).inboundNodes[z].outputTensors;y.push(A[S]);\n    }return new e({ inputs: m, outputs: y, name: s });\n  }, Object.defineProperty(t.prototype, \"stateful\", { get: function () {\n      if (this._stateful) throw new ValueError(\"Container instance unexpectedly has _stateful = true. The statefulness of a Container is determined by the Layers it contains. Its _stateful property must remain the default false.\");for (var e = 0, t = this.layers; e < t.length; e++) {\n        if (t[e].stateful) return !0;\n      }return !1;\n    }, enumerable: !0, configurable: !0 }), t.prototype.resetStates = function () {\n    var e = this;(0, _tfjsCore.tidy)(function () {\n      e.layers.forEach(function (e) {\n        e.stateful && e.resetStates();\n      });\n    });\n  }, t;\n}(Layer);function assertFeedCompatibility(e, t) {\n  if (null != e.shape) {\n    if (e.shape.length !== t.shape.length) throw new ValueError(\"The rank of feed (\" + t.shape.length + \") does not match the rank of the key (\" + e.shape.length + \").\");for (var n = 0; n < e.shape.length; ++n) if (null != e.shape[n] && e.shape[n] !== t.shape[n]) throw new ValueError(\"The \" + n + \"-th dimension of the feed (\" + t.shape[n] + \") is incompatible with that of the key (\" + e.shape[n] + \").\");\n  }if (null == e.dtype || e.dtype === t.dtype) return t;try {\n    return (0, _tfjsCore.cast)(t, e.dtype);\n  } catch (n) {\n    throw new ValueError(\"The dtype of the feed (\" + t.dtype + \") can not be cast to the dtype of the key '\" + e.name + \"' (\" + e.dtype + \").\");\n  }\n}var ModelLoggingVerbosity,\n    FeedDict = function () {\n  function e(t) {\n    if (this.id2Value = {}, t instanceof e) for (var n in t.id2Value) this.id2Value[n] = t.id2Value[n];else {\n      if (null == t) return;for (var r = 0, i = t; r < i.length; r++) {\n        var a = i[r];this.add(a.key, a.value);\n      }\n    }\n  }return e.prototype.add = function (e, t) {\n    if (null != this.id2Value[e.id]) throw new ValueError(\"Duplicate key: name=\" + e.name + \", id=\" + e.id);return this.id2Value[e.id] = assertFeedCompatibility(e, t), this;\n  }, e.prototype.addFeed = function (e) {\n    this.add(e.key, e.value);\n  }, e.prototype.hasKey = function (e) {\n    return null != this.id2Value[e.id];\n  }, e.prototype.getValue = function (e) {\n    if (null == this.id2Value[e.id]) throw new ValueError(\"Nonexistent key: \" + JSON.stringify(e));return this.id2Value[e.id];\n  }, e;\n}();function execute(e, t, n) {\n  for (var r = Array.isArray(e), i = r ? e : [e], a = [], o = new FeedDict(t), s = 0, l = i; s < l.length; s++) {\n    var u = l[s];a.push(executeInternal(u, o, n));\n  }return r ? a : a[0];\n}function executeInternal(e, t, n) {\n  if (t.hasKey(e)) return t.getValue(e);if (e.sourceLayer instanceof InputLayer) throw new ValueError(\"Missing a feed value for SymbolicTensor from InputLayer '\" + InputLayer.name + \"'\");for (var r = [], i = 0, a = e.inputs; i < a.length; i++) {\n    var o = executeInternal(a[i], t, n);r.push(o);\n  }var s = e.sourceLayer.apply(r, n);Array.isArray(s) || (s = [s]);for (var l = getNodeOutputs(e), u = Array.isArray(l) ? l : [l], c = 0; c < u.length; ++c) t.add(u[c], s[c]);return 1 === s.length ? s[0] : s[e.outputTensorIndex];\n}function getNodeOutputs(e) {\n  var t;if (1 === e.sourceLayer.inboundNodes.length) t = e.sourceLayer.output;else {\n    for (var n = null, r = 0; r < e.sourceLayer.inboundNodes.length; ++r) for (var i = 0, a = e.sourceLayer.inboundNodes[r].outputTensors; i < a.length; i++) {\n      if (a[i].id === e.id) {\n        n = r;break;\n      }\n    }t = e.sourceLayer.getOutputAt(n);\n  }return t;\n}function isDataTensor(e) {\n  return e instanceof _tfjsCore.Tensor;\n}function isDataArray(e) {\n  return Array.isArray(e);\n}function isDataDict(e) {\n  return !isDataTensor(e) && !isDataArray(e);\n}function standardizeInputData(e, t, n, r, i) {\n  if (void 0 === r && (r = !0), void 0 === i && (i = \"\"), null == t || 0 === t.length) {\n    if (null != e) {\n      var a = !1;if (isDataArray(e) && e.length > 0) a = !0;else if (isDataDict(e)) {\n        for (var o in e) if (e.hasOwnProperty(o)) {\n          a = !0;break;\n        }\n      } else a = !0;if (a) throw new ValueError(\"Error when checking model \" + i + \" expected no data, but got \" + e);\n    }return [];\n  }if (null == e) return t.map(function (e) {\n    return null;\n  });var s;if (isDataDict(e)) {\n    e = e, s = [];for (var l = 0, u = t; l < u.length; l++) {\n      var c = u[l];if (null == e[c]) throw new ValueError('No data provided for \"' + c + '\". Need data for each key in: ' + t);s.push(e[c]);\n    }\n  } else if (isDataArray(e)) {\n    if ((e = e).length !== t.length) throw new ValueError(\"Error when checking model \" + i + \": the Array of Tensors that you are passing to your model is not the size the model expected. Expected to see \" + t.length + \" Tensor(s), but instead got the following list of Tensor(s): \" + e);s = e;\n  } else {\n    if (e = e, t.length > 1) throw new ValueError(\"The model \" + i + \" expects \" + t.length + \" Tensor(s), but only received one Tensor. Found: Tensor with shape \" + e.shape);s = [e];\n  }for (var p = 0; p < t.length; ++p) {\n    1 === (h = s[p]).shape.length && (s[p] = expandDims(h, 1));\n  }if (null != n) for (p = 0; p < t.length; ++p) if (null != n[p]) {\n    var h;if ((h = s[p]).shape.length !== n[p].length) throw new ValueError(\"Error when checking \" + i + \": expected \" + t[p] + \" to have \" + n[p].length + \" dimension(s). but got array with shape \" + h.shape);for (var d = 0; d < n[p].length; ++d) if (0 !== d || r) {\n      var g = h.shape[d],\n          f = n[p][d];if (null != f && f >= 0 && g !== f) throw new ValueError(\"Error when checking \" + i + \": expected \" + t[p] + \" to have shape [\" + n[p] + \"], but got array with shape [\" + h.shape + \"].\");\n    }\n  }return s;\n}function checkArrayLengths(e, t, n) {\n  var r = unique(e.map(function (e) {\n    return e.shape[0];\n  }));r.sort();var i = unique(t.map(function (e) {\n    return e.shape[0];\n  }));if (i.sort(), r.length > 1) throw new ValueError(\"All input Tensors (x) should have the same number of samples. Got array shapes: \" + JSON.stringify(e.map(function (e) {\n    return e.shape;\n  })));if (i.length > 1) throw new ValueError(\"All target Tensors (y) should have the same number of samples. Got array shapes: \" + JSON.stringify(t.map(function (e) {\n    return e.shape;\n  })));if (r.length > 0 && i.length > 0 && !_tfjsCore.util.arraysEqual(r, i)) throw new ValueError(\"Input Tensors should have the same number of samples as target Tensors. Found \" + r[0] + \" input sample(s) and \" + i[0] + \" target sample(s).\");\n}function checkLossAndTargetCompatibility(e, t, n) {\n  for (var r = [meanSquaredError, binaryCrossentropy, categoricalCrossentropy], i = 0; i < e.length; ++i) {\n    var a = e[i],\n        o = t[i],\n        s = n[i];if (null != o) {\n      if (o === categoricalCrossentropy && 1 === a.shape[a.shape.length - 1]) throw new ValueError(\"You are passing a target array of shape \" + a.shape + \" while using a loss 'categorical_crossentropy'. 'categorical_crossentropy'expects targets to be binary matrices (1s and 0s) of shape [samples, classes].\");if (-1 !== r.indexOf(o)) for (var l = a.shape.slice(1), u = s.slice(1), c = 0; c < l.length; ++c) {\n        var p = l[c],\n            h = u[c];if (null != h && p !== h) throw new ValueError(\"A target Tensor with shape \" + a.shape + \" was passed for an output of shape \" + s + \", while using a loss function that expects targets to have the same shape as the output.\");\n      }\n    }\n  }\n}function makeBatches(e, t) {\n  for (var n = [], r = 0, i = null; r < e;) (i = r + t) >= e && (i = e), n.push([r, i]), r = i;return n;\n}function sliceArrays(e, t, n) {\n  return null == e ? [null] : Array.isArray(e) ? e.map(function (e) {\n    return sliceAlongFirstAxis(e, t, n - t);\n  }) : sliceAlongFirstAxis(e, t, n - t);\n}function sliceArraysByIndices(e, t) {\n  return (0, _tfjsCore.tidy)(function () {\n    return null == e ? null : Array.isArray(e) ? e.map(function (e) {\n      return sliceArraysByIndices(e, t);\n    }) : gather$1(e, \"int32\" === t.dtype ? t : t.toInt());\n  });\n}function checkInputData(e, t, n, r, i) {\n  var a;if (void 0 === r && (r = !0), void 0 === i && (i = \"\"), Array.isArray(e)) {\n    if (e.length !== t.length) throw new ValueError(\"Error when checking model \" + i + \": the Array of Tensors that you are passing to your model is not the size the the model expected. Expected to see \" + t.length + \" Tensor(s), but instead got \" + e.length + \" Tensors(s).\");a = e;\n  } else {\n    if (t.length > 1) throw new ValueError(\"The model expects \" + t.length + \" \" + i + \" Tensors, but only received one Tensor. Found: array with shape \" + JSON.stringify(e.shape) + \".\");a = [e];\n  }if (null != n) for (var o = 0; o < t.length; ++o) if (null != n[o]) {\n    var s = a[o];if (s.shape.length !== n[o].length) throw new ValueError(\"Error when checking \" + i + \": expected \" + t[o] + \" to have \" + n[o].length + \" dimension(s), but got array with shape \" + JSON.stringify(s.shape));for (var l = 0; l < n[o].length; ++l) if (0 !== l || r) {\n      var u = s.shape[l],\n          c = n[o][l];if (null != c && c !== u) throw new ValueError(\"Error when checking \" + i + \": expected \" + t[o] + \" to have shape \" + JSON.stringify(n[o]) + \" but got array with shape \" + JSON.stringify(s.shape) + \".\");\n    }\n  }\n}function collectMetrics(e, t) {\n  if (null == e || Array.isArray(e) && 0 === e.length) return t.map(function (e) {\n    return [];\n  });if (Array.isArray(e)) return t.map(function (t) {\n    return e;\n  });if (null != e) {\n    for (var n = [], r = 0, i = t; r < i.length; r++) {\n      var a = i[r],\n          o = e.hasOwnProperty(a) ? e[a] : [];Array.isArray(o) || (o = [o]), n.push(o);\n    }return n;\n  }throw new TypeError(\"Type of metrics argument not understood. Expected an Array or Object, found: \" + e);\n}function checkBatchSize(e) {\n  _tfjsCore.util.assert(e > 0 && Number.isInteger(e), \"batchSize is required to be a positive integer, but got \" + e);\n}!function (e) {\n  e[e.SILENT = 0] = \"SILENT\", e[e.VERBOSE = 1] = \"VERBOSE\";\n}(ModelLoggingVerbosity || (ModelLoggingVerbosity = {}));var Model = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this;return n.isTraining = !1, n;\n  }return __extends(t, e), t.prototype.summary = function (e, t, n) {\n    if (void 0 === n && (n = console.log), !this.built) throw new ValueError(\"This model has never been called, thus its weights have not been created yet. So no summary can be displayed. Build the model first (e.g., by calling it on some test data).\");printSummary(this, e, t, n);\n  }, t.prototype.compile = function (e) {\n    var t = this;if (null == e.loss && (e.loss = []), this.loss = e.loss, \"string\" == typeof e.optimizer) this.optimizer = getOptimizer(e.optimizer);else {\n      if (!(e.optimizer instanceof _tfjsCore.Optimizer)) throw new ValueError(\"User-defined optimizer must be an instance of tf.Optimizer.\");this.optimizer = e.optimizer;\n    }var n = [];if (Array.isArray(e.loss) || \"string\" == typeof e.loss || \"function\" == typeof e.loss) {\n      if (Array.isArray(e.loss)) {\n        if (e.loss.length !== this.outputs.length) throw new ValueError(\"When passing an Array as loss, it should have one entry per model output. The model has \" + this.outputs.length + \" output(s), but you passed loss=\" + e.loss + \".\");var r = e.loss;n = r.map(function (e) {\n          return get(e);\n        });\n      } else {\n        var i = get(e.loss);this.outputs.map(function (e) {\n          n.push(i);\n        });\n      }\n    } else {\n      for (var a in e.loss = e.loss, e.loss) if (-1 === this.outputNames.indexOf(a)) throw new ValueError('Unknown entry in loss dictionary: \"' + a + '\". Only expect the following keys: ' + this.outputNames);for (var o in this.outputNames) null == e.loss[o] && console.warn('Output \"' + o + '\" is missing from loss dictionary. We assume this was done on purpose, and we will not be expecting data to be passed to ' + o + \" during training\"), n.push(get(e.loss[o]));\n    }this.lossFunctions = n, this.feedOutputNames = [], this.feedOutputShapes = [], this.feedLossFns = [];for (var s = 0; s < this.outputs.length; ++s) {\n      var l = this.internalOutputShapes[s],\n          u = this.outputNames[s];this.feedOutputNames.push(u), this.feedOutputShapes.push(l), this.feedLossFns.push(this.lossFunctions[s]);\n    }var c = [];this.metrics = e.metrics, this.metricsNames = [\"loss\"], this.metricsTensors = [], nameScope(\"loss\", function () {\n      for (var e = 0; e < t.outputs.length; ++e) if (-1 === c.indexOf(e)) {\n        var n = t.lossFunctions[e];t.outputs.length > 1 && (t.metricsTensors.push([n, e]), t.metricsNames.push(t.outputNames[e] + \"_loss\"));\n      }\n    });var p = collectMetrics(e.metrics, this.outputNames);nameScope(\"metric\", function () {\n      for (var e = function (e) {\n        if (-1 !== c.indexOf(e)) return \"continue\";!function (n) {\n          for (var r, i, a, o = function (n) {\n            if (-1 !== [\"accuracy\", \"acc\", \"crossentropy\", \"ce\"].indexOf(n)) {\n              var o = t.internalOutputShapes[e];1 === o[o.length - 1] || t.lossFunctions[e] === binaryCrossentropy ? -1 !== [\"accuracy\", \"acc\"].indexOf(n) ? i = binaryAccuracy : -1 !== [\"crossentropy\", \"ce\"].indexOf(n) && (i = binaryCrossentropy$1) : t.lossFunctions[e] === sparseCategoricalCrossentropy ? -1 !== [\"accuracy\", \"acc\"].indexOf(n) ? i = sparseCategoricalAccuracy : -1 !== [\"crossentropy\", \"ce\"].indexOf(n) && (i = sparseCategoricalCrossentropy$1) : -1 !== [\"accuracy\", \"acc\"].indexOf(n) ? i = categoricalAccuracy : -1 !== [\"crossentropy\", \"ce\"].indexOf(n) && (i = categoricalCrossentropy$1);var s = void 0;-1 !== [\"accuracy\", \"acc\"].indexOf(n) ? s = \"acc\" : -1 !== [\"crossentropy\", \"ce\"].indexOf(n) && (s = \"ce\"), a = i, r = \"\" + s;\n            } else {\n              var l = get$1(n);a = l, r = \"\" + n;\n            }var u;nameScope(r, function () {\n              u = a;\n            }), function (e, n, r) {\n              t.outputNames.length > 1 && (n = t.outputNames[e] + \"_\" + n), t.metricsNames.push(n), t.metricsTensors.push([r, e]);\n            }(e, r, u);\n          }, s = 0, l = n; s < l.length; s++) o(l[s]);\n        }(p[e]);\n      }, n = 0; n < t.outputs.length; ++n) e(n);\n    }), this.collectedTrainableWeights = this.trainableWeights;\n  }, t.prototype.checkTrainableWeightsConsistency = function () {\n    null != this.collectedTrainableWeights && this.trainableWeights.length !== this.collectedTrainableWeights.length && console.warn(\"Discrepancy between trainableweights and collected trainable weights. Did you set `model.trainable` without calling `model.compile()` afterwards?\");\n  }, t.prototype.evaluate = function (e, t, n) {\n    void 0 === n && (n = {});var r = null == n.batchSize ? 32 : n.batchSize;checkBatchSize(r);var i = this.standardizeUserData(e, t, !0, r),\n        a = i[0].concat(i[1]);this.makeTestFunction();var o = this.testFunction;return singletonOrArray(this.testLoop(o, a, r, n.verbose, n.steps));\n  }, t.prototype.checkNumSamples = function (e, t, n, r) {\n    var i;if (void 0 === r && (r = \"steps\"), null != n) {\n      if (i = null, null != t) throw new ValueError(\"If \" + r + \" is set, batchSize must be null or undefined.Got batchSize = \" + t);\n    } else {\n      if (null == e) throw new ValueError(\"Either the input data should have a defined shape, or \" + r + \" shoud be specified.\");i = Array.isArray(e) ? e[0].shape[0] : e.shape[0];\n    }return i;\n  }, t.prototype.execute = function (e, t) {\n    if (Array.isArray(t) && 0 === t.length) throw new ValueError(\"`outputs` is an empty Array, which is not allowed.\");var n = Array.isArray(t),\n        r = n ? t : [t],\n        i = this.retrieveSymbolicTensors(r),\n        a = new FeedDict();if (e instanceof _tfjsCore.Tensor && (e = [e]), Array.isArray(e)) {\n      if (e.length !== this.inputs.length) throw new ValueError(\"The number of inputs provided (\" + e.length + \") does not match the number of inputs of this model (\" + this.inputs.length + \").\");for (var o = 0; o < this.inputs.length; ++o) a.add(this.inputs[o], e[o]);\n    } else for (var s = 0, l = this.inputs; s < l.length; s++) {\n      var u = l[s],\n          c = e[u.name];if (null == c) throw new ValueError(\"No value is provided for the model's input \" + u.name);a.add(u, c);\n    }var p = execute(i, a);return n ? p : p[0];\n  }, t.prototype.retrieveSymbolicTensors = function (e) {\n    for (var t = pyListRepeat(null, e.length), n = e.length, r = 0, i = this.layers; r < i.length; r++) {\n      for (var a = i[r], o = Array.isArray(a.output) ? a.output : [a.output], s = o.map(function (e) {\n        return e.name;\n      }), l = 0; l < e.length; ++l) {\n        var u = s.indexOf(e[l]);if (-1 !== u && (t[l] = o[u], n--), 0 === n) break;\n      }if (0 === n) break;\n    }if (n > 0) {\n      var c = [];throw t.forEach(function (t, n) {\n        null == t && c.push(e[n]);\n      }), new ValueError(\"Cannot find SymbolicTensors for output name(s): \" + JSON.stringify(c));\n    }return t;\n  }, t.prototype.predictLoop = function (e, t, n) {\n    var r = this;return void 0 === t && (t = 32), void 0 === n && (n = !1), (0, _tfjsCore.tidy)(function () {\n      var i = r.checkNumSamples(e);if (n) throw new NotImplementedError(\"Verbose predictLoop() is not implemented yet.\");for (var a = makeBatches(i, t), o = [], s = function (t) {\n        var n = (0, _tfjsCore.tidy)(function () {\n          var n = a[t][0],\n              i = a[t][1],\n              o = sliceArrays(e, n, i),\n              s = [];if (Array.isArray(o)) for (var l = 0; l < o.length; ++l) s.push({ key: r.inputs[l], value: o[l] });else s.push({ key: r.inputs[0], value: o });var u = new FeedDict(s);return execute(r.outputs, u);\n        });if (0 === t) for (var i = 0, s = n; i < s.length; i++) {\n          var l = s[i];o.push(l);\n        } else for (var u = 0; u < n.length; ++u) o[u] = concatAlongFirstAxis(o[u], n[u]);\n      }, l = 0; l < a.length; ++l) s(l);return singletonOrArray(o);\n    });\n  }, t.prototype.predict = function (e, t) {\n    void 0 === t && (t = {}), checkInputData(e, this.inputNames, this.feedInputShapes, !1);var n = null == t.batchSize ? 32 : t.batchSize;return checkBatchSize(n), this.predictLoop(e, n);\n  }, t.prototype.predictOnBatch = function (e) {\n    return checkInputData(e, this.inputNames, this.feedInputShapes, !0), this.predictLoop(e, e.shape[0]);\n  }, t.prototype.standardizeUserData = function (e, t, n, r) {\n    if (void 0 === n && (n = !0), null == this.optimizer) throw new RuntimeError(\"You must compile a model before training/testing. Use Model.compile(modelCompileConfig).\");for (var i = [], a = 0; a < this.feedOutputShapes.length; ++a) {\n      var o = this.feedOutputShapes[a];this.feedLossFns[a] === sparseCategoricalCrossentropy ? i.push(o.slice(0, o.length - 1).concat([1])) : i.push(o);\n    }if (checkArrayLengths(e = standardizeInputData(e, this.feedInputNames, this.feedInputShapes, !1, \"input\"), t = standardizeInputData(t, this.feedOutputNames, i, !1, \"target\"), null), checkLossAndTargetCompatibility(t, this.feedLossFns, this.feedOutputShapes), this.stateful && null != r && r > 0 && e[0].shape[0] % r != 0) throw new ValueError(\"In a stateful network, you should only pass inputs with a number of samples that is divisible by the batch size \" + r + \". Found: \" + e[0].shape[0] + \" sample(s).\");return [e, t, null];\n  }, t.prototype.fitLoop = function (e, t, n, r, i, a, o, s, l, u, c, p, h, d, g) {\n    return __awaiter(this, void 0, void 0, function () {\n      var f,\n          m,\n          y,\n          v,\n          b,\n          w,\n          z,\n          S,\n          A = this;return __generator(this, function (I) {\n        switch (I.label) {case 0:\n            if (null == r && (r = 32), null == i && (i = 1), null == u && (u = !0), null == p && (p = 0), f = !1, null != s && null != l && (f = !0), null != d && (f = !0, null == h)) throw new ValueError(\"Can only use `validationSteps` when doing step-wise training, i.e., `stepsPerEpoch` must be set.\");return null != (m = this.checkNumSamples(t, r, h, \"steps_per_epoch\")) && (y = range(0, m)), null == a && (a = 1), this.history = new History(), v = [new BaseLogger(g)].concat(CallbackConstructorRegistry.createCallbacks(a)), null != o && v.push.apply(v, o), v.push(this.history), (b = new CallbackList(v)).setModel(this), b.setParams({ epochs: i, initialEpoch: p, samples: m, steps: h, batchSize: r, verbose: a, doValidation: f, metrics: c }), [4, b.onTrainBegin()];case 1:\n            I.sent(), this.stopTraining_ = !1, w = function (i) {\n              var a, o, c, p, d;return __generator(this, function (g) {\n                switch (g.label) {case 0:\n                    return [4, b.onEpochBegin(i)];case 1:\n                    if (g.sent(), a = {}, null == h) return [3, 2];throw new NotImplementedError(\"stepsPerEpoch mode is not implemented yet.\");case 2:\n                    if (\"batch\" === u) throw new NotImplementedError(\"batch shuffling is not implemneted yet\");u && _tfjsCore.util.shuffle(y), o = (0, _tfjsCore.tensor1d)(y), c = makeBatches(m, r), p = function (i) {\n                      var u;return __generator(this, function (p) {\n                        switch (p.label) {case 0:\n                            return u = {}, [4, b.onBatchBegin(i, u)];case 1:\n                            return p.sent(), (0, _tfjsCore.tidy)(function () {\n                              var p = c[i][0],\n                                  h = c[i][1],\n                                  d = sliceAlongFirstAxis(o, p, h - p);u.batch = i, u.size = h - p;for (var g = sliceArraysByIndices(t, d), m = e(g), y = 0; y < n.length; ++y) {\n                                var v = n[y],\n                                    b = m[y];u[v] = b, (0, _tfjsCore.keep)(b);\n                              }if (i === c.length - 1 && f) {\n                                var w = A.testLoop(s, l, r);for (y = 0; y < n.length; ++y) {\n                                  v = n[y], b = w[y];(0, _tfjsCore.keep)(b), a[\"val_\" + v] = b;\n                                }\n                              }\n                            }), [4, b.onBatchEnd(i, u)];case 2:\n                            return p.sent(), disposeTensorsInLogs(u), z.stopTraining_ ? [2, \"break\"] : [2];}\n                      });\n                    }, d = 0, g.label = 3;case 3:\n                    return d < c.length ? [5, p(d)] : [3, 6];case 4:\n                    if (\"break\" === g.sent()) return [3, 6];g.label = 5;case 5:\n                    return ++d, [3, 3];case 6:\n                    o.dispose(), g.label = 7;case 7:\n                    return [4, b.onEpochEnd(i, a)];case 8:\n                    return g.sent(), z.stopTraining_ ? [2, \"break\"] : [2];}\n              });\n            }, z = this, S = p, I.label = 2;case 2:\n            return S < i ? [5, w(S)] : [3, 5];case 3:\n            if (\"break\" === I.sent()) return [3, 5];I.label = 4;case 4:\n            return ++S, [3, 2];case 5:\n            return [4, b.onTrainEnd()];case 6:\n            return I.sent(), [4, this.history.syncData()];case 7:\n            return I.sent(), [2, this.history];}\n      });\n    });\n  }, t.prototype.testLoop = function (e, t, n, r, i) {\n    var a = this;return void 0 === r && (r = 0), (0, _tfjsCore.tidy)(function () {\n      var o = a.checkNumSamples(t, n, i, \"steps\"),\n          s = [];if (1 === r) throw new NotImplementedError(\"Verbose mode is not implemented yet.\");if (null != i) throw new NotImplementedError(\"steps mode in testLoop() is not implemented yet\");for (var l = makeBatches(o, n), u = (0, _tfjsCore.tensor1d)(range(0, o)), c = 0; c < l.length; ++c) {\n        var p = l[c][0],\n            h = l[c][1],\n            d = sliceAlongFirstAxis(u, p, h - p),\n            g = sliceArraysByIndices(t, d),\n            f = e(g);if (0 === c) for (var m = 0; m < f.length; ++m) s.push(getScalar(0));for (m = 0; m < f.length; ++m) {\n          var y = f[m];s[m] = (0, _tfjsCore.add)(s[m], (0, _tfjsCore.mul)(getScalar(h - p), y));\n        }\n      }for (m = 0; m < s.length; ++m) s[m] = (0, _tfjsCore.div)(s[m], getScalar(o));return s;\n    });\n  }, t.prototype.getDedupedMetricsNames = function () {\n    for (var e = this.metricsNames, t = [], n = 0; n < e.length; ++n) {\n      var r = e[n],\n          i = r;if (count(e, r) > 1) i += \"_\" + count(e.slice(0, n), r);t.push(i);\n    }return t;\n  }, t.prototype.makeTestFunction = function () {\n    var e = this;this.testFunction = function (t) {\n      return (0, _tfjsCore.tidy)(function () {\n        for (var n, r = [], i = t.slice(0, e.inputs.length), a = t.slice(e.inputs.length, e.inputs.length + e.outputs.length), o = [], s = 0; s < e.inputs.length; ++s) o.push({ key: e.inputs[s], value: i[s] });var l = new FeedDict(o),\n            u = execute(e.outputs, l);for (s = 0; s < e.lossFunctions.length; ++s) {\n          var c = e.lossFunctions[s],\n              p = (0, _tfjsCore.mean)(c(a[s], u[s]));n = 0 === s ? p : (0, _tfjsCore.add)(n, p), r.push(n);\n        }for (s = 0; s < e.metricsTensors.length; ++s) {\n          var h = e.metricsTensors[s][0],\n              d = e.metricsTensors[s][1],\n              g = (0, _tfjsCore.mean)(h(a[d], u[d]));r.push(g);\n        }return r;\n      });\n    };\n  }, t.prototype.fit = function (e, t, n) {\n    return void 0 === n && (n = {}), __awaiter(this, void 0, void 0, function () {\n      var r,\n          i,\n          a,\n          o,\n          s,\n          l,\n          u,\n          c,\n          p,\n          h,\n          d,\n          g,\n          f,\n          m,\n          y,\n          v,\n          b,\n          w,\n          z,\n          S = this;return __generator(this, function (A) {\n        switch (A.label) {case 0:\n            if (this.isTraining) throw new Error(\"Cannot start training because another fit() call is ongoing.\");this.isTraining = !0, A.label = 1;case 1:\n            if (A.trys.push([1,, 3, 4]), checkBatchSize(r = null == n.batchSize ? 32 : n.batchSize), i = this.standardizeUserData(e, t, !1, r), a = i[0], o = i[1], s = !1, l = void 0, u = void 0, c = void 0, p = !1, null != n.validationData && n.validationData.length > 0) {\n              if (s = !0, 2 !== n.validationData.length) throw 3 === n.validationData.length ? new NotImplementedError(\"validationData including sample weights is not supported yet.\") : new ValueError(\"When passing validation data, it must contain 2 (valX, valY) or 3 (valX, valY, valSampleWeight) items; \" + n.validationData + \" is invalid.\");l = n.validationData[0], u = n.validationData[1], h = this.standardizeUserData(l, u, !0, r), l = h[0], u = h[1], c = l.concat(u);\n            } else null != n.validationSplit && n.validationSplit > 0 && n.validationSplit < 1 ? (s = !0, d = Math.floor(a[0].shape[0] * (1 - n.validationSplit)), g = a[0].shape[0], l = sliceArrays(a, d, g), a = sliceArrays(a, 0, d), u = sliceArrays(o, d, g), o = sliceArrays(o, 0, d), p = !0, c = l.concat(u)) : null != n.validationSteps && (s = !0);return f = a.concat(o), this.checkTrainableWeightsConsistency(), m = function (e) {\n              var t = e.slice(0, S.inputs.length),\n                  n = e.slice(S.inputs.length, S.inputs.length + S.outputs.length),\n                  r = [],\n                  i = S.collectedTrainableWeights.map(function (e) {\n                return e.read();\n              });return [S.optimizer.minimize(function () {\n                for (var e = [], i = 0; i < S.inputs.length; ++i) e.push({ key: S.inputs[i], value: t[i] });var a,\n                    o = new FeedDict(e),\n                    s = execute(S.outputs, o, { training: !0 });for (i = 0; i < S.lossFunctions.length; ++i) {\n                  var l = (0, S.lossFunctions[i])(n[i], s[i]);(0, _tfjsCore.mean)(l), a = 0 === i ? l : (0, _tfjsCore.add)(a, l);\n                }for (i = 0; i < S.metricsTensors.length; ++i) {\n                  var u = S.metricsTensors[i][0],\n                      c = S.metricsTensors[i][1],\n                      p = (0, _tfjsCore.mean)(u(n[c], s[c]));(0, _tfjsCore.keep)(p), r.push(p);\n                }return a = (0, _tfjsCore.mean)(a), S.calculateLosses().forEach(function (e) {\n                  a = (0, _tfjsCore.add)(a, e);\n                }), a;\n              }, !0, i)].concat(r);\n            }, y = this.getDedupedMetricsNames(), v = void 0, b = void 0, s ? (this.makeTestFunction(), v = this.testFunction, b = y.slice().concat(y.map(function (e) {\n              return \"val_\" + e;\n            }))) : (v = null, c = [], b = y.slice()), w = standardizeCallbacks(n.callbacks), [4, this.fitLoop(m, f, y, r, n.epochs, n.verbose, w, v, c, n.shuffle, b, n.initialEpoch, null, null, n.yieldEvery)];case 2:\n            return z = A.sent(), p && (c.forEach(function (e) {\n              return e.dispose();\n            }), a.forEach(function (e) {\n              return e.dispose();\n            }), o.forEach(function (e) {\n              return e.dispose();\n            })), this.isTraining = !1, [2, z];case 3:\n            return this.isTraining = !1, [7];case 4:\n            return [2];}\n      });\n    });\n  }, t.prototype.getNamedWeights = function (e) {\n    for (var t = {}, n = null != e && e.trainableOnly, r = n ? this.trainableWeights : this.weights, i = this.getWeights(n), a = 0; a < r.length; ++a) n && !r[a].trainable || (t[r[a].originalName] = i[a]);return t;\n  }, Object.defineProperty(t.prototype, \"stopTraining\", { set: function (e) {\n      this.stopTraining_ = e;\n    }, enumerable: !0, configurable: !0 }), t.prototype.save = function (e, t) {\n    return __awaiter(this, void 0, void 0, function () {\n      var n, r, i, a, o;return __generator(this, function (s) {\n        switch (s.label) {case 0:\n            if (\"string\" == typeof e) {\n              if (0 === (n = _tfjsCore.io.getSaveHandlers(e)).length) throw new ValueError(\"Cannot find any save handlers for URL '\" + e + \"'\");if (n.length > 1) throw new ValueError(\"Found more than one (\" + n.length + \") save handlers for URL '\" + e + \"'\");e = n[0];\n            }if (null == e.save) throw new ValueError(\"Model.save() cannot proceed because the IOHandler provided does not have the `save` attribute defined.\");return [4, _tfjsCore.io.encodeWeights(this.getNamedWeights(t))];case 1:\n            return r = s.sent(), i = !1, a = null, o = this.toJSON(a, i), [2, e.save({ modelTopology: o, weightData: r.data, weightSpecs: r.specs })];}\n      });\n    });\n  }, t.className = \"Model\", t;\n}(Container);function loadModelInternal(e, t) {\n  return void 0 === t && (t = !0), __awaiter(this, void 0, void 0, function () {\n    var n;return __generator(this, function (r) {\n      if (\"string\" == typeof e) {\n        if (0 === (n = _tfjsCore.io.getLoadHandlers(e)).length) n.push(_tfjsCore.io.browserHTTPRequest(e));else if (n.length > 1) throw new ValueError(\"Found more than one (\" + n.length + \") load handlers for URL '\" + e + \"'\");e = n[0];\n      }return [2, loadModelFromIOHandler(e, void 0, t)];\n    });\n  });\n}function loadModelFromIOHandler(e, t, n) {\n  return void 0 === n && (n = !0), __awaiter(this, void 0, void 0, function () {\n    var r, i, a, o, s;return __generator(this, function (l) {\n      switch (l.label) {case 0:\n          if (null == e.load) throw new ValueError(\"Cannot proceed with model loading because the IOHandler provided does not have the `load` method implemented.\");return [4, e.load()];case 1:\n          if (r = l.sent(), null != (i = r.modelTopology).model_config && (i = i.model_config), a = deserialize(convertPythonicToTs(i), t), null != r.weightData) {\n            if (null == r.weightSpecs) throw new ValueError(\"Model artifacts contains weight data, but not weight specs. Therefore loading of weights cannot proceed.\");o = !1, s = !0, a.loadWeights(_tfjsCore.io.decodeWeights(r.weightData, r.weightSpecs), o, s, n);\n          }return [2, a];}\n    });\n  });\n}_tfjsCore.serialization.registerClass(Model);var Sequential = function (e) {\n  function t(t) {\n    var n = e.call(this, { inputs: [], outputs: [] }) || this;if (t = t || {}, n.trainable = !0, n._updatable = !0, n.built = !1, n.name = null != t.name ? t.name : getUid(\"sequential_\"), null != t.layers) for (var r = 0, i = t.layers; r < i.length; r++) {\n      var a = i[r];n.add(a);\n    }return n;\n  }return __extends(t, e), t.prototype.checkShape = function (e) {\n    if (e.inboundNodes[0].outputTensors[0].shape.some(function (e) {\n      return e < 0;\n    })) throw new ValueError(\"Negative dimension size caused by adding layer \" + e.name + \" with input shape [\" + e.inboundNodes[0].inputTensors[0].shape + \"]\");\n  }, t.prototype.add = function (e) {\n    var n,\n        r = e instanceof t || e instanceof Model;if (r) {\n      if (1 !== (n = e).outputs.length) throw new ValueError(\"All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.\");if (1 !== n.inputs.length) throw new ValueError(\"All layers in a Sequential model should have a single input tensor. For multi-input layers, use the functional API.\");\n    }if (0 === this.outputs.length) {\n      if (0 === e.inboundNodes.length) {\n        if (null == e.batchInputShape) throw new ValueError(\"The first layer in a Sequential model must get an `inputShape` or `batchInputShape` argument.\");var i = Input({ batchShape: e.batchInputShape, dtype: e.dtype, name: e.name + \"_input\" });e.apply(i);\n      }if (r) this.outputs = n.outputs, this.inputs = n.inputs;else {\n        if (1 !== e.inboundNodes.length) throw new ValueError(\"A layer added to a Sequential model must not already be connected somewhere else. Model received layer \" + e.name + \" which has \" + e.inboundNodes.length + \" pre-existing inbound connections.\");if (1 !== e.inboundNodes[0].outputTensors.length) throw new ValueError(\"All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.\");this.checkShape(e), this.outputs = [e.inboundNodes[0].outputTensors[0]], this.inputs = getSourceInputs(this.outputs[0]);\n      }this.inboundNodes = [], new Node({ outboundLayer: this, inboundLayers: [], nodeIndices: [], tensorIndices: [], inputTensors: this.inputs, outputTensors: this.outputs, inputMasks: pyListRepeat(null, this.inputs.length), outputMasks: [null], inputShapes: this.inputs.map(function (e) {\n          return e.shape;\n        }), outputShapes: this.outputs[0].shape });\n    } else {\n      var a = e.apply(this.outputs[0]);if (Array.isArray(a)) throw new TypeError(\"All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.\");this.checkShape(e), this.outputs = [a], this.inboundNodes[0].outputTensors = this.outputs, this.inboundNodes[0].outputShapes = [this.outputs[0].shape];\n    }this.layers.push(e), this.built = !1;\n  }, t.prototype.pop = function () {\n    if (0 === this.layers.length) throw new TypeError(\"There are no layers in the model.\");if (this.layers.pop(), 0 === this.layers.length) this.outputs = [], this.inboundNodes = [], this.outboundNodes = [];else {\n      var e = this.layers.length - 1;this.layers[e].outboundNodes = [], this.outputs = [this.layers[e].output], this.inboundNodes[0].outputTensors = this.outputs, this.inboundNodes[0].outputShapes = [this.outputs[0].shape];\n    }\n  }, t.prototype.call = function (e, t) {\n    return null == this.model && this.build(), this.model.call(e, t);\n  }, t.prototype.build = function (e) {\n    if (getExactlyOneShape(e), 0 === this.inputs.length || 0 === this.outputs.length) throw new TypeError(\"Sequential model cannot be built: model is empty. Add some layers first.\");this.model = new Model({ inputs: this.inputs, outputs: this.outputs[0], name: this.name + \"_model\" }), this.model.trainable = this.trainable, this.model.updatable = this.updatable, this.supportsMasking = this.model.supportsMasking, this.inputLayers = this.model.inputLayers, this.inputLayersNodeIndices = this.model.inputLayersNodeIndices, this.inputLayersTensorIndices = this.model.inputLayersTensorIndices, this.outputLayers = this.model.outputLayers, this.outputLayersNodeIndices = this.model.outputLayersNodeIndices, this.outputLayersTensorIndices = this.model.outputLayersTensorIndices, this.nodesByDepth = this.model.nodesByDepth, this.containerNodes = this.model.containerNodes, this.outputNames = this.model.outputNames, this.inputNames = this.model.inputNames, this.built = !0;\n  }, t.prototype.countParams = function () {\n    return this.built || this.build(), e.prototype.countParams.call(this);\n  }, t.prototype.summary = function (t, n, r) {\n    void 0 === r && (r = console.log), this.built || this.build(), e.prototype.summary.call(this, t, n, r);\n  }, t.prototype.setWeights = function (e) {\n    null == this.model && this.build(), this.model.setWeights(e);\n  }, Object.defineProperty(t.prototype, \"updatable\", { get: function () {\n      return this._updatable;\n    }, set: function (e) {\n      this.built && (this.model.updatable = e), this._updatable = e;\n    }, enumerable: !0, configurable: !0 }), t.prototype.evaluate = function (e, t, n) {\n    if (void 0 === n && (n = {}), !this.built) throw new RuntimeError(\"The model needs to be compiled before being used.\");return this.model.evaluate(e, t, n);\n  }, t.prototype.predict = function (e, t) {\n    return void 0 === t && (t = {}), null == this.model && this.build(), this.model.predict(e, t);\n  }, t.prototype.predictOnBatch = function (e) {\n    return null == this.model && this.build(), this.model.predictOnBatch(e);\n  }, t.prototype.compile = function (e) {\n    this.build(), this.model.compile(e), this.optimizer = this.model.optimizer, this.loss = this.model.loss, this.metrics = this.model.metrics, this.metricsTensors = this.model.metricsTensors, this.metricsNames = this.model.metricsNames;\n  }, t.prototype.fit = function (e, t, n) {\n    return void 0 === n && (n = {}), __awaiter(this, void 0, void 0, function () {\n      return __generator(this, function (r) {\n        if (!this.built) throw new RuntimeError(\"The model needs to be compiled before being used.\");return [2, this.model.fit(e, t, n)];\n      });\n    });\n  }, t.fromConfig = function (e, n) {\n    var r = new e({});if (!(r instanceof t)) throw new ValueError(\"Sequential.fromConfig called on non-Sequential input: \" + r);if (!(n instanceof Array)) throw new ValueError(\"Sequential.fromConfig called without an array of configs\");if (null == n[0].className || \"Merge\" === n[0].className) throw new ValueError(\"Legacy serialization format not supported yet.\");for (var i = 0, a = n; i < a.length; i++) {\n      var o = deserialize(a[i]);r.add(o);\n    }return r;\n  }, Object.defineProperty(t.prototype, \"stopTraining\", { set: function (e) {\n      this.model.stopTraining = e;\n    }, enumerable: !0, configurable: !0 }), t.prototype.getConfig = function () {\n    for (var e = [], t = 0, n = this.layers; t < n.length; t++) {\n      var r = n[t];e.push({ className: r.getClassName(), config: r.getConfig() });\n    }return e;\n  }, t.className = \"Sequential\", t;\n}(Model);function model(e) {\n  return new Model(e);\n}function sequential(e) {\n  return new Sequential(e);\n}function loadModel(e, t) {\n  return void 0 === t && (t = !0), loadModelInternal(e, t);\n}function input(e) {\n  return Input(e);\n}function registerCallbackConstructor(e, t) {\n  CallbackConstructorRegistry.registerCallbackConstructor(e, t);\n}_tfjsCore.serialization.registerClass(Sequential);var Activation = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }return __extends(t, e), t.prototype.getConfig = function () {\n    return {};\n  }, t;\n}(_tfjsCore.serialization.Serializable),\n    Elu = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }return __extends(t, e), t.prototype.apply = function (e, t) {\n    return void 0 === t && (t = 1), elu$1(e, t);\n  }, t.className = \"elu\", t;\n}(Activation);_tfjsCore.serialization.registerClass(Elu);var Selu = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }return __extends(t, e), t.prototype.apply = function (e) {\n    return (0, _tfjsCore.selu)(e);\n  }, t.className = \"selu\", t;\n}(Activation);_tfjsCore.serialization.registerClass(Selu);var Relu = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }return __extends(t, e), t.prototype.apply = function (e) {\n    return (0, _tfjsCore.relu)(e);\n  }, t.className = \"relu\", t;\n}(Activation);_tfjsCore.serialization.registerClass(Relu);var Relu6 = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }return __extends(t, e), t.prototype.apply = function (e) {\n    return (0, _tfjsCore.tidy)(function () {\n      return (0, _tfjsCore.minimum)(getScalar(6), (0, _tfjsCore.relu)(e));\n    });\n  }, t.className = \"relu6\", t;\n}(Activation);_tfjsCore.serialization.registerClass(Relu6);var Linear = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }return __extends(t, e), t.prototype.apply = function (e) {\n    return e;\n  }, t.className = \"linear\", t;\n}(Activation);_tfjsCore.serialization.registerClass(Linear);var Sigmoid = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }return __extends(t, e), t.prototype.apply = function (e) {\n    return (0, _tfjsCore.sigmoid)(e);\n  }, t.className = \"sigmoid\", t;\n}(Activation);_tfjsCore.serialization.registerClass(Sigmoid);var HardSigmoid = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }return __extends(t, e), t.prototype.apply = function (e) {\n    return hardSigmoid(e);\n  }, t.className = \"hardSigmoid\", t;\n}(Activation);_tfjsCore.serialization.registerClass(HardSigmoid);var Softplus = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }return __extends(t, e), t.prototype.apply = function (e) {\n    return (0, _tfjsCore.softplus)(e);\n  }, t.className = \"softplus\", t;\n}(Activation);_tfjsCore.serialization.registerClass(Softplus);var Softsign = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }return __extends(t, e), t.prototype.apply = function (e) {\n    return softsign(e);\n  }, t.className = \"softsign\", t;\n}(Activation);_tfjsCore.serialization.registerClass(Softsign);var Tanh = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }return __extends(t, e), t.prototype.apply = function (e) {\n    return (0, _tfjsCore.tanh)(e);\n  }, t.className = \"tanh\", t;\n}(Activation);_tfjsCore.serialization.registerClass(Tanh);var Softmax = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }return __extends(t, e), t.prototype.apply = function (e, t) {\n    return void 0 === t && (t = -1), (0, _tfjsCore.softmax)(e, t);\n  }, t.className = \"softmax\", t;\n}(Activation);function serializeActivation(e) {\n  return e.getClassName();\n}function deserializeActivation(e, t) {\n  return void 0 === t && (t = {}), deserializeKerasObject(e, _tfjsCore.serialization.SerializationMap.getMap().classNameMap, t, \"activation\");\n}function getActivation(e) {\n  return null == e ? deserializeActivation({ className: \"linear\", config: {} }) : \"string\" == typeof e ? deserializeActivation({ className: e, config: {} }) : e instanceof Activation ? e : deserializeActivation(e);\n}_tfjsCore.serialization.registerClass(Softmax);var ReLU = function (e) {\n  function t(t) {\n    var n = e.call(this, null == t ? {} : t) || this;return n.supportsMasking = !0, null != t && (n.maxValue = t.maxValue), n;\n  }return __extends(t, e), t.prototype.call = function (e, t) {\n    e = getExactlyOneTensor(e);var n = (0, _tfjsCore.relu)(e);return null != this.maxValue && (n = (0, _tfjsCore.clipByValue)(n, 0, this.maxValue)), n;\n  }, t.prototype.computeOutputShape = function (e) {\n    return e;\n  }, t.prototype.getConfig = function () {\n    var t = { maxValue: this.maxValue },\n        n = e.prototype.getConfig.call(this);return Object.assign(t, n), t;\n  }, t.className = \"ReLU\", t;\n}(Layer);_tfjsCore.serialization.registerClass(ReLU);var LeakyReLU = function (e) {\n  function t(t) {\n    var n = e.call(this, null == t ? {} : t) || this;return n.DEFAULT_ALPHA = .3, null == t && (t = {}), n.alpha = null == t.alpha ? n.DEFAULT_ALPHA : t.alpha, n;\n  }return __extends(t, e), t.prototype.call = function (e, t) {\n    var n = getExactlyOneTensor(e);return (0, _tfjsCore.leakyRelu)(n, this.alpha);\n  }, t.prototype.computeOutputShape = function (e) {\n    return e;\n  }, t.prototype.getConfig = function () {\n    var t = { alpha: this.alpha },\n        n = e.prototype.getConfig.call(this);return Object.assign(t, n), t;\n  }, t.className = \"LeakyReLU\", t;\n}(Layer);_tfjsCore.serialization.registerClass(LeakyReLU);var ELU = function (e) {\n  function t(t) {\n    var n = e.call(this, null == t ? {} : t) || this;if (n.DEFAULT_ALPHA = 1, null == t && (t = {}), null != t.alpha && t.alpha !== n.DEFAULT_ALPHA) throw new NotImplementedError(\"Non-default alpha value (\" + t.alpha + \") is not supported by the ELU layer yet.\");return n.alpha = null == t.alpha ? n.DEFAULT_ALPHA : t.alpha, n;\n  }return __extends(t, e), t.prototype.call = function (e, t) {\n    var n = getExactlyOneTensor(e);return (0, _tfjsCore.elu)(n);\n  }, t.prototype.computeOutputShape = function (e) {\n    return e;\n  }, t.prototype.getConfig = function () {\n    var t = { alpha: this.alpha },\n        n = e.prototype.getConfig.call(this);return Object.assign(t, n), t;\n  }, t.className = \"ELU\", t;\n}(Layer);_tfjsCore.serialization.registerClass(ELU);var ThresholdedReLU = function (e) {\n  function t(t) {\n    var n = e.call(this, null == t ? {} : t) || this;return n.DEFAULT_THETA = 1, null == t && (t = {}), n.theta = null == t.theta ? n.DEFAULT_THETA : t.theta, n.thetaTensor = getScalar(n.theta), n;\n  }return __extends(t, e), t.prototype.call = function (e, t) {\n    var n = getExactlyOneTensor(e);return n.mul(cast$1(n.greater(this.thetaTensor), \"float32\"));\n  }, t.prototype.computeOutputShape = function (e) {\n    return e;\n  }, t.prototype.getConfig = function () {\n    var t = { theta: this.theta },\n        n = e.prototype.getConfig.call(this);return Object.assign(t, n), t;\n  }, t.className = \"ThresholdedReLU\", t;\n}(Layer);_tfjsCore.serialization.registerClass(ThresholdedReLU);var Softmax$1 = function (e) {\n  function t(t) {\n    var n = e.call(this, null == t ? {} : t) || this;return n.DEFAULT_AXIS = 1, null == t && (t = {}), n.softmax = new Softmax().apply, n.axis = null == t.axis ? n.DEFAULT_AXIS : t.axis, n;\n  }return __extends(t, e), t.prototype.call = function (e, t) {\n    var n = getExactlyOneTensor(e);return this.softmax(n, this.axis);\n  }, t.prototype.computeOutputShape = function (e) {\n    return e;\n  }, t.prototype.getConfig = function () {\n    var t = { axis: this.axis },\n        n = e.prototype.getConfig.call(this);return Object.assign(t, n), t;\n  }, t.className = \"Softmax\", t;\n}(Layer);_tfjsCore.serialization.registerClass(Softmax$1);var Regularizer = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }return __extends(t, e), t;\n}(_tfjsCore.serialization.Serializable),\n    L1L2 = function (e) {\n  function t(t) {\n    var n = e.call(this) || this,\n        r = null == t || null == t.l1 ? .01 : t.l1,\n        i = null == t || null == t.l2 ? .01 : t.l2;return n.hasL1 = 0 !== r, n.hasL2 = 0 !== i, n.l1 = getScalar(r), n.l2 = getScalar(i), n;\n  }return __extends(t, e), t.prototype.apply = function (e) {\n    var t = this;return (0, _tfjsCore.tidy)(function () {\n      var n = (0, _tfjsCore.zeros)([1]);return t.hasL1 && (n = (0, _tfjsCore.add)(n, (0, _tfjsCore.sum)((0, _tfjsCore.mul)(t.l1, (0, _tfjsCore.abs)(e))))), t.hasL2 && (n = (0, _tfjsCore.add)(n, (0, _tfjsCore.sum)((0, _tfjsCore.mul)(t.l2, square(e))))), n.asScalar();\n    });\n  }, t.prototype.getConfig = function () {\n    return { l1: this.l1.dataSync()[0], l2: this.l2.dataSync()[0] };\n  }, t.fromConfig = function (e, t) {\n    return new e({ l1: t.l1, l2: t.l2 });\n  }, t.className = \"L1L2\", t;\n}(Regularizer);function l1(e) {\n  return new L1L2({ l1: null != e ? e.l1 : null, l2: 0 });\n}function l2(e) {\n  return new L1L2({ l2: null != e ? e.l2 : null, l1: 0 });\n}_tfjsCore.serialization.registerClass(L1L2);var REGULARIZER_IDENTIFIER_REGISTRY_SYMBOL_MAP = { l1l2: \"L1L2\" };function serializeRegularizer(e) {\n  return serializeKerasObject(e);\n}function deserializeRegularizer(e, t) {\n  return void 0 === t && (t = {}), deserializeKerasObject(e, _tfjsCore.serialization.SerializationMap.getMap().classNameMap, t, \"regularizer\");\n}function getRegularizer(e) {\n  return null == e ? null : \"string\" == typeof e ? deserializeRegularizer({ className: e in REGULARIZER_IDENTIFIER_REGISTRY_SYMBOL_MAP ? REGULARIZER_IDENTIFIER_REGISTRY_SYMBOL_MAP[e] : e, config: {} }) : e instanceof Regularizer ? e : deserializeRegularizer(e);\n}function normalizeArray(e, t, n) {\n  if (\"number\" == typeof e) return pyListRepeat(e, t);if (e.length !== t) throw new ValueError(\"The \" + n + \" argument must be a tuple of \" + t + \" integers. Received: \" + e.length + \" elements.\");for (var r = 0; r < t; ++r) {\n    var i = e[r];if (!isInteger(i)) throw new ValueError(\"The \" + n + \" argument must be a tuple of \" + t + \" integers. Received: \" + JSON.stringify(e) + \" including a non-integer number \" + i);\n  }return e;\n}function convOutputLength(e, t, n, r, i) {\n  return void 0 === i && (i = 1), null == e ? e : (a = \"same\" === n ? e : e - (t + (t - 1) * (i - 1)) + 1, Math.floor((a + r - 1) / r));var a;\n}function deconvLength(e, t, n, r) {\n  if (null == e) return null;if (\"valid\" === r) e = e * t + max$1([n - t, 0]);else {\n    if (\"same\" !== r) throw new ValueError(\"Unsupport padding mode: \" + r + \".\");e *= t;\n  }return e;\n}function preprocessConv2DInput(e, t) {\n  return (0, _tfjsCore.tidy)(function () {\n    return checkDataFormat(t), \"channelsFirst\" === t ? (0, _tfjsCore.transpose)(e, [0, 2, 3, 1]) : e;\n  });\n}function conv1dWithBias(e, t, n, r, i, a, o) {\n  return void 0 === r && (r = 1), void 0 === i && (i = \"valid\"), void 0 === o && (o = 1), (0, _tfjsCore.tidy)(function () {\n    if (null == a && (a = imageDataFormat()), checkDataFormat(a), 3 !== e.shape.length) throw new ValueError(\"The input of a conv1dWithBias operation should be 3, but is \" + e.shape.length + \" instead.\");if (3 !== t.shape.length) throw new ValueError(\"The kernel for a conv1dWithBias operation should be 3, but is \" + t.shape.length + \" instead\");if (null != n && 1 !== n.shape.length) throw new ValueError(\"The bias for a conv1dWithBias operation should be 1, but is \" + t.shape.length + \" instead\");if (\"channelsFirst\" === a && (e = (0, _tfjsCore.transpose)(e, [0, 2, 1])), \"causal\" === i) throw new NotImplementedError(\"The support for CAUSAL padding mode in conv1dWithBias is not implemented yet.\");var s = (0, _tfjsCore.conv1d)(e, t, r, \"same\" === i ? \"same\" : \"valid\", \"NWC\", o);return null != n && (s = biasAdd(s, n)), s;\n  });\n}function conv2dWithBias(e, t, n, r, i, a, o) {\n  return void 0 === r && (r = [1, 1]), void 0 === i && (i = \"valid\"), (0, _tfjsCore.tidy)(function () {\n    if (null == a && (a = imageDataFormat()), checkDataFormat(a), 3 !== e.rank && 4 !== e.rank) throw new ValueError(\"conv2dWithBias expects input to be of rank 3 or 4, but received \" + e.rank + \".\");if (3 !== t.rank && 4 !== t.rank) throw new ValueError(\"conv2dWithBias expects kernel to be of rank 3 or 4, but received \" + e.rank + \".\");var s = preprocessConv2DInput(e, a);if (\"causal\" === i) throw new NotImplementedError(\"The support for CAUSAL padding mode in conv1dWithBias is not implemented yet.\");return s = (0, _tfjsCore.conv2d)(s, t, r, \"same\" === i ? \"same\" : \"valid\", \"NHWC\", o), null != n && (s = biasAdd(s, n)), \"channelsFirst\" === a && (s = (0, _tfjsCore.transpose)(s, [0, 3, 1, 2])), s;\n  });\n}var BaseConv = function (e) {\n  function t(n, r) {\n    var i = e.call(this, r) || this;if (i.bias = null, i.DEFAULT_KERNEL_INITIALIZER = \"glorotNormal\", i.DEFAULT_BIAS_INITIALIZER = \"zeros\", t.verifyConfig(r), i.rank = n, 1 !== i.rank && 2 !== i.rank) throw new NotImplementedError(\"Convolution layer for rank other than 1 or 2 (\" + i.rank + \") is not implemented yet.\");if (i.kernelSize = normalizeArray(r.kernelSize, n, \"kernelSize\"), i.strides = normalizeArray(null == r.strides ? 1 : r.strides, n, \"strides\"), i.padding = null == r.padding ? \"valid\" : r.padding, checkPaddingMode(i.padding), i.dataFormat = null == r.dataFormat ? \"channelsLast\" : r.dataFormat, checkDataFormat(i.dataFormat), i.activation = getActivation(r.activation), i.useBias = null == r.useBias || r.useBias, i.biasInitializer = getInitializer(r.biasInitializer || i.DEFAULT_BIAS_INITIALIZER), i.biasConstraint = getConstraint(r.biasConstraint), i.biasRegularizer = getRegularizer(r.biasRegularizer), i.activityRegularizer = getRegularizer(r.activityRegularizer), i.dilationRate = normalizeArray(null == r.dilationRate ? 1 : r.dilationRate, n, \"dilationRate\"), 1 === i.rank && Array.isArray(i.dilationRate) && 1 !== i.dilationRate.length) throw new ValueError(\"dilationRate must be a number or an array of a single number for 1D convolution, but received \" + JSON.stringify(i.dilationRate));if (2 === i.rank) if (\"number\" == typeof i.dilationRate) i.dilationRate = [i.dilationRate, i.dilationRate];else if (2 !== i.dilationRate.length) throw new ValueError(\"dilationRate must be a number or array of two numbers for 2D convolution, but received \" + JSON.stringify(i.dilationRate));return i;\n  }return __extends(t, e), t.verifyConfig = function (e) {\n    if (assert(\"kernelSize\" in e, \"required key 'kernelSize' not in config\"), \"number\" != typeof e.kernelSize && !checkArrayTypeAndLength(e.kernelSize, \"number\", 1, 2)) throw new ValueError(\"BaseConv expects config.kernelSize to be number or number[] with length 1 or 2, but received \" + JSON.stringify(e.kernelSize) + \".\");\n  }, t.prototype.getConfig = function () {\n    var t = { kernelSize: this.kernelSize, strides: this.strides, padding: this.padding, dataFormat: this.dataFormat, dilationRate: this.dilationRate, activation: serializeActivation(this.activation), useBias: this.useBias, biasInitializer: serializeInitializer(this.biasInitializer), biasRegularizer: serializeRegularizer(this.biasRegularizer), activityRegularizer: serializeRegularizer(this.activityRegularizer), biasConstraint: serializeConstraint(this.biasConstraint) },\n        n = e.prototype.getConfig.call(this);return Object.assign(t, n), t;\n  }, t;\n}(Layer),\n    Conv = function (e) {\n  function t(n, r) {\n    var i = e.call(this, n, r) || this;return i.kernel = null, t.verifyConfig(r), i.filters = r.filters, i.kernelInitializer = getInitializer(r.kernelInitializer || i.DEFAULT_KERNEL_INITIALIZER), i.kernelConstraint = getConstraint(r.kernelConstraint), i.kernelRegularizer = getRegularizer(r.kernelRegularizer), i;\n  }return __extends(t, e), t.prototype.build = function (e) {\n    e = getExactlyOneShape(e);var t = \"channelsFirst\" === this.dataFormat ? 1 : e.length - 1;if (null == e[t]) throw new ValueError(\"The channel dimension of the input should be defined. Found \" + e[t]);var n,\n        r = e[t],\n        i = this.kernelSize.concat([r, this.filters]);this.kernel = this.addWeight(\"kernel\", i, null, this.kernelInitializer, this.kernelRegularizer, !0, this.kernelConstraint), this.useBias && (this.bias = this.addWeight(\"bias\", [this.filters], null, this.biasInitializer, this.biasRegularizer, !0, this.biasConstraint)), this.inputSpec = [{ ndim: this.rank + 2, axes: (n = {}, n[t] = r, n) }], this.built = !0;\n  }, t.prototype.call = function (e, t) {\n    var n = this;return (0, _tfjsCore.tidy)(function () {\n      var t;e = getExactlyOneTensor(e);var r = null == n.bias ? null : n.bias.read();if (1 === n.rank) t = conv1dWithBias(e, n.kernel.read(), r, n.strides[0], n.padding, n.dataFormat, n.dilationRate[0]);else if (2 === n.rank) t = conv2dWithBias(e, n.kernel.read(), r, n.strides, n.padding, n.dataFormat, n.dilationRate);else if (3 === n.rank) throw new NotImplementedError(\"3D convolution is not implemented yet.\");return null != n.activation && (t = n.activation.apply(t)), t;\n    });\n  }, t.prototype.computeOutputShape = function (e) {\n    e = getExactlyOneShape(e);for (var t = [], n = \"channelsLast\" === this.dataFormat ? e.slice(1, e.length - 1) : e.slice(2), r = 0; r < n.length; ++r) {\n      var i = convOutputLength(n[r], this.kernelSize[r], this.padding, this.strides[r], \"number\" == typeof this.dilationRate ? this.dilationRate : this.dilationRate[r]);t.push(i);\n    }var a = [e[0]];return \"channelsLast\" === this.dataFormat ? (a = a.concat(t)).push(this.filters) : (a.push(this.filters), a = a.concat(t)), a;\n  }, t.prototype.getConfig = function () {\n    var t = { filters: this.filters, kernelInitializer: serializeInitializer(this.kernelInitializer), kernelRegularizer: serializeRegularizer(this.kernelRegularizer), kernelConstraint: serializeConstraint(this.kernelConstraint) },\n        n = e.prototype.getConfig.call(this);return Object.assign(t, n), t;\n  }, t.verifyConfig = function (e) {\n    if (!(\"filters\" in e) || \"number\" != typeof e.filters || e.filters < 1) throw new ValueError(\"Convolution layer expected config.filters to be a 'number' > 0 but got \" + JSON.stringify(e.filters));\n  }, t;\n}(BaseConv),\n    Conv2D = function (e) {\n  function t(n) {\n    var r = e.call(this, 2, n) || this;return t.verifyConfig(n), r;\n  }return __extends(t, e), t.prototype.getConfig = function () {\n    var t = e.prototype.getConfig.call(this);return delete t.rank, t;\n  }, t.verifyConfig = function (e) {\n    if (\"number\" != typeof e.kernelSize && !checkArrayTypeAndLength(e.kernelSize, \"number\", 1, 2)) throw new ValueError(\"Conv2D expects config.kernelSize to be number or number[] with length 1 or 2, but received \" + JSON.stringify(e.kernelSize) + \".\");\n  }, t.className = \"Conv2D\", t;\n}(Conv);_tfjsCore.serialization.registerClass(Conv2D);var Conv2DTranspose = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this;if (n.inputSpec = [new InputSpec({ ndim: 4 })], \"same\" !== n.padding && \"valid\" !== n.padding) throw new ValueError(\"Conv2DTranspose currently supports only padding modes 'same' and 'valid', but received padding mode \" + n.padding);return n;\n  }return __extends(t, e), t.prototype.build = function (e) {\n    if (4 !== (e = getExactlyOneShape(e)).length) throw new ValueError(\"Input should have rank 4; Received input shape: \" + JSON.stringify(e));var t = \"channelsFirst\" === this.dataFormat ? 1 : e.length - 1;if (null == e[t]) throw new ValueError(\"The channel dimension of the inputs should be defined. Found `None`.\");var n,\n        r = e[t],\n        i = this.kernelSize.concat([this.filters, r]);this.kernel = this.addWeight(\"kernel\", i, \"float32\", this.kernelInitializer, this.kernelRegularizer, !0, this.kernelConstraint), this.useBias && (this.bias = this.addWeight(\"bias\", [this.filters], \"float32\", this.biasInitializer, this.biasRegularizer, !0, this.biasConstraint)), this.inputSpec = [new InputSpec({ ndim: 4, axes: (n = {}, n[t] = r, n) })], this.built = !0;\n  }, t.prototype.call = function (e, t) {\n    var n = this;return (0, _tfjsCore.tidy)(function () {\n      var t = getExactlyOneTensor(e);if (4 !== t.shape.length) throw new ValueError(\"Conv2DTranspose.call() expects input tensor to be rank-4, but received a tensor of rank-\" + t.shape.length);var r,\n          i,\n          a = t.shape,\n          o = a[0];\"channelsFirst\" === n.dataFormat ? (r = 2, i = 3) : (r = 1, i = 2);var s = a[r],\n          l = a[i],\n          u = n.kernelSize[0],\n          c = n.kernelSize[1],\n          p = n.strides[0],\n          h = n.strides[1],\n          d = [o, deconvLength(s, p, u, n.padding), deconvLength(l, h, c, n.padding), n.filters];\"channelsLast\" !== n.dataFormat && (t = (0, _tfjsCore.transpose)(t, [0, 2, 3, 1]));var g = (0, _tfjsCore.conv2dTranspose)(t, n.kernel.read(), d, n.strides, n.padding);return \"channelsLast\" !== n.dataFormat && (g = (0, _tfjsCore.transpose)(g, [0, 3, 1, 2])), null != n.bias && (g = biasAdd(g, n.bias.read(), n.dataFormat)), null != n.activation && (g = n.activation.apply(g)), g;\n    });\n  }, t.prototype.computeOutputShape = function (e) {\n    var t,\n        n,\n        r,\n        i = (e = getExactlyOneShape(e)).slice();\"channelsFirst\" === this.dataFormat ? (t = 1, n = 2, r = 3) : (t = 3, n = 1, r = 2);var a = this.kernelSize[0],\n        o = this.kernelSize[1],\n        s = this.strides[0],\n        l = this.strides[1];return i[t] = this.filters, i[n] = deconvLength(i[n], s, a, this.padding), i[r] = deconvLength(i[r], l, o, this.padding), i;\n  }, t.prototype.getConfig = function () {\n    var t = e.prototype.getConfig.call(this);return delete t.dilationRate, t;\n  }, t.className = \"Conv2DTranspose\", t;\n}(Conv2D);_tfjsCore.serialization.registerClass(Conv2DTranspose);var SeparableConv = function (e) {\n  function t(t, n) {\n    var r = e.call(this, t, n) || this;if (r.DEFAULT_DEPTHWISE_INITIALIZER = \"glorotUniform\", r.DEFAULT_POINTWISE_INITIALIZER = \"glorotUniform\", r.depthwiseKernel = null, r.pointwiseKernel = null, null == n.filters) throw new ValueError(\"The `filters` configuration field is required by SeparableConv, but is unspecified.\");if (null != n.kernelInitializer || null != n.kernelRegularizer || null != n.kernelConstraint) throw new ValueError(\"Fields kernelInitializer, kernelRegularizer and kernelConstraint are invalid for SeparableConv2D. Use depthwiseInitializer, depthwiseRegularizer, depthwiseConstraint, pointwiseInitializer, pointwiseRegularizer and pointwiseConstraint instead.\");if (null != n.padding && \"same\" !== n.padding && \"valid\" !== n.padding) throw new ValueError(\"SeparableConv\" + r.rank + \"D supports only padding modes: 'same' and 'valid', but received \" + JSON.stringify(n.padding));return r.depthMultiplier = null == n.depthMultiplier ? 1 : n.depthMultiplier, r.depthwiseInitializer = getInitializer(n.depthwiseInitializer || r.DEFAULT_DEPTHWISE_INITIALIZER), r.depthwiseRegularizer = getRegularizer(n.depthwiseRegularizer), r.depthwiseConstraint = getConstraint(n.depthwiseConstraint), r.pointwiseInitializer = getInitializer(n.depthwiseInitializer || r.DEFAULT_POINTWISE_INITIALIZER), r.pointwiseRegularizer = getRegularizer(n.pointwiseRegularizer), r.pointwiseConstraint = getConstraint(n.pointwiseConstraint), r;\n  }return __extends(t, e), t.prototype.build = function (e) {\n    if ((e = getExactlyOneShape(e)).length < this.rank + 2) throw new ValueError(\"Inputs to SeparableConv\" + this.rank + \"D should have rank \" + (this.rank + 2) + \", but received input shape: \" + JSON.stringify(e));var t = \"channelsFirst\" === this.dataFormat ? 1 : e.length - 1;if (null == e[t] || e[t] < 0) throw new ValueError(\"The channel dimension of the inputs should be defined, but found \" + JSON.stringify(e[t]));for (var n = e[t], r = this.kernelSize.concat([n, this.depthMultiplier]), i = [], a = 0; a < this.rank; ++a) i.push(1);i.push(n * this.depthMultiplier, this.filters);var o;this.depthwiseKernel = this.addWeight(\"depthwise_kernel\", r, \"float32\", this.depthwiseInitializer, this.depthwiseRegularizer, !0, this.depthwiseConstraint), this.pointwiseKernel = this.addWeight(\"pointwise_kernel\", i, \"float32\", this.pointwiseInitializer, this.pointwiseRegularizer, !0, this.pointwiseConstraint), this.useBias ? this.bias = this.addWeight(\"bias\", [this.filters], \"float32\", this.biasInitializer, this.biasRegularizer, !0, this.biasConstraint) : this.bias = null, this.inputSpec = [new InputSpec({ ndim: this.rank + 2, axes: (o = {}, o[t] = n, o) })], this.built = !0;\n  }, t.prototype.call = function (e, t) {\n    var n = this;return (0, _tfjsCore.tidy)(function () {\n      var t;if (e = getExactlyOneTensor(e), 1 === n.rank) throw new NotImplementedError(\"1D separable convolution is not implemented yet.\");return 2 === n.rank && (\"channelsFirst\" === n.dataFormat && (e = (0, _tfjsCore.transpose)(e, [0, 2, 3, 1])), t = (0, _tfjsCore.separableConv2d)(e, n.depthwiseKernel.read(), n.pointwiseKernel.read(), n.strides, n.padding, n.dilationRate, \"NHWC\")), n.useBias && (t = biasAdd(t, n.bias.read(), n.dataFormat)), null != n.activation && (t = n.activation.apply(t)), \"channelsFirst\" === n.dataFormat && (t = (0, _tfjsCore.transpose)(t, [0, 3, 1, 2])), t;\n    });\n  }, t.prototype.getConfig = function () {\n    var t = e.prototype.getConfig.call(this);return delete t.rank, delete t.kernelInitializer, delete t.kernelRegularizer, delete t.kernelConstraint, t.depthwiseInitializer = serializeInitializer(this.depthwiseInitializer), t.pointwiseInitializer = serializeInitializer(this.pointwiseInitializer), t.depthwiseRegularizer = serializeRegularizer(this.depthwiseRegularizer), t.pointwiseRegularizer = serializeRegularizer(this.pointwiseRegularizer), t.depthwiseConstraint = serializeConstraint(this.depthwiseConstraint), t.pointwiseConstraint = serializeConstraint(this.pointwiseConstraint), t;\n  }, t.className = \"SeparableConv\", t;\n}(Conv),\n    SeparableConv2D = function (e) {\n  function t(t) {\n    return e.call(this, 2, t) || this;\n  }return __extends(t, e), t.className = \"SeparableConv2D\", t;\n}(SeparableConv);_tfjsCore.serialization.registerClass(SeparableConv2D);var Conv1D = function (e) {\n  function t(n) {\n    var r = e.call(this, 1, n) || this;return t.verifyConfig(n), r.inputSpec = [{ ndim: 3 }], r;\n  }return __extends(t, e), t.prototype.getConfig = function () {\n    var t = e.prototype.getConfig.call(this);return delete t.rank, delete t.dataFormat, t;\n  }, t.verifyConfig = function (e) {\n    if (\"number\" != typeof e.kernelSize && !checkArrayTypeAndLength(e.kernelSize, \"number\", 1, 1)) throw new ValueError(\"Conv1D expects config.kernelSize to be number or number[] with length 1, but received \" + JSON.stringify(e.kernelSize) + \".\");\n  }, t.className = \"Conv1D\", t;\n}(Conv);_tfjsCore.serialization.registerClass(Conv1D);var Cropping2D = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this;return \"number\" == typeof t.cropping ? n.cropping = [[t.cropping, t.cropping], [t.cropping, t.cropping]] : \"number\" == typeof t.cropping[0] ? n.cropping = [[t.cropping[0], t.cropping[0]], [t.cropping[1], t.cropping[1]]] : n.cropping = t.cropping, n.dataFormat = void 0 === t.dataFormat ? \"channelsLast\" : t.dataFormat, n.inputSpec = [{ ndim: 4 }], n;\n  }return __extends(t, e), t.prototype.computeOutputShape = function (e) {\n    return \"channelsFirst\" === this.dataFormat ? [e[0], e[1], e[2] - this.cropping[0][0] - this.cropping[0][1], e[3] - this.cropping[1][0] - this.cropping[1][1]] : [e[0], e[1] - this.cropping[0][0] - this.cropping[0][1], e[2] - this.cropping[1][0] - this.cropping[1][1], e[3]];\n  }, t.prototype.call = function (e, t) {\n    var n = this;return (0, _tfjsCore.tidy)(function () {\n      return e = getExactlyOneTensor(e), \"channelsLast\" === n.dataFormat ? sliceAlongAxis(sliceAlongAxis(e, n.cropping[0][0], e.shape[1] - n.cropping[0][0] - n.cropping[0][1], 2), n.cropping[1][0], e.shape[2] - n.cropping[1][1] - n.cropping[1][0], 3) : sliceAlongAxis(sliceAlongAxis(e, n.cropping[0][0], e.shape[2] - n.cropping[0][0] - n.cropping[0][1], 3), n.cropping[1][0], e.shape[3] - n.cropping[1][1] - n.cropping[1][0], 4);\n    });\n  }, t.prototype.getConfig = function () {\n    var t = { cropping: this.cropping, dataFormat: this.dataFormat },\n        n = e.prototype.getConfig.call(this);return Object.assign(t, n), t;\n  }, t.className = \"Cropping2D\", t;\n}(Layer);_tfjsCore.serialization.registerClass(Cropping2D);var UpSampling2D = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this;return n.DEFAULT_SIZE = [2, 2], n.inputSpec = [{ ndim: 4 }], n.size = null == t.size ? n.DEFAULT_SIZE : t.size, n.dataFormat = null == t.dataFormat ? \"channelsLast\" : t.dataFormat, n;\n  }return __extends(t, e), t.prototype.computeOutputShape = function (e) {\n    if (\"channelsFirst\" === this.dataFormat) {\n      var t = null == e[2] ? null : this.size[0] * e[2],\n          n = null == e[3] ? null : this.size[1] * e[3];return [e[0], e[1], t, n];\n    }t = null == e[1] ? null : this.size[0] * e[1], n = null == e[2] ? null : this.size[1] * e[2];return [e[0], t, n, e[3]];\n  }, t.prototype.call = function (e, t) {\n    var n = this;return (0, _tfjsCore.tidy)(function () {\n      var t = getExactlyOneTensor(e),\n          r = t.shape;if (\"channelsFirst\" === n.dataFormat) {\n        t = (0, _tfjsCore.transpose)(t, [0, 2, 3, 1]);var i = n.size[0] * r[2],\n            a = n.size[1] * r[3],\n            o = t.resizeNearestNeighbor([i, a]);return (0, _tfjsCore.transpose)(o, [0, 3, 1, 2]);\n      }i = n.size[0] * r[1], a = n.size[1] * r[2];return t.resizeNearestNeighbor([i, a]);\n    });\n  }, t.prototype.getConfig = function () {\n    var t = { size: this.size, dataFormat: this.dataFormat },\n        n = e.prototype.getConfig.call(this);return Object.assign(t, n), t;\n  }, t.className = \"UpSampling2D\", t;\n}(Layer);function depthwiseConv2d$1(e, t, n, r, i, a) {\n  return void 0 === n && (n = [1, 1]), void 0 === r && (r = \"valid\"), (0, _tfjsCore.tidy)(function () {\n    null == i && (i = imageDataFormat()), checkDataFormat(i);var o = preprocessConv2DInput(e, i);if (4 !== e.rank) throw new ValueError(\"Input for depthwiseConv2d is required to be 4-D, but is instead \" + e.rank + \"-D\");if (4 !== t.rank) throw new ValueError(\"depthwiseKernel is required to be 4-D, but is instead \" + t.rank + \"-D\");return o = (0, _tfjsCore.depthwiseConv2d)(o, t, n, \"same\" === r ? \"same\" : \"valid\", \"NHWC\", a), \"channelsFirst\" === i && (o = (0, _tfjsCore.transpose)(o, [0, 3, 1, 2])), o;\n  });\n}_tfjsCore.serialization.registerClass(UpSampling2D);var DepthwiseConv2D = function (e) {\n  function t(t) {\n    var n = e.call(this, 2, t) || this;return n.depthwiseKernel = null, n.depthMultiplier = null == t.depthMultiplier ? 1 : t.depthMultiplier, n.depthwiseInitializer = getInitializer(t.depthwiseInitializer || n.DEFAULT_KERNEL_INITIALIZER), n.depthwiseConstraint = getConstraint(t.depthwiseConstraint), n.depthwiseRegularizer = getRegularizer(t.depthwiseRegularizer), n;\n  }return __extends(t, e), t.prototype.build = function (e) {\n    if ((e = getExactlyOneShape(e)).length < 4) throw new ValueError(\"Inputs to DepthwiseConv2D should have rank 4. Received input shape: \" + JSON.stringify(e) + \".\");var t = \"channelsFirst\" === this.dataFormat ? 1 : 3;if (null == e[t] || e[t] < 0) throw new ValueError(\"The channel dimension of the inputs to DepthwiseConv2D should be defined, but is not (\" + e[t] + \").\");var n = e[t],\n        r = [this.kernelSize[0], this.kernelSize[1], n, this.depthMultiplier];this.depthwiseKernel = this.addWeight(\"depthwise_kernel\", r, null, this.depthwiseInitializer, this.depthwiseRegularizer, !0, this.depthwiseConstraint), this.useBias ? this.bias = this.addWeight(\"bias\", [n * this.depthMultiplier], null, this.biasInitializer, this.biasRegularizer, !0, this.biasConstraint) : this.bias = null, this.built = !0;\n  }, t.prototype.call = function (e, t) {\n    var n = this;return (0, _tfjsCore.tidy)(function () {\n      var t = depthwiseConv2d$1(e = getExactlyOneTensor(e), n.depthwiseKernel.read(), n.strides, n.padding, n.dataFormat, null);return n.useBias && (t = biasAdd(t, n.bias.read(), n.dataFormat)), null != n.activation && (t = n.activation.apply(t)), t;\n    });\n  }, t.prototype.computeOutputShape = function (e) {\n    e = getExactlyOneShape(e);var t = \"channelsFirst\" === this.dataFormat ? e[2] : e[1],\n        n = \"channelsFirst\" === this.dataFormat ? e[3] : e[2],\n        r = \"channelsFirst\" === this.dataFormat ? e[1] * this.depthMultiplier : e[3] * this.depthMultiplier,\n        i = convOutputLength(t, this.kernelSize[0], this.padding, this.strides[0]),\n        a = convOutputLength(n, this.kernelSize[1], this.padding, this.strides[1]);return \"channelsFirst\" === this.dataFormat ? [e[0], r, i, a] : [e[0], i, a, r];\n  }, t.prototype.getConfig = function () {\n    var t = e.prototype.getConfig.call(this);return t.depthMultiplier = this.depthMultiplier, t.depthwiseInitializer = serializeInitializer(this.depthwiseInitializer), t.depthwiseRegularizer = serializeRegularizer(this.depthwiseRegularizer), t.depthwiseConstraint = serializeConstraint(this.depthwiseRegularizer), t;\n  }, t.className = \"DepthwiseConv2D\", t;\n}(BaseConv);_tfjsCore.serialization.registerClass(DepthwiseConv2D);var Dropout = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this;if (n.rate = Math.max(Math.min(t.rate, 1), 0), n.rateScalar = getScalar(n.rate), n.noiseShape = t.noiseShape, n.seed = t.seed, null != n.seed) throw new NotImplementedError(\"Non-default seed is not implemented in Dropout layer yet: \" + n.seed);return n.supportsMasking = !0, n;\n  }return __extends(t, e), t.prototype.getNoiseShape = function (e) {\n    if (null == this.noiseShape) return this.noiseShape;for (var t = e.shape, n = [], r = 0; r < this.noiseShape.length; ++r) n.push(null == this.noiseShape[r] ? t[r] : this.noiseShape[r]);return n;\n  }, t.prototype.call = function (e, t) {\n    var n = this;return (0, _tfjsCore.tidy)(function () {\n      n.invokeCallHook(e, t);var r = getExactlyOneTensor(e);if (null != n.noiseShape && !_tfjsCore.util.arraysEqual(r.shape, n.noiseShape)) throw new NotImplementedError(\"Non-default noise shape is not implemented in Dropout layer yet: \" + JSON.stringify(n.noiseShape));if (0 < n.rate && n.rate < 1) {\n        var i = null != t.training && t.training,\n            a = n.getNoiseShape(r);return inTrainPhase(function () {\n          return dropout(r, n.rateScalar, a, n.seed);\n        }, function () {\n          return r;\n        }, i);\n      }return e;\n    });\n  }, t.prototype.getConfig = function () {\n    var t = { rate: this.rate, noiseShape: this.noiseShape, seed: this.seed },\n        n = e.prototype.getConfig.call(this);return Object.assign(t, n), t;\n  }, t.className = \"Dropout\", t;\n}(Layer);_tfjsCore.serialization.registerClass(Dropout);var Dense = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this;if (n.activation = null, n.useBias = !0, n.kernel = null, n.bias = null, n.DEFAULT_KERNEL_INITIALIZER = \"glorotNormal\", n.DEFAULT_BIAS_INITIALIZER = \"zeros\", null == t.batchInputShape && null == t.inputShape && null != t.inputDim) {\n      var r = null;null != t.batchSize && (r = t.batchSize), n.batchInputShape = [r, t.inputDim];\n    }return n.units = t.units, n.activation = getActivation(t.activation), null != t.useBias && (n.useBias = t.useBias), n.kernelInitializer = getInitializer(t.kernelInitializer || n.DEFAULT_KERNEL_INITIALIZER), n.biasInitializer = getInitializer(t.biasInitializer || n.DEFAULT_BIAS_INITIALIZER), n.kernelConstraint = getConstraint(t.kernelConstraint), n.biasConstraint = getConstraint(t.biasConstraint), n.kernelRegularizer = getRegularizer(t.kernelRegularizer), n.biasRegularizer = getRegularizer(t.biasRegularizer), n.activityRegularizer = getRegularizer(t.activityRegularizer), n.inputSpec = [{ minNDim: 2 }], n;\n  }return __extends(t, e), t.prototype.build = function (e) {\n    var t,\n        n = (e = getExactlyOneShape(e))[e.length - 1];null == this.kernel && (this.kernel = this.addWeight(\"kernel\", [n, this.units], null, this.kernelInitializer, this.kernelRegularizer, !0, this.kernelConstraint), this.useBias && (this.bias = this.addWeight(\"bias\", [this.units], null, this.biasInitializer, this.biasRegularizer, !0, this.biasConstraint))), this.inputSpec = [{ minNDim: 2, axes: (t = {}, t[-1] = n, t) }], this.built = !0;\n  }, t.prototype.computeOutputShape = function (e) {\n    var t = (e = getExactlyOneShape(e)).slice();return t[t.length - 1] = this.units, t;\n  }, t.prototype.call = function (e, t) {\n    var n = this;return (0, _tfjsCore.tidy)(function () {\n      n.invokeCallHook(e, t);var r = dot(getExactlyOneTensor(e), n.kernel.read());return null != n.bias && (r = biasAdd(r, n.bias.read())), null != n.activation && (r = n.activation.apply(r)), r;\n    });\n  }, t.prototype.getConfig = function () {\n    var t = { units: this.units, activation: serializeActivation(this.activation), useBias: this.useBias, kernelInitializer: serializeInitializer(this.kernelInitializer), biasInitializer: serializeInitializer(this.biasInitializer), kernelRegularizer: serializeRegularizer(this.kernelRegularizer), biasRegularizer: serializeRegularizer(this.biasRegularizer), activityRegularizer: serializeRegularizer(this.activityRegularizer), kernelConstraint: serializeConstraint(this.kernelConstraint), biasConstraint: serializeConstraint(this.biasConstraint) },\n        n = e.prototype.getConfig.call(this);return Object.assign(t, n), t;\n  }, t.className = \"Dense\", t;\n}(Layer);_tfjsCore.serialization.registerClass(Dense);var Flatten = function (e) {\n  function t(t) {\n    var n = e.call(this, t || {}) || this;return n.inputSpec = [{ minNDim: 3 }], n;\n  }return __extends(t, e), t.prototype.computeOutputShape = function (e) {\n    for (var t = 0, n = (e = getExactlyOneShape(e)).slice(1); t < n.length; t++) {\n      if (null == n[t]) throw new ValueError('The shape of the input to \"Flatten\" is not fully defined (got ' + e.slice(1) + '). Make sure to pass a complete \"input_shape\" or \"batch_input_shape\" argument to the first layer in your model.');\n    }return [e[0], arrayProd(e, 1)];\n  }, t.prototype.call = function (e, t) {\n    var n = this;return (0, _tfjsCore.tidy)(function () {\n      return n.invokeCallHook(e, t), batchFlatten(getExactlyOneTensor(e));\n    });\n  }, t.className = \"Flatten\", t;\n}(Layer);_tfjsCore.serialization.registerClass(Flatten);var Activation$1 = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this;return n.supportsMasking = !0, n.activation = getActivation(t.activation), n;\n  }return __extends(t, e), t.prototype.call = function (e, t) {\n    var n = this;return (0, _tfjsCore.tidy)(function () {\n      n.invokeCallHook(e, t);var r = getExactlyOneTensor(e);return n.activation.apply(r);\n    });\n  }, t.prototype.getConfig = function () {\n    var t = { activation: serializeActivation(this.activation) },\n        n = e.prototype.getConfig.call(this);return Object.assign(t, n), t;\n  }, t.className = \"Activation\", t;\n}(Layer);_tfjsCore.serialization.registerClass(Activation$1);var RepeatVector = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this;return n.n = t.n, n.inputSpec = [{ ndim: 2 }], n;\n  }return __extends(t, e), t.prototype.computeOutputShape = function (e) {\n    return [e[0], this.n, e[1]];\n  }, t.prototype.call = function (e, t) {\n    var n = this;return (0, _tfjsCore.tidy)(function () {\n      return repeat(e = getExactlyOneTensor(e), n.n);\n    });\n  }, t.prototype.getConfig = function () {\n    var t = { n: this.n },\n        n = e.prototype.getConfig.call(this);return Object.assign(t, n), t;\n  }, t.className = \"RepeatVector\", t;\n}(Layer);_tfjsCore.serialization.registerClass(RepeatVector);var Reshape = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this;n.targetShape = t.targetShape;for (var r = 0; r < n.targetShape.length; ++r) n.isUnknown(n.targetShape[r]) && (n.targetShape[r] = null);return n;\n  }return __extends(t, e), t.prototype.isUnknown = function (e) {\n    return e < 0 || null == e;\n  }, t.prototype.fixUnknownDimension = function (e, t) {\n    for (var n = \"Total size of new array must be unchanged.\", r = t.slice(), i = 1, a = null, o = 0; o < r.length; ++o) {\n      var s = r[o];if (this.isUnknown(s)) {\n        if (null !== a) throw new ValueError(\"Can only specifiy one unknown dimension.\");a = o;\n      } else i *= s;\n    }var l = arrayProd(e);if (null !== a) {\n      if (0 === i || l % i != 0) throw new ValueError(n);r[a] = l / i;\n    } else if (l !== i) throw new ValueError(n);return r;\n  }, t.prototype.computeOutputShape = function (e) {\n    for (var t = !1, n = 0; n < e.length; ++n) if (this.isUnknown(e[n])) {\n      t = !0;break;\n    }return t ? e.slice(0, 1).concat(this.targetShape) : e.slice(0, 1).concat(this.fixUnknownDimension(e.slice(1), this.targetShape));\n  }, t.prototype.call = function (e, t) {\n    var n = this;return (0, _tfjsCore.tidy)(function () {\n      n.invokeCallHook(e, t);var r = getExactlyOneTensor(e),\n          i = r.shape,\n          a = i.slice(0, 1).concat(n.fixUnknownDimension(i.slice(1), n.targetShape));return r.reshape(a);\n    });\n  }, t.prototype.getConfig = function () {\n    var t = { targetShape: this.targetShape },\n        n = e.prototype.getConfig.call(this);return Object.assign(t, n), t;\n  }, t.className = \"Reshape\", t;\n}(Layer);_tfjsCore.serialization.registerClass(Reshape);var Permute = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this;if (null == t.dims) throw new Error(\"Required configuration field `dims` is missing during Permute constructor call.\");if (!Array.isArray(t.dims)) throw new Error(\"Permute constructor requires `dims` to be an Array, but received \" + t.dims + \" instead.\");var r = range(1, t.dims.length + 1);if (!_tfjsCore.util.arraysEqual(t.dims.slice().sort(), r)) throw new Error(\"Invalid permutation `dims`: \" + JSON.stringify(t.dims) + \" `dims` must contain consecutive integers starting from 1.\");return n.dims = t.dims, n.dimsIncludingBatch = [0].concat(n.dims), n.inputSpec = [new InputSpec({ ndim: n.dims.length + 1 })], n;\n  }return __extends(t, e), t.prototype.computeOutputShape = function (e) {\n    var t = (e = getExactlyOneShape(e)).slice();return this.dims.forEach(function (n, r) {\n      t[r + 1] = e[n];\n    }), t;\n  }, t.prototype.call = function (e, t) {\n    return (0, _tfjsCore.transpose)(getExactlyOneTensor(e), this.dimsIncludingBatch);\n  }, t.prototype.getConfig = function () {\n    var t = { dims: this.dims },\n        n = e.prototype.getConfig.call(this);return Object.assign(t, n), t;\n  }, t.className = \"Permute\", t;\n}(Layer);_tfjsCore.serialization.registerClass(Permute);var Embedding = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this;if (n.embeddings = null, n.DEFAULT_EMBEDDINGS_INITIALIZER = \"randomUniform\", null == t.batchInputShape && null == t.inputShape) {\n      var r = null;null != t.batchSize && (r = t.batchSize), null == t.inputLength ? n.batchInputShape = [r, null] : n.batchInputShape = [r].concat(toList(t.inputLength));\n    }return n.inputDim = t.inputDim, n.outputDim = t.outputDim, n.embeddingsInitializer = getInitializer(t.embeddingsInitializer || n.DEFAULT_EMBEDDINGS_INITIALIZER), n.embeddingsRegularizer = getRegularizer(t.embeddingsRegularizer), n.activityRegularizer = getRegularizer(t.activityRegularizer), n.embeddingsConstraint = getConstraint(t.embeddingsConstraint), n.maskZero = t.maskZero, n.inputLength = t.inputLength, n;\n  }return __extends(t, e), t.prototype.build = function (e) {\n    this.embeddings = this.addWeight(\"embeddings\", [this.inputDim, this.outputDim], this.dtype, this.embeddingsInitializer, this.embeddingsRegularizer, !0, this.embeddingsConstraint), this.built = !0;\n  }, t.prototype.warnOnIncompatibleInputShape = function (e) {}, t.prototype.computeMask = function (e, t) {\n    throw new NotImplementedError(\"computeMask has not been implemented for Embedding yet\");\n  }, t.prototype.computeOutputShape = function (e) {\n    if (e = getExactlyOneShape(e), null == this.inputLength) return e.concat([this.outputDim]);var t = toList(this.inputLength);if (t.length !== e.length - 1) throw new ValueError('\"inputLength\" is ' + this.inputLength + \", but received input shape has shape \" + e);for (var n = 0, r = 0; r < t.length; ++r) {\n      var i = t[r],\n          a = e[r + 1];if (null != i && null != a && i !== a) throw new ValueError('\"inputLength\" is ' + this.inputLength + \", but received input shape has shape \" + e);null == i && (t[n] = a), n++;\n    }return [e[0]].concat(t, [this.outputDim]);\n  }, t.prototype.call = function (e, t) {\n    var n = this;return (0, _tfjsCore.tidy)(function () {\n      n.invokeCallHook(e, t);var r = getExactlyOneTensor(e);return \"int32\" !== r.dtype && (r = cast$1(r, \"int32\")), gather$1(n.embeddings.read(), r.as1D()).reshape(getExactlyOneShape(n.computeOutputShape(r.shape)));\n    });\n  }, t.prototype.getConfig = function () {\n    var t = { inputDim: this.inputDim, outputDim: this.outputDim, embeddingsInitializer: serializeInitializer(this.embeddingsInitializer), embeddingsRegularizer: serializeRegularizer(this.embeddingsRegularizer), activityRegularizer: serializeRegularizer(this.activityRegularizer), embeddingsConstraint: serializeConstraint(this.embeddingsConstraint), maskZero: this.maskZero, inputLength: this.inputLength },\n        n = e.prototype.getConfig.call(this);return Object.assign(t, n), t;\n  }, t.className = \"Embedding\", t;\n}(Layer);_tfjsCore.serialization.registerClass(Embedding);var Merge = function (e) {\n  function t(t) {\n    var n = e.call(this, t || {}) || this;return n.supportsMasking = !0, n;\n  }return __extends(t, e), t.prototype.mergeFunction = function (e) {\n    throw new NotImplementedError();\n  }, t.prototype.computeElementwiseOpOutputShape = function (e, t) {\n    if (null == e || null == t) return null;if (e.length < t.length) return this.computeElementwiseOpOutputShape(t, e);if (0 === t.length) return e;for (var n = e.slice(0, e.length - t.length), r = 0; r < t.length; ++r) {\n      var i = e[e.length - t.length + r],\n          a = t[r];if (null == i || null == a || i < 0 || a < 0) n.push(null);else if (1 === i) n.push(a);else if (1 === a) n.push(i);else {\n        if (i !== a) throw new ValueError(\"Operands could not be broadcast together with shapes \" + JSON.stringify(e) + \" \" + JSON.stringify(t));n.push(i);\n      }\n    }return n;\n  }, t.prototype.build = function (e) {\n    if (Array.isArray(e) && !Array.isArray(e[0]) && (e = [getExactlyOneShape(e)]), (e = e).length < 2) throw new ValueError(\"A merge layer should be called on an Array of at least 2 inputs. Got \" + e.length + \" input(s).\");for (var t = [], n = 0, r = e; n < r.length; n++) {\n      null != (o = r[n]) && null !== o[0] && t.push(o[0]);\n    }if ((t = unique(t)).length > 1) throw new ValueError(\"Can not merge tensors with different batch sizes. Got tensors with shapes: \" + JSON.stringify(e) + \".\");for (var i = null == e[0] ? null : e[0].slice(1), a = 1; a < e.length; ++a) {\n      var o = null == e[a] ? null : e[a].slice(1);i = this.computeElementwiseOpOutputShape(i, o);\n    }var s = e.map(function (e) {\n      return e.length;\n    });-1 === e.indexOf(null) && 1 === unique(s).length ? this.reshapeRequired = !1 : this.reshapeRequired = !0;\n  }, t.prototype.call = function (e, t) {\n    var n = this;return (0, _tfjsCore.tidy)(function () {\n      if (e = e, n.reshapeRequired) {\n        var t = [],\n            r = e.map(function (e) {\n          return e.rank;\n        });if (-1 === r.indexOf(null)) {\n          for (var i = max$1(r), a = 0, o = e; a < o.length; a++) {\n            for (var s = (h = o[a]).rank, l = 0; l < i - s; ++l) h = expandDims(h, 1);t.push(h);\n          }return n.mergeFunction(t);\n        }for (var u = !1, c = 0, p = e; c < p.length; c++) {\n          var h;if (null == (s = (h = p[c]).rank)) {\n            var d = h.shape,\n                g = d[0],\n                f = d.slice(1).concat([g]),\n                m = h.reshape([g].concat(arrayProd(d.slice(1))));m = (m = (0, _tfjsCore.transpose)(m, [1, 0])).reshape(f), t.push(m), u = !0;\n          } else if (s > 1) {\n            var y = range(1, s).concat([0]);t.push((0, _tfjsCore.transpose)(h, y)), u = !0;\n          } else t.push(h);\n        }var v = n.mergeFunction(t),\n            b = v.rank;if (u) if (null == b) {\n          var w = v.shape;f = [g = w[w.length - 1]].concat(w.slice(0, w.length - 1));v = (0, _tfjsCore.transpose)(v.reshape([-1, g]), [1, 0]).reshape(f);\n        } else if (b > 1) {\n          y = [b - 1].concat(range(0, b - 1));v = (0, _tfjsCore.transpose)(v, y);\n        }return v;\n      }return n.mergeFunction(e);\n    });\n  }, t.prototype.computeOutputShape = function (e) {\n    var t;t = null == (e = e)[0] ? null : e[0].slice(1);for (var n = 1; n < e.length; ++n) {\n      var r = null == e[n] ? null : e[n].slice(1);t = this.computeElementwiseOpOutputShape(t, r);\n    }for (var i = [], a = 0, o = e; a < o.length; a++) {\n      null != (r = o[a]) && null !== r[0] && i.push(r[0]);\n    }return t = 1 === (i = unique(i)).length ? i.concat(t) : [null].concat(t);\n  }, t;\n}(Layer),\n    Add = function (e) {\n  function t(t) {\n    return e.call(this, t) || this;\n  }return __extends(t, e), t.prototype.mergeFunction = function (e) {\n    return (0, _tfjsCore.tidy)(function () {\n      for (var t = e[0].clone(), n = 1; n < e.length; ++n) t = (0, _tfjsCore.add)(t, e[n]);return t;\n    });\n  }, t.className = \"Add\", t;\n}(Merge);_tfjsCore.serialization.registerClass(Add);var Multiply = function (e) {\n  function t(t) {\n    return e.call(this, t) || this;\n  }return __extends(t, e), t.prototype.mergeFunction = function (e) {\n    return (0, _tfjsCore.tidy)(function () {\n      for (var t = e[0].clone(), n = 1; n < e.length; ++n) t = (0, _tfjsCore.mul)(t, e[n]);return t;\n    });\n  }, t.className = \"Multiply\", t;\n}(Merge);_tfjsCore.serialization.registerClass(Multiply);var Average = function (e) {\n  function t(t) {\n    return e.call(this, t) || this;\n  }return __extends(t, e), t.prototype.mergeFunction = function (e) {\n    return (0, _tfjsCore.tidy)(function () {\n      for (var t = e[0].clone(), n = 1; n < e.length; ++n) t = (0, _tfjsCore.add)(t, e[n]);return (0, _tfjsCore.mul)(getScalar(1 / e.length), t);\n    });\n  }, t.className = \"Average\", t;\n}(Merge);_tfjsCore.serialization.registerClass(Average);var Maximum = function (e) {\n  function t(t) {\n    return e.call(this, t) || this;\n  }return __extends(t, e), t.prototype.mergeFunction = function (e) {\n    return (0, _tfjsCore.tidy)(function () {\n      for (var t = e[0], n = 1; n < e.length; ++n) t = (0, _tfjsCore.maximum)(t, e[n]);return t;\n    });\n  }, t.className = \"Maximum\", t;\n}(Merge);_tfjsCore.serialization.registerClass(Maximum);var Minimum = function (e) {\n  function t(t) {\n    return e.call(this, t) || this;\n  }return __extends(t, e), t.prototype.mergeFunction = function (e) {\n    return (0, _tfjsCore.tidy)(function () {\n      for (var t = e[0], n = 1; n < e.length; ++n) t = (0, _tfjsCore.minimum)(t, e[n]);return t;\n    });\n  }, t.className = \"Minimum\", t;\n}(Merge);_tfjsCore.serialization.registerClass(Minimum);var Concatenate = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this;return n.DEFAULT_AXIS = -1, null == t && (t = {}), n.axis = null == t.axis ? n.DEFAULT_AXIS : t.axis, n.supportsMasking = !0, n.reshapeRequired = !1, n;\n  }return __extends(t, e), t.prototype.build = function (e) {\n    if (!Array.isArray(e) || !Array.isArray(e[0]) || 1 === e.length) throw new ValueError(\"A `Concatenate` layer should be called on a list of at least 2 inputs\");for (var t = !0, n = 0, r = e = e; n < r.length; n++) {\n      if (null != (c = r[n])) {\n        t = !1;break;\n      }\n    }if (!t) {\n      for (var i = [], a = 0; a < e.length; ++a) {\n        var o = e[a].slice();o.splice(this.axis, 1);for (var s = !1, l = 0, u = i; l < u.length; l++) {\n          var c = u[l];if (_tfjsCore.util.arraysEqual(c, o)) {\n            s = !0;break;\n          }\n        }s || i.push(o);\n      }if (i.length > 1) throw new ValueError(\"A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got input shapes: \" + JSON.stringify(e));\n    }\n  }, t.prototype.mergeFunction = function (e) {\n    var t = this;return (0, _tfjsCore.tidy)(function () {\n      return concatenate(e, t.axis);\n    });\n  }, t.prototype.computeOutputShape = function (e) {\n    if (!Array.isArray(e) || !Array.isArray(e[0])) throw new ValueError(\"A `Concatenate` layer should be called on a list of inputs.\");for (var t = e, n = t[0].slice(), r = this.axis < 0 ? n.length + this.axis : this.axis, i = 0, a = t.slice(1); i < a.length; i++) {\n      var o = a[i];if (null == n[r] || null == o[r]) {\n        n[r] = null;break;\n      }n[r] += o[r];\n    }return n;\n  }, t.prototype.getConfig = function () {\n    var t = { axis: this.axis },\n        n = e.prototype.getConfig.call(this);return Object.assign(t, n), t;\n  }, t.className = \"Concatenate\", t;\n}(Merge);function batchNormalization(e, t, n, r, i, a) {\n  var o;if (void 0 === a && (a = .001), 2 === e.rank) o = (0, _tfjsCore.batchNormalization2d)(e, t, n, a, i, r);else if (3 === e.rank) o = (0, _tfjsCore.batchNormalization3d)(e, t, n, a, i, r);else {\n    if (4 !== e.rank) throw new NotImplementedError(\"batchNormalization is not implememnted for array of rank \" + e.rank + \" yet\");o = (0, _tfjsCore.batchNormalization4d)(e, t, n, a, i, r);\n  }return o;\n}function regularNormalizeBatchInTraining(e, t, n, r, i) {\n  return void 0 === i && (i = .001), (0, _tfjsCore.tidy)(function () {\n    var a = (0, _tfjsCore.moments)(e, r),\n        o = a.mean,\n        s = a.variance;return [batchNormalization(e, o, s, n, t, i), o, s];\n  });\n}function broadcastNormalizeBatchInTraining(e, t, n, r, i) {\n  return void 0 === i && (i = .001), (0, _tfjsCore.tidy)(function () {\n    for (var a = (0, _tfjsCore.moments)(e, r), o = a.mean, s = a.variance, l = [], u = 0, c = range(0, e.rank); u < c.length; u++) {\n      var p = c[u];-1 !== r.indexOf(p) ? l.push(1) : l.push(e.shape[p]);\n    }var h = o.reshape(l),\n        d = s.reshape(l),\n        g = null == t ? null : t.reshape(l),\n        f = null == n ? null : n.reshape(l);return [batchNormalization(e, h, d, f, g, i), o, s];\n  });\n}function normalizeBatchInTraining(e, t, n, r, i) {\n  return void 0 === i && (i = .001), _tfjsCore.util.arraysEqual(r.slice().sort(), range(0, e.rank - 1)) ? regularNormalizeBatchInTraining(e, t, n, r, i) : broadcastNormalizeBatchInTraining(e, t, n, r, i);\n}_tfjsCore.serialization.registerClass(Concatenate);var BatchNormalization = function (e) {\n  function t(t) {\n    var n = this;return null == t && (t = {}), (n = e.call(this, t) || this).supportsMasking = !0, n.axis = null == t.axis ? -1 : t.axis, n.momentum = null == t.momentum ? .99 : t.momentum, n.epsilon = null == t.epsilon ? .001 : t.epsilon, n.center = null == t.center || t.center, n.scale = null == t.scale || t.scale, n.betaInitializer = getInitializer(t.betaInitializer || \"zeros\"), n.gammaInitializer = getInitializer(t.gammaInitializer || \"ones\"), n.movingMeanInitializer = getInitializer(t.movingMeanInitializer || \"zeros\"), n.movingVarianceInitializer = getInitializer(t.movingVarianceInitializer || \"ones\"), n.betaConstraint = getConstraint(t.betaConstraint), n.gammaConstraint = getConstraint(t.gammaConstraint), n.betaRegularizer = getRegularizer(t.betaRegularizer), n.gammaRegularizer = getRegularizer(t.gammaRegularizer), n.stepCount = 0, n;\n  }return __extends(t, e), t.prototype.build = function (e) {\n    e = getExactlyOneShape(e);var t = this.axis >= 0 ? this.axis : this.axis + e.length,\n        n = e[t];if (null == n) throw new ValueError(\"Axis \" + t + \" of input tensor should have a defined dimension but the layer received an input with shape \" + JSON.stringify(e) + \".\");this.inputSpec = [new InputSpec({ ndim: e.length, axes: (r = {}, r[t] = n, r) })];var r,\n        i = [n];this.scale && (this.gamma = this.addWeight(\"gamma\", i, null, this.gammaInitializer, this.gammaRegularizer, !0, this.gammaConstraint)), this.center && (this.beta = this.addWeight(\"beta\", i, null, this.betaInitializer, this.betaRegularizer, !0, this.betaConstraint)), this.movingMean = this.addWeight(\"moving_mean\", i, null, this.movingMeanInitializer, null, !1), this.movingVariance = this.addWeight(\"moving_variance\", i, null, this.movingVarianceInitializer, null, !1), this.built = !0;\n  }, t.prototype.call = function (e, t) {\n    var n = this;return (0, _tfjsCore.tidy)(function () {\n      var r = null != t.training && t.training,\n          i = getExactlyOneTensor(e),\n          a = i.shape,\n          o = a.length,\n          s = range(0, o),\n          l = n.axis >= 0 ? n.axis : n.axis + o;s.splice(l, 1);var u = pyListRepeat(1, o);u[l] = a[l];var c = s.slice();c.sort();var p = !_tfjsCore.util.arraysEqual(c, range(0, o).slice(0, o - 1));if (!r) return function () {\n        if (p) {\n          var e = n.movingMean.read().reshape(u),\n              t = n.movingVariance.read().reshape(u),\n              r = n.center ? n.beta.read().reshape(u) : null,\n              a = n.scale ? n.gamma.read().reshape(u) : null;return batchNormalization(i, e, t, r, a, n.epsilon);\n        }return batchNormalization(i, n.movingMean.read(), n.movingVariance.read(), null == n.beta ? null : n.beta.read(), null == n.gamma ? null : n.gamma.read(), n.epsilon);\n      }();var h = normalizeBatchInTraining(i, n.gamma.read(), n.beta.read(), s, n.epsilon),\n          d = h[0],\n          g = h[1],\n          f = h[2],\n          m = arrayProd(s.map(function (e) {\n        return i.shape[e];\n      })),\n          y = f.mul(getScalar(m / (m - (1 + n.epsilon))));return function () {\n        n.stepCount++;var e = (0, _tfjsCore.movingAverage)(n.movingMean.read(), g, n.momentum, n.stepCount);n.movingMean.write(e);var t = (0, _tfjsCore.movingAverage)(n.movingVariance.read(), y, n.momentum, n.stepCount);n.movingVariance.write(t);\n      }(), d;\n    });\n  }, t.prototype.getConfig = function () {\n    var t = { axis: this.axis, momentum: this.momentum, epsilon: this.epsilon, center: this.center, scale: this.scale, betaInitializer: serializeInitializer(this.betaInitializer), gammaInitializer: serializeInitializer(this.gammaInitializer), movingMeanInitializer: serializeInitializer(this.movingMeanInitializer), movingVarianceInitializer: serializeInitializer(this.movingVarianceInitializer), betaRegularizer: serializeRegularizer(this.betaRegularizer), gammaRegularizer: serializeRegularizer(this.gammaRegularizer), betaConstraint: serializeConstraint(this.betaConstraint), gammaConstraint: serializeConstraint(this.gammaConstraint) },\n        n = e.prototype.getConfig.call(this);return Object.assign(t, n), t;\n  }, t.className = \"BatchNormalization\", t;\n}(Layer);function spatial2dPadding(e, t, n) {\n  return (0, _tfjsCore.tidy)(function () {\n    if (4 !== e.rank) throw new ValueError(\"temporalPadding expects input tensor to be 4-D, but received a \" + e.rank + \"-D tensor.\");if (null == t && (t = [[1, 1], [1, 1]]), 2 !== t.length || 2 !== t[0].length || 2 !== t[1].length) throw new ValueError(\"spatial2dPadding expects `padding` to be an Array of two Arrays, each of which is an Array of two integers.\");if (null == n && (n = imageDataFormat()), \"channelsLast\" !== n && \"channelsFirst\" !== n) throw new ValueError(\"Unknown data format: \" + n + \". Supported data formats are 'channelsLast' and 'channelsFirst.\");var r;return r = \"channelsFirst\" === n ? [[0, 0], [0, 0], t[0], t[1]] : [[0, 0], t[0], t[1], [0, 0]], (0, _tfjsCore.pad)(e, r);\n  });\n}_tfjsCore.serialization.registerClass(BatchNormalization);var ZeroPadding2D = function (e) {\n  function t(t) {\n    var n = this;if (null == t && (t = {}), (n = e.call(this, t) || this).dataFormat = null == t.dataFormat ? imageDataFormat() : t.dataFormat, null == t.padding) n.padding = [[1, 1], [1, 1]];else if (\"number\" == typeof t.padding) n.padding = [[t.padding, t.padding], [t.padding, t.padding]];else {\n      if (t.padding = t.padding, 2 !== t.padding.length) throw new ValueError(\"ZeroPadding2D expects padding to be a length-2 array, but received a length-\" + t.padding.length + \" array.\");var r = void 0,\n          i = void 0;if (\"number\" == typeof t.padding[0]) r = [t.padding[0], t.padding[0]], i = [t.padding[1], t.padding[1]];else {\n        if (t.padding = t.padding, 2 !== t.padding[0].length) throw new ValueError(\"ZeroPadding2D expects height padding to be a length-2 array, but received a length-\" + t.padding[0].length + \" array.\");if (r = t.padding[0], 2 !== t.padding[1].length) throw new ValueError(\"ZeroPadding2D expects width padding to be a length-2 array, but received a length-\" + t.padding[1].length + \" array.\");i = t.padding[1];\n      }n.padding = [r, i];\n    }return n.inputSpec = [new InputSpec({ ndim: 4 })], n;\n  }return __extends(t, e), t.prototype.computeOutputShape = function (e) {\n    var t, n;return e = getExactlyOneShape(e), \"channelsFirst\" === this.dataFormat ? (t = null != e[2] && e[2] >= 0 ? e[2] + this.padding[0][0] + this.padding[0][1] : null, n = null != e[3] && e[3] >= 0 ? e[3] + this.padding[1][0] + this.padding[1][1] : null, [e[0], e[1], t, n]) : (t = null != e[1] && e[1] >= 0 ? e[1] + this.padding[0][0] + this.padding[0][1] : null, n = null != e[2] && e[2] >= 0 ? e[2] + this.padding[1][0] + this.padding[1][1] : null, [e[0], t, n, e[3]]);\n  }, t.prototype.call = function (e, t) {\n    var n = this;return (0, _tfjsCore.tidy)(function () {\n      return spatial2dPadding(getExactlyOneTensor(e), n.padding, n.dataFormat);\n    });\n  }, t.prototype.getConfig = function () {\n    var t = { padding: this.padding, dataFormat: this.dataFormat },\n        n = e.prototype.getConfig.call(this);return Object.assign(t, n), t;\n  }, t.className = \"ZeroPadding2D\", t;\n}(Layer);function pool2d(e, t, n, r, i, a) {\n  return (0, _tfjsCore.tidy)(function () {\n    var o;checkDataFormat(i), checkPoolMode(a), checkPaddingMode(r), null == n && (n = [1, 1]), null == r && (r = \"valid\"), null == i && (i = imageDataFormat()), null == a && (a = \"max\"), e = preprocessConv2DInput(e, i);var s = \"same\" === r ? \"same\" : \"valid\";return o = \"max\" === a ? (0, _tfjsCore.maxPool)(e, t, n, s) : (0, _tfjsCore.avgPool)(e, t, n, s), \"channelsFirst\" === i && (o = (0, _tfjsCore.transpose)(o, [0, 3, 1, 2])), o;\n  });\n}_tfjsCore.serialization.registerClass(ZeroPadding2D);var Pooling1D = function (e) {\n  function t(t) {\n    var n = this;if (null == t.poolSize && (t.poolSize = 2), n = e.call(this, t) || this, \"number\" == typeof t.poolSize) n.poolSize = [t.poolSize];else {\n      if (!Array.isArray(t.poolSize) || 1 !== t.poolSize.length || \"number\" != typeof t.poolSize[0]) throw new ValueError(\"poolSize for 1D convolutional layer must be a number or an Array of a single number, but received \" + JSON.stringify(t.poolSize));n.poolSize = t.poolSize;\n    }if (null == t.strides) n.strides = n.poolSize;else if (\"number\" == typeof t.strides) n.strides = [t.strides];else {\n      if (!Array.isArray(t.strides) || 1 !== t.strides.length || \"number\" != typeof t.strides[0]) throw new ValueError(\"strides for 1D convolutional layer must be a number or an Array of a single number, but received \" + JSON.stringify(t.strides));n.strides = t.strides;\n    }return n.padding = null == t.padding ? \"valid\" : t.padding, checkPaddingMode(n.padding), n.inputSpec = [new InputSpec({ ndim: 3 })], n;\n  }return __extends(t, e), t.prototype.computeOutputShape = function (e) {\n    var t = convOutputLength((e = getExactlyOneShape(e))[1], this.poolSize[0], this.padding, this.strides[0]);return [e[0], t, e[2]];\n  }, t.prototype.call = function (e, t) {\n    var n = this;return (0, _tfjsCore.tidy)(function () {\n      n.invokeCallHook(e, t), e = expandDims(getExactlyOneTensor(e), 2);var r = n.poolingFunction(getExactlyOneTensor(e), [n.poolSize[0], 1], [n.strides[0], 1], n.padding, \"channelsLast\");return (0, _tfjsCore.squeeze)(r, [2]);\n    });\n  }, t.prototype.getConfig = function () {\n    var t = { poolSize: this.poolSize, padding: this.padding, strides: this.strides },\n        n = e.prototype.getConfig.call(this);return Object.assign(t, n), t;\n  }, t;\n}(Layer),\n    MaxPooling1D = function (e) {\n  function t(t) {\n    return e.call(this, t) || this;\n  }return __extends(t, e), t.prototype.poolingFunction = function (e, t, n, r, i) {\n    return checkDataFormat(i), checkPaddingMode(r), pool2d(e, t, n, r, i, \"max\");\n  }, t.className = \"MaxPooling1D\", t;\n}(Pooling1D);_tfjsCore.serialization.registerClass(MaxPooling1D);var AveragePooling1D = function (e) {\n  function t(t) {\n    return e.call(this, t) || this;\n  }return __extends(t, e), t.prototype.poolingFunction = function (e, t, n, r, i) {\n    return checkDataFormat(i), checkPaddingMode(r), pool2d(e, t, n, r, i, \"avg\");\n  }, t.className = \"AveragePooling1D\", t;\n}(Pooling1D);_tfjsCore.serialization.registerClass(AveragePooling1D);var Pooling2D = function (e) {\n  function t(t) {\n    var n = this;if (null == t.poolSize && (t.poolSize = [2, 2]), (n = e.call(this, t) || this).poolSize = Array.isArray(t.poolSize) ? t.poolSize : [t.poolSize, t.poolSize], null == t.strides) n.strides = n.poolSize;else if (Array.isArray(t.strides)) {\n      if (2 !== t.strides.length) throw new ValueError(\"If the strides property of a 2D pooling layer is an Array, it is expected to have a length of 2, but received length \" + t.strides.length + \".\");n.strides = t.strides;\n    } else n.strides = [t.strides, t.strides];return n.padding = null == t.padding ? \"valid\" : t.padding, n.dataFormat = null == t.dataFormat ? \"channelsLast\" : t.dataFormat, checkDataFormat(n.dataFormat), checkPaddingMode(n.padding), n.inputSpec = [new InputSpec({ ndim: 4 })], n;\n  }return __extends(t, e), t.prototype.computeOutputShape = function (e) {\n    e = getExactlyOneShape(e);var t = \"channelsFirst\" === this.dataFormat ? e[2] : e[1],\n        n = \"channelsFirst\" === this.dataFormat ? e[3] : e[2];return t = convOutputLength(t, this.poolSize[0], this.padding, this.strides[0]), n = convOutputLength(n, this.poolSize[1], this.padding, this.strides[1]), \"channelsFirst\" === this.dataFormat ? [e[0], e[1], t, n] : [e[0], t, n, e[3]];\n  }, t.prototype.call = function (e, t) {\n    var n = this;return (0, _tfjsCore.tidy)(function () {\n      return n.invokeCallHook(e, t), n.poolingFunction(getExactlyOneTensor(e), n.poolSize, n.strides, n.padding, n.dataFormat);\n    });\n  }, t.prototype.getConfig = function () {\n    var t = { poolSize: this.poolSize, padding: this.padding, strides: this.strides, dataFormat: this.dataFormat },\n        n = e.prototype.getConfig.call(this);return Object.assign(t, n), t;\n  }, t;\n}(Layer),\n    MaxPooling2D = function (e) {\n  function t(t) {\n    return e.call(this, t) || this;\n  }return __extends(t, e), t.prototype.poolingFunction = function (e, t, n, r, i) {\n    return checkDataFormat(i), checkPaddingMode(r), pool2d(e, t, n, r, i, \"max\");\n  }, t.className = \"MaxPooling2D\", t;\n}(Pooling2D);_tfjsCore.serialization.registerClass(MaxPooling2D);var AveragePooling2D = function (e) {\n  function t(t) {\n    return e.call(this, t) || this;\n  }return __extends(t, e), t.prototype.poolingFunction = function (e, t, n, r, i) {\n    return checkDataFormat(i), checkPaddingMode(r), pool2d(e, t, n, r, i, \"avg\");\n  }, t.className = \"AveragePooling2D\", t;\n}(Pooling2D);_tfjsCore.serialization.registerClass(AveragePooling2D);var GlobalPooling1D = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this;return n.inputSpec = [new InputSpec({ ndim: 3 })], n;\n  }return __extends(t, e), t.prototype.computeOutputShape = function (e) {\n    return [e[0], e[2]];\n  }, t.prototype.call = function (e, t) {\n    throw new NotImplementedError();\n  }, t;\n}(Layer),\n    GlobalAveragePooling1D = function (e) {\n  function t(t) {\n    return e.call(this, t) || this;\n  }return __extends(t, e), t.prototype.call = function (e, t) {\n    return (0, _tfjsCore.tidy)(function () {\n      var t = getExactlyOneTensor(e);return (0, _tfjsCore.mean)(t, 1);\n    });\n  }, t.className = \"GlobalAveragePooling1D\", t;\n}(GlobalPooling1D);_tfjsCore.serialization.registerClass(GlobalAveragePooling1D);var GlobalMaxPooling1D = function (e) {\n  function t(t) {\n    return e.call(this, t) || this;\n  }return __extends(t, e), t.prototype.call = function (e, t) {\n    return (0, _tfjsCore.tidy)(function () {\n      var t = getExactlyOneTensor(e);return (0, _tfjsCore.max)(t, 1);\n    });\n  }, t.className = \"GlobalMaxPooling1D\", t;\n}(GlobalPooling1D);_tfjsCore.serialization.registerClass(GlobalMaxPooling1D);var GlobalPooling2D = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this;return n.dataFormat = null == t.dataFormat ? \"channelsLast\" : t.dataFormat, checkDataFormat(n.dataFormat), n.inputSpec = [new InputSpec({ ndim: 4 })], n;\n  }return __extends(t, e), t.prototype.computeOutputShape = function (e) {\n    return e = e, \"channelsLast\" === this.dataFormat ? [e[0], e[3]] : [e[0], e[1]];\n  }, t.prototype.call = function (e, t) {\n    throw new NotImplementedError();\n  }, t.prototype.getConfig = function () {\n    var t = { dataFormat: this.dataFormat },\n        n = e.prototype.getConfig.call(this);return Object.assign(t, n), t;\n  }, t;\n}(Layer),\n    GlobalAveragePooling2D = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }return __extends(t, e), t.prototype.call = function (e, t) {\n    var n = this;return (0, _tfjsCore.tidy)(function () {\n      var t = getExactlyOneTensor(e);return \"channelsLast\" === n.dataFormat ? (0, _tfjsCore.mean)(t, [1, 2]) : (0, _tfjsCore.mean)(t, [2, 3]);\n    });\n  }, t.className = \"GlobalAveragePooling2D\", t;\n}(GlobalPooling2D);_tfjsCore.serialization.registerClass(GlobalAveragePooling2D);var GlobalMaxPooling2D = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }return __extends(t, e), t.prototype.call = function (e, t) {\n    var n = this;return (0, _tfjsCore.tidy)(function () {\n      var t = getExactlyOneTensor(e);return \"channelsLast\" === n.dataFormat ? (0, _tfjsCore.max)(t, [1, 2]) : (0, _tfjsCore.max)(t, [2, 3]);\n    });\n  }, t.className = \"GlobalMaxPooling2D\", t;\n}(GlobalPooling2D);function standardizeArgs(e, t, n, r) {\n  if (Array.isArray(e)) {\n    if (null != t || null != n) throw new ValueError(\"When inputs is an array, neither initialState or constants should be provided\");null != r && (n = e.slice(e.length - r, e.length), e = e.slice(0, e.length - r)), e.length > 1 && (t = e.slice(1, e.length)), e = e[0];\n  }function i(e) {\n    return null == e || Array.isArray(e) ? e : [e];\n  }return { inputs: e, initialState: t = i(t), constants: n = i(n) };\n}function rnn(e, t, n, r, i, a, o, s) {\n  void 0 === r && (r = !1), void 0 === o && (o = !1), void 0 === s && (s = !1);var l = t.shape.length;if (l < 3) throw new ValueError(\"Input should be at least 3D, but is \" + l + \"D.\");var u,\n      c,\n      p = [1, 0].concat(range(2, l));if (t = (0, _tfjsCore.transpose)(t, p), null != i) throw new NotImplementedError(\"The rnn() function of the deeplearn.js backend does not support masking yet.\");if (null != a) throw new NotImplementedError(\"The rnn() functoin of the deeplearn.js backend does not support constants yet.\");o && console.warn(\"Backend rnn(): the unroll = true option is not applicable to the imperative deeplearn.js backend.\"), r && (t = (0, _tfjsCore.reverse)(t, 0));for (var h = n, d = t.shape[0], g = function (n) {\n    var r = sliceAlongFirstAxis(t, n, 1);r = r.reshape(r.shape.slice(1));var i = (0, _tfjsCore.tidy)(function () {\n      return e(r, h);\n    });if (c = i[0], s) if (0 === n) u = c.expandDims(1);else {\n      var a = (0, _tfjsCore.concat)([u, c.expandDims(1)], 1);u.dispose(), u = a;\n    }h = i[1];\n  }, f = 0; f < d; ++f) g(f);return [c, u, h];\n}_tfjsCore.serialization.registerClass(GlobalMaxPooling2D);var RNN = function (e) {\n  function t(t) {\n    var n,\n        r = e.call(this, t) || this;if (null == t.cell) throw new ValueError(\"cell property is missing for the constructor of RNN.\");if (null == (n = Array.isArray(t.cell) ? new StackedRNNCells({ cells: t.cell }) : t.cell).stateSize) throw new ValueError(\"The RNN cell should have an attribute `stateSize` (tuple of integers, one integer per RNN state).\");return r.cell = n, r.returnSequences = null != t.returnSequences && t.returnSequences, r.returnState = null != t.returnState && t.returnState, r.goBackwards = null != t.goBackwards && t.goBackwards, r._stateful = null != t.stateful && t.stateful, r.unroll = null != t.unroll && t.unroll, r.supportsMasking = !0, r.inputSpec = [new InputSpec({ ndim: 3 })], r.stateSpec = null, r.states = null, r.numConstants = null, r.keptStates = [], r;\n  }return __extends(t, e), t.prototype.getStates = function () {\n    return null == this.states ? range(0, Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1).map(function (e) {\n      return null;\n    }) : this.states;\n  }, t.prototype.setStates = function (e) {\n    this.states = e;\n  }, t.prototype.computeOutputShape = function (e) {\n    isArrayOfShapes(e) && (e = e[0]), e = e;var t = this.cell.stateSize;Array.isArray(t) || (t = [t]);var n,\n        r = t[0];if (n = this.returnSequences ? [e[0], e[1], r] : [e[0], r], this.returnState) {\n      for (var i = [], a = 0, o = t; a < o.length; a++) {\n        var s = o[a];i.push([e[0], s]);\n      }return [n].concat(i);\n    }return n;\n  }, t.prototype.computeMask = function (e, t) {\n    throw new NotImplementedError(\"computeMask has not been implemented for RNN yet\");\n  }, t.prototype.build = function (e) {\n    if (null != this.numConstants) throw new NotImplementedError(\"Constants support is not implemented in RNN yet.\");isArrayOfShapes(e) && (e = e[0]), e = e;var t = this.stateful ? e[0] : null,\n        n = e[e.length - 1];this.inputSpec[0] = new InputSpec({ shape: [t, null, n] });var r,\n        i = [e[0]].concat(e.slice(2));if (this.cell.build(i), r = Array.isArray(this.cell.stateSize) ? this.cell.stateSize : [this.cell.stateSize], null != this.stateSpec) {\n      if (!_tfjsCore.util.arraysEqual(this.stateSpec.map(function (e) {\n        return e.shape[e.shape.length - 1];\n      }), r)) throw new ValueError(\"An initialState was passed that is not compatible with cell.stateSize. Received stateSpec=\" + this.stateSpec + \"; However cell.stateSize is \" + this.cell.stateSize);\n    } else this.stateSpec = r.map(function (e) {\n      return new InputSpec({ shape: [null, e] });\n    });this.stateful && this.resetStates();\n  }, t.prototype.resetStates = function (e) {\n    var t = this;(0, _tfjsCore.tidy)(function () {\n      if (!t.stateful) throw new AttributeError(\"Cannot call resetStates() on an RNN Layer that is not stateful.\");var n = t.inputSpec[0].shape[0];if (null == n) throw new ValueError(\"If an RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \\n- If using a Sequential model, specify the batch size by passing a `batchInputShape` option to your first layer.\\n- If using the functional API, specify the batch size by passing a `batchShape` option to your Input layer.\");if (null == t.states) Array.isArray(t.cell.stateSize) ? t.states = t.cell.stateSize.map(function (e) {\n        return (0, _tfjsCore.zeros)([n, e]);\n      }) : t.states = [(0, _tfjsCore.zeros)([n, t.cell.stateSize])];else if (null == e) (0, _tfjsCore.dispose)(t.states), null != t.keptStates && ((0, _tfjsCore.dispose)(t.keptStates), t.keptStates = []), Array.isArray(t.cell.stateSize) ? t.states = t.cell.stateSize.map(function (e) {\n        return (0, _tfjsCore.zeros)([n, e]);\n      }) : t.states[0] = (0, _tfjsCore.zeros)([n, t.cell.stateSize]);else {\n        if (t.keptStates.push(t.states.slice()), Array.isArray(e) || (e = [e]), e.length !== t.states.length) throw new ValueError(\"Layer \" + t.name + \" expects \" + t.states.length + \" state(s), but it received \" + e.length + \" state value(s). Input received: \" + e);for (var r = 0; r < t.states.length; ++r) {\n          var i = e[r],\n              a = Array.isArray(t.cell.stateSize) ? t.cell.stateSize[r] : t.cell.stateSize,\n              o = [n, a];if (!_tfjsCore.util.arraysEqual(i.shape, o)) throw new ValueError(\"State \" + r + \" is incompatible with layer \" + t.name + \": expected shape=\" + o + \", received shape=\" + i.shape);t.states[r] = i;\n        }\n      }t.states.forEach(function (e) {\n        return (0, _tfjsCore.keep)(e);\n      });\n    });\n  }, t.prototype.apply = function (t, n) {\n    var r = null == n ? null : n.initialState,\n        i = null == n ? null : n.constants;null == n && (n = {});var a = standardizeArgs(t, r, i, this.numConstants);t = a.inputs, r = a.initialState, i = a.constants;var o = [],\n        s = [];if (null != r) {\n      n.initialState = r, o = o.concat(r), this.stateSpec = [];for (var l = 0, u = r; l < u.length; l++) {\n        var c = u[l];this.stateSpec.push(new InputSpec({ shape: c.shape }));\n      }s = s.concat(this.stateSpec);\n    }if (null != i && (n.constants = i, o = o.concat(i), this.numConstants = i.length), o[0] instanceof SymbolicTensor) {\n      var p = [t].concat(o),\n          h = this.inputSpec.concat(s),\n          d = this.inputSpec;this.inputSpec = h;var g = e.prototype.apply.call(this, p, n);return this.inputSpec = d, g;\n    }return e.prototype.apply.call(this, t, n);\n  }, t.prototype.call = function (e, t) {\n    var n = this;return (0, _tfjsCore.tidy)(function () {\n      var r = null == t ? null : t.mask,\n          i = null == t ? null : t.training,\n          a = null == t ? null : t.initialState;if (e = getExactlyOneTensor(e), null == a && (a = n.stateful ? n.states : n.getInitialState(e)), null != r) throw new NotImplementedError(\"Masking is not implemented for RNN yet\");var o = Array.isArray(n.cell.stateSize) ? n.cell.stateSize.length : 1;if (a.length !== o) throw new ValueError(\"RNN Layer has \" + o + \" state(s) but was passed \" + a.length + \" initial state(s).\");n.unroll && console.warn(\"Ignoring unroll = true for RNN layer, due to imperative backend.\");var s = { training: i },\n          l = rnn(function (e, t) {\n        var r = n.cell.call([e].concat(t), s);return [r[0], r.slice(1)];\n      }, e, a, n.goBackwards, null, null, n.unroll, n.returnSequences),\n          u = l[0],\n          c = l[1],\n          p = l[2];n.stateful && n.resetStates(p);var h = n.returnSequences ? c : u;return n.returnState ? [h].concat(p) : h;\n    });\n  }, t.prototype.getInitialState = function (e) {\n    var t = this;return (0, _tfjsCore.tidy)(function () {\n      var n = (0, _tfjsCore.zeros)(e.shape);return n = expandDims(n = (0, _tfjsCore.sum)(n, [1, 2])), Array.isArray(t.cell.stateSize) ? t.cell.stateSize.map(function (e) {\n        return e > 1 ? tile$1(n, [1, e]) : n;\n      }) : t.cell.stateSize > 1 ? [tile$1(n, [1, t.cell.stateSize])] : [n];\n    });\n  }, Object.defineProperty(t.prototype, \"trainableWeights\", { get: function () {\n      return this.trainable ? this.cell.trainableWeights : [];\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"nonTrainableWeights\", { get: function () {\n      return this.trainable ? this.cell.nonTrainableWeights : this.cell.weights;\n    }, enumerable: !0, configurable: !0 }), t.prototype.getConfig = function () {\n    var t = { returnSequences: this.returnSequences, returnState: this.returnState, goBackwards: this.goBackwards, stateful: this.stateful, unroll: this.unroll };null != this.numConstants && (t.numConstants = this.numConstants);var n = this.cell.getConfig();t.cell = { className: this.cell.getClassName(), config: n };var r = e.prototype.getConfig.call(this);return Object.assign(t, r), t;\n  }, t.className = \"RNN\", t;\n}(Layer);_tfjsCore.serialization.registerClass(RNN);var RNNCell = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }return __extends(t, e), t;\n}(Layer),\n    SimpleRNNCell = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this;return n.DEFAULT_ACTIVATION = \"tanh\", n.DEFAULT_KERNEL_INITIALIZER = \"glorotNormal\", n.DEFAULT_RECURRENT_INITIALIZER = \"orthogonal\", n.DEFAULT_BIAS_INITIALIZER = \"zeros\", n.units = t.units, n.activation = getActivation(null == t.activation ? n.DEFAULT_ACTIVATION : t.activation), n.useBias = null == t.useBias || t.useBias, n.kernelInitializer = getInitializer(t.kernelInitializer || n.DEFAULT_KERNEL_INITIALIZER), n.recurrentInitializer = getInitializer(t.recurrentInitializer || n.DEFAULT_RECURRENT_INITIALIZER), n.biasInitializer = getInitializer(t.biasInitializer || n.DEFAULT_BIAS_INITIALIZER), n.kernelRegularizer = getRegularizer(t.kernelRegularizer), n.recurrentRegularizer = getRegularizer(t.recurrentRegularizer), n.biasRegularizer = getRegularizer(t.biasRegularizer), n.kernelConstraint = getConstraint(t.kernelConstraint), n.recurrentConstraint = getConstraint(t.recurrentConstraint), n.biasConstraint = getConstraint(t.biasConstraint), n.dropout = min$1([1, max$1([0, null == t.dropout ? 0 : t.dropout])]), n.recurrentDropout = min$1([1, max$1([0, null == t.recurrentDropout ? 0 : t.recurrentDropout])]), n.stateSize = n.units, n.dropoutMask = null, n.recurrentDropoutMask = null, n;\n  }return __extends(t, e), t.prototype.build = function (e) {\n    e = getExactlyOneShape(e), this.kernel = this.addWeight(\"kernel\", [e[e.length - 1], this.units], null, this.kernelInitializer, this.kernelRegularizer, !0, this.kernelConstraint), this.recurrentKernel = this.addWeight(\"recurrent_kernel\", [this.units, this.units], null, this.recurrentInitializer, this.recurrentRegularizer, !0, this.recurrentConstraint), this.useBias ? this.bias = this.addWeight(\"bias\", [this.units], null, this.biasInitializer, this.biasRegularizer, !0, this.biasConstraint) : this.bias = null, this.built = !0;\n  }, t.prototype.call = function (e, t) {\n    var n = this;return (0, _tfjsCore.tidy)(function () {\n      if (2 !== (e = e).length) throw new ValueError(\"SimpleRNNCell expects 2 input Tensors, got \" + e.length + \".\");var r = e[1];e = e[0];var i,\n          a = null != t.training && t.training;0 < n.dropout && n.dropout < 1 && null == n.dropoutMask && (n.dropoutMask = generateDropoutMask(function () {\n        return (0, _tfjsCore.onesLike)(e);\n      }, n.dropout, a)), 0 < n.recurrentDropout && n.recurrentDropout < 1 && null == n.recurrentDropoutMask && (n.recurrentDropoutMask = generateDropoutMask(function () {\n        return (0, _tfjsCore.onesLike)(r);\n      }, n.recurrentDropout, a));var o = n.dropoutMask,\n          s = n.recurrentDropoutMask;i = dot(null != o ? (0, _tfjsCore.mul)(e, o) : e, n.kernel.read()), null != n.bias && (i = biasAdd(i, n.bias.read())), null != s && (r = (0, _tfjsCore.mul)(r, s));var l = (0, _tfjsCore.add)(i, dot(r, n.recurrentKernel.read()));return null != n.activation && (l = n.activation.apply(l)), [l, l];\n    });\n  }, t.prototype.getConfig = function () {\n    var t = { units: this.units, activation: serializeActivation(this.activation), useBias: this.useBias, kernelInitializer: serializeInitializer(this.kernelInitializer), recurrentInitializer: serializeInitializer(this.recurrentInitializer), biasInitializer: serializeInitializer(this.biasInitializer), kernelRegularizer: serializeRegularizer(this.kernelRegularizer), recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer), biasRegularizer: serializeRegularizer(this.biasRegularizer), activityRegularizer: serializeRegularizer(this.activityRegularizer), kernelConstraint: serializeConstraint(this.kernelConstraint), recurrentConstraint: serializeConstraint(this.recurrentConstraint), biasConstraint: serializeConstraint(this.biasConstraint), dropout: this.dropout, recurrentDropout: this.recurrentDropout },\n        n = e.prototype.getConfig.call(this);return Object.assign(t, n), t;\n  }, t.className = \"SimpleRNNCell\", t;\n}(RNNCell);_tfjsCore.serialization.registerClass(SimpleRNNCell);var SimpleRNN = function (e) {\n  function t(t) {\n    return t.cell = new SimpleRNNCell(t), e.call(this, t) || this;\n  }return __extends(t, e), t.prototype.call = function (t, n) {\n    var r = this;return (0, _tfjsCore.tidy)(function () {\n      null != r.cell.dropoutMask && ((0, _tfjsCore.dispose)(r.cell.dropoutMask), r.cell.dropoutMask = null), null != r.cell.recurrentDropoutMask && ((0, _tfjsCore.dispose)(r.cell.recurrentDropoutMask), r.cell.recurrentDropoutMask = null);var i = null == n ? null : n.mask,\n          a = null == n ? null : n.training,\n          o = null == n ? null : n.initialState;return e.prototype.call.call(r, t, { mask: i, training: a, initialState: o });\n    });\n  }, Object.defineProperty(t.prototype, \"units\", { get: function () {\n      return this.cell.units;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"activation\", { get: function () {\n      return this.cell.activation;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"useBias\", { get: function () {\n      return this.cell.useBias;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"kernelInitializer\", { get: function () {\n      return this.cell.kernelInitializer;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"recurrentInitializer\", { get: function () {\n      return this.cell.recurrentInitializer;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"biasInitializer\", { get: function () {\n      return this.cell.biasInitializer;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"kernelRegularizer\", { get: function () {\n      return this.cell.kernelRegularizer;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"recurrentRegularizer\", { get: function () {\n      return this.cell.recurrentRegularizer;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"biasRegularizer\", { get: function () {\n      return this.cell.biasRegularizer;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"kernelConstraint\", { get: function () {\n      return this.cell.kernelConstraint;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"recurrentConstraint\", { get: function () {\n      return this.cell.recurrentConstraint;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"biasConstraint\", { get: function () {\n      return this.cell.biasConstraint;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"dropout\", { get: function () {\n      return this.cell.dropout;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"recurrentDropout\", { get: function () {\n      return this.cell.recurrentDropout;\n    }, enumerable: !0, configurable: !0 }), t.prototype.getConfig = function () {\n    var t = { units: this.units, activation: serializeActivation(this.activation), useBias: this.useBias, kernelInitializer: serializeInitializer(this.kernelInitializer), recurrentInitializer: serializeInitializer(this.recurrentInitializer), biasInitializer: serializeInitializer(this.biasInitializer), kernelRegularizer: serializeRegularizer(this.kernelRegularizer), recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer), biasRegularizer: serializeRegularizer(this.biasRegularizer), activityRegularizer: serializeRegularizer(this.activityRegularizer), kernelConstraint: serializeConstraint(this.kernelConstraint), recurrentConstraint: serializeConstraint(this.recurrentConstraint), biasConstraint: serializeConstraint(this.biasConstraint), dropout: this.dropout, recurrentDropout: this.recurrentDropout },\n        n = e.prototype.getConfig.call(this);return delete n.cell, Object.assign(t, n), t;\n  }, t.className = \"SimpleRNN\", t;\n}(RNN);_tfjsCore.serialization.registerClass(SimpleRNN);var GRUCell = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this;return n.DEFAULT_ACTIVATION = \"tanh\", n.DEFAULT_RECURRENT_ACTIVATION = \"hardSigmoid\", n.DEFAULT_KERNEL_INITIALIZER = \"glorotNormal\", n.DEFAULT_RECURRENT_INITIALIZER = \"orthogonal\", n.DEFAULT_BIAS_INITIALIZER = \"zeros\", n.units = t.units, n.activation = getActivation(void 0 === t.activation ? n.DEFAULT_ACTIVATION : t.activation), n.recurrentActivation = getActivation(void 0 === t.recurrentActivation ? n.DEFAULT_RECURRENT_ACTIVATION : t.recurrentActivation), n.useBias = null == t.useBias || t.useBias, n.kernelInitializer = getInitializer(t.kernelInitializer || n.DEFAULT_KERNEL_INITIALIZER), n.recurrentInitializer = getInitializer(t.recurrentInitializer || n.DEFAULT_RECURRENT_INITIALIZER), n.biasInitializer = getInitializer(t.biasInitializer || n.DEFAULT_BIAS_INITIALIZER), n.kernelRegularizer = getRegularizer(t.kernelRegularizer), n.recurrentRegularizer = getRegularizer(t.recurrentRegularizer), n.biasRegularizer = getRegularizer(t.biasRegularizer), n.kernelConstraint = getConstraint(t.kernelConstraint), n.recurrentConstraint = getConstraint(t.recurrentConstraint), n.biasConstraint = getConstraint(t.biasConstraint), n.dropout = min$1([1, max$1([0, null == t.dropout ? 0 : t.dropout])]), n.recurrentDropout = min$1([1, max$1([0, null == t.recurrentDropout ? 0 : t.recurrentDropout])]), n.implementation = t.implementation, n.stateSize = n.units, n.dropoutMask = null, n.recurrentDropoutMask = null, n;\n  }return __extends(t, e), t.prototype.build = function (e) {\n    var t = (e = getExactlyOneShape(e))[e.length - 1];this.kernel = this.addWeight(\"kernel\", [t, 3 * this.units], null, this.kernelInitializer, this.kernelRegularizer, !0, this.kernelConstraint), this.recurrentKernel = this.addWeight(\"recurrent_kernel\", [this.units, 3 * this.units], null, this.recurrentInitializer, this.recurrentRegularizer, !0, this.recurrentConstraint), this.useBias ? this.bias = this.addWeight(\"bias\", [3 * this.units], null, this.biasInitializer, this.biasRegularizer, !0, this.biasConstraint) : this.bias = null, this.built = !0;\n  }, t.prototype.call = function (e, t) {\n    var n = this;return (0, _tfjsCore.tidy)(function () {\n      if (2 !== (e = e).length) throw new ValueError(\"GRUCell expects 2 input Tensors (inputs, h, c), got \" + e.length + \".\");var r = null != t.training && t.training,\n          i = e[1];e = e[0], 0 < n.dropout && n.dropout < 1 && null == n.dropoutMask && (n.dropoutMask = generateDropoutMask(function () {\n        return (0, _tfjsCore.onesLike)(e);\n      }, n.dropout, r, 3)), 0 < n.recurrentDropout && n.recurrentDropout < 1 && null == n.recurrentDropoutMask && (n.recurrentDropoutMask = generateDropoutMask(function () {\n        return (0, _tfjsCore.onesLike)(i);\n      }, n.recurrentDropout, r, 3));var a,\n          o,\n          s,\n          l = n.dropoutMask,\n          u = n.recurrentDropoutMask;if (1 === n.implementation) {\n        var c = sliceAlongLastAxis(n.kernel.read(), 0, n.units),\n            p = sliceAlongLastAxis(n.kernel.read(), n.units, n.units),\n            h = sliceAlongLastAxis(n.kernel.read(), 2 * n.units, n.units),\n            d = sliceAlongLastAxis(n.recurrentKernel.read(), 0, n.units),\n            g = sliceAlongLastAxis(n.recurrentKernel.read(), n.units, n.units),\n            f = sliceAlongLastAxis(n.recurrentKernel.read(), 2 * n.units, n.units),\n            m = void 0,\n            y = void 0,\n            v = void 0;0 < n.dropout && n.dropout < 1 ? (m = (0, _tfjsCore.mul)(e, l[0]), y = (0, _tfjsCore.mul)(e, l[1]), v = (0, _tfjsCore.mul)(e, l[2])) : (m = e, y = e, v = e);var b = dot(m, c),\n            w = dot(y, p),\n            z = dot(v, h);if (n.useBias) {\n          var S = sliceAlongFirstAxis(n.bias.read(), 0, n.units),\n              A = sliceAlongFirstAxis(n.bias.read(), n.units, n.units),\n              I = sliceAlongFirstAxis(n.bias.read(), 2 * n.units, n.units);b = biasAdd(b, S), w = biasAdd(w, A), z = biasAdd(z, I);\n        }var C = void 0,\n            N = void 0,\n            E = void 0;0 < n.recurrentDropout && n.recurrentDropout < 1 ? (C = (0, _tfjsCore.mul)(i, u[0]), N = (0, _tfjsCore.mul)(i, u[1]), E = (0, _tfjsCore.mul)(i, u[2])) : (C = i, N = i, E = i), a = n.recurrentActivation.apply((0, _tfjsCore.add)(b, dot(C, d))), o = n.recurrentActivation.apply((0, _tfjsCore.add)(w, dot(N, g))), s = n.activation.apply((0, _tfjsCore.add)(z, dot((0, _tfjsCore.mul)(o, E), f)));\n      } else {\n        0 < n.dropout && n.dropout < 1 && (e = (0, _tfjsCore.mul)(e, l[0]));var _ = dot(e, n.kernel.read());n.useBias && (_ = biasAdd(_, n.bias.read())), 0 < n.dropout && n.dropout < 1 && (i = (0, _tfjsCore.mul)(i, u[0]));var k = dot(i, sliceAlongLastAxis(n.recurrentKernel.read(), 0, 2 * n.units)),\n            L = (b = sliceAlongLastAxis(_, 0, n.units), w = sliceAlongLastAxis(_, n.units, n.units), sliceAlongLastAxis(k, 0, n.units)),\n            x = sliceAlongLastAxis(k, n.units, n.units);a = n.recurrentActivation.apply((0, _tfjsCore.add)(b, L)), o = n.recurrentActivation.apply((0, _tfjsCore.add)(w, x));z = sliceAlongLastAxis(_, 2 * n.units, n.units);var T = dot((0, _tfjsCore.mul)(o, i), sliceAlongLastAxis(n.recurrentKernel.read(), 2 * n.units, n.units));s = n.activation.apply((0, _tfjsCore.add)(z, T));\n      }var R = (0, _tfjsCore.add)((0, _tfjsCore.mul)(a, i), (0, _tfjsCore.mul)((0, _tfjsCore.add)(getScalar(1), (0, _tfjsCore.neg)(a)), s));return [R, R];\n    });\n  }, t.prototype.getConfig = function () {\n    var t = { units: this.units, activation: serializeActivation(this.activation), recurrentActivation: serializeActivation(this.recurrentActivation), useBias: this.useBias, kernelInitializer: serializeInitializer(this.kernelInitializer), recurrentInitializer: serializeInitializer(this.recurrentInitializer), biasInitializer: serializeInitializer(this.biasInitializer), kernelRegularizer: serializeRegularizer(this.kernelRegularizer), recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer), biasRegularizer: serializeRegularizer(this.biasRegularizer), activityRegularizer: serializeRegularizer(this.activityRegularizer), kernelConstraint: serializeConstraint(this.kernelConstraint), recurrentConstraint: serializeConstraint(this.recurrentConstraint), biasConstraint: serializeConstraint(this.biasConstraint), dropout: this.dropout, recurrentDropout: this.recurrentDropout, implementation: this.implementation },\n        n = e.prototype.getConfig.call(this);return Object.assign(t, n), t;\n  }, t.className = \"GRUCell\", t;\n}(RNNCell);_tfjsCore.serialization.registerClass(GRUCell);var GRU = function (e) {\n  function t(t) {\n    return 0 === t.implementation && console.warn(\"`implementation=0` has been deprecated, and now defaults to `implementation=1`. Please update your layer call.\"), t.cell = new GRUCell(t), e.call(this, t) || this;\n  }return __extends(t, e), t.prototype.call = function (t, n) {\n    var r = this;return (0, _tfjsCore.tidy)(function () {\n      null != r.cell.dropoutMask && ((0, _tfjsCore.dispose)(r.cell.dropoutMask), r.cell.dropoutMask = null), null != r.cell.recurrentDropoutMask && ((0, _tfjsCore.dispose)(r.cell.recurrentDropoutMask), r.cell.recurrentDropoutMask = null);var i = null == n ? null : n.mask,\n          a = null == n ? null : n.training,\n          o = null == n ? null : n.initialState;return e.prototype.call.call(r, t, { mask: i, training: a, initialState: o });\n    });\n  }, Object.defineProperty(t.prototype, \"units\", { get: function () {\n      return this.cell.units;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"activation\", { get: function () {\n      return this.cell.activation;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"recurrentActivation\", { get: function () {\n      return this.cell.recurrentActivation;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"useBias\", { get: function () {\n      return this.cell.useBias;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"kernelInitializer\", { get: function () {\n      return this.cell.kernelInitializer;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"recurrentInitializer\", { get: function () {\n      return this.cell.recurrentInitializer;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"biasInitializer\", { get: function () {\n      return this.cell.biasInitializer;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"kernelRegularizer\", { get: function () {\n      return this.cell.kernelRegularizer;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"recurrentRegularizer\", { get: function () {\n      return this.cell.recurrentRegularizer;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"biasRegularizer\", { get: function () {\n      return this.cell.biasRegularizer;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"kernelConstraint\", { get: function () {\n      return this.cell.kernelConstraint;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"recurrentConstraint\", { get: function () {\n      return this.cell.recurrentConstraint;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"biasConstraint\", { get: function () {\n      return this.cell.biasConstraint;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"dropout\", { get: function () {\n      return this.cell.dropout;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"recurrentDropout\", { get: function () {\n      return this.cell.recurrentDropout;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"implementation\", { get: function () {\n      return this.cell.implementation;\n    }, enumerable: !0, configurable: !0 }), t.prototype.getConfig = function () {\n    var t = { units: this.units, activation: serializeActivation(this.activation), recurrentActivation: serializeActivation(this.recurrentActivation), useBias: this.useBias, kernelInitializer: serializeInitializer(this.kernelInitializer), recurrentInitializer: serializeInitializer(this.recurrentInitializer), biasInitializer: serializeInitializer(this.biasInitializer), kernelRegularizer: serializeRegularizer(this.kernelRegularizer), recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer), biasRegularizer: serializeRegularizer(this.biasRegularizer), activityRegularizer: serializeRegularizer(this.activityRegularizer), kernelConstraint: serializeConstraint(this.kernelConstraint), recurrentConstraint: serializeConstraint(this.recurrentConstraint), biasConstraint: serializeConstraint(this.biasConstraint), dropout: this.dropout, recurrentDropout: this.recurrentDropout, implementation: this.implementation },\n        n = e.prototype.getConfig.call(this);return delete n.cell, Object.assign(t, n), t;\n  }, t.fromConfig = function (e, t) {\n    return 0 === t.implmentation && (t.implementation = 1), new e(t);\n  }, t.className = \"GRU\", t;\n}(RNN);_tfjsCore.serialization.registerClass(GRU);var LSTMCell = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this;return n.DEFAULT_ACTIVATION = \"tanh\", n.DEFAULT_RECURRENT_ACTIVATION = \"hardSigmoid\", n.DEFAULT_KERNEL_INITIALIZER = \"glorotNormal\", n.DEFAULT_RECURRENT_INITIALIZER = \"orthogonal\", n.DEFAULT_BIAS_INITIALIZER = \"zeros\", n.units = t.units, n.activation = getActivation(void 0 === t.activation ? n.DEFAULT_ACTIVATION : t.activation), n.recurrentActivation = getActivation(void 0 === t.recurrentActivation ? n.DEFAULT_RECURRENT_ACTIVATION : t.recurrentActivation), n.useBias = null == t.useBias || t.useBias, n.kernelInitializer = getInitializer(t.kernelInitializer || n.DEFAULT_KERNEL_INITIALIZER), n.recurrentInitializer = getInitializer(t.recurrentInitializer || n.DEFAULT_RECURRENT_INITIALIZER), n.biasInitializer = getInitializer(t.biasInitializer || n.DEFAULT_BIAS_INITIALIZER), n.unitForgetBias = t.unitForgetBias, n.kernelRegularizer = getRegularizer(t.kernelRegularizer), n.recurrentRegularizer = getRegularizer(t.recurrentRegularizer), n.biasRegularizer = getRegularizer(t.biasRegularizer), n.kernelConstraint = getConstraint(t.kernelConstraint), n.recurrentConstraint = getConstraint(t.recurrentConstraint), n.biasConstraint = getConstraint(t.biasConstraint), n.dropout = min$1([1, max$1([0, null == t.dropout ? 0 : t.dropout])]), n.recurrentDropout = min$1([1, max$1([0, null == t.recurrentDropout ? 0 : t.recurrentDropout])]), n.implementation = t.implementation, n.stateSize = [n.units, n.units], n.dropoutMask = null, n.recurrentDropoutMask = null, n;\n  }return __extends(t, e), t.prototype.build = function (e) {\n    var t,\n        n,\n        r = (e = getExactlyOneShape(e))[e.length - 1];if (this.kernel = this.addWeight(\"kernel\", [r, 4 * this.units], null, this.kernelInitializer, this.kernelRegularizer, !0, this.kernelConstraint), this.recurrentKernel = this.addWeight(\"recurrent_kernel\", [this.units, 4 * this.units], null, this.recurrentInitializer, this.recurrentRegularizer, !0, this.recurrentConstraint), this.useBias) {\n      if (this.unitForgetBias) {\n        var i = this.biasInitializer,\n            a = this.units;t = new ((n = function (e) {\n          function t() {\n            return null !== e && e.apply(this, arguments) || this;\n          }return __extends(t, e), t.prototype.apply = function (e, t) {\n            var n = i.apply([a]),\n                r = new Ones().apply([a]),\n                o = i.apply([2 * a]);return concatAlongFirstAxis(concatAlongFirstAxis(n, r), o);\n          }, t;\n        }(Initializer)).className = \"CustomInit\", n)();\n      } else t = this.biasInitializer;this.bias = this.addWeight(\"bias\", [4 * this.units], null, t, this.biasRegularizer, !0, this.biasConstraint);\n    } else this.bias = null;this.built = !0;\n  }, t.prototype.call = function (e, t) {\n    var n = this;return (0, _tfjsCore.tidy)(function () {\n      var r = null != t.training && t.training;if (3 !== (e = e).length) throw new ValueError(\"LSTMCell expects 3 input Tensors (inputs, h, c), got \" + e.length + \".\");var i = e[1],\n          a = e[2];e = e[0], 0 < n.dropout && n.dropout < 1 && null == n.dropoutMask && (n.dropoutMask = generateDropoutMask(function () {\n        return (0, _tfjsCore.onesLike)(e);\n      }, n.dropout, r, 4)), 0 < n.recurrentDropout && n.recurrentDropout < 1 && null == n.recurrentDropoutMask && (n.recurrentDropoutMask = generateDropoutMask(function () {\n        return (0, _tfjsCore.onesLike)(i);\n      }, n.recurrentDropout, r, 4));var o,\n          s,\n          l,\n          u,\n          c = n.dropoutMask,\n          p = n.recurrentDropoutMask;if (1 === n.implementation) {\n        var h = sliceAlongLastAxis(n.kernel.read(), 0, n.units),\n            d = sliceAlongLastAxis(n.kernel.read(), n.units, n.units),\n            g = sliceAlongLastAxis(n.kernel.read(), 2 * n.units, n.units),\n            f = sliceAlongLastAxis(n.kernel.read(), 3 * n.units, n.units),\n            m = sliceAlongLastAxis(n.recurrentKernel.read(), 0, n.units),\n            y = sliceAlongLastAxis(n.recurrentKernel.read(), n.units, n.units),\n            v = sliceAlongLastAxis(n.recurrentKernel.read(), 2 * n.units, n.units),\n            b = sliceAlongLastAxis(n.recurrentKernel.read(), 3 * n.units, n.units),\n            w = void 0,\n            z = void 0,\n            S = void 0,\n            A = void 0;0 < n.dropout && n.dropout < 1 ? (w = (0, _tfjsCore.mul)(e, c[0]), z = (0, _tfjsCore.mul)(e, c[1]), S = (0, _tfjsCore.mul)(e, c[2]), A = (0, _tfjsCore.mul)(e, c[3])) : (w = e, z = e, S = e, A = e);var I = dot(w, h),\n            C = dot(z, d),\n            N = dot(S, g),\n            E = dot(A, f);if (n.useBias) {\n          var _ = sliceAlongFirstAxis(n.bias.read(), 0, n.units),\n              k = sliceAlongFirstAxis(n.bias.read(), n.units, n.units),\n              L = sliceAlongFirstAxis(n.bias.read(), 2 * n.units, n.units),\n              x = sliceAlongFirstAxis(n.bias.read(), 3 * n.units, n.units);I = biasAdd(I, _), C = biasAdd(C, k), N = biasAdd(N, L), E = biasAdd(E, x);\n        }var T = void 0,\n            R = void 0,\n            O = void 0,\n            D = void 0;0 < n.recurrentDropout && n.recurrentDropout < 1 ? (T = (0, _tfjsCore.mul)(i, p[0]), R = (0, _tfjsCore.mul)(i, p[1]), O = (0, _tfjsCore.mul)(i, p[2]), D = (0, _tfjsCore.mul)(i, p[3])) : (T = i, R = i, O = i, D = i), o = n.recurrentActivation.apply((0, _tfjsCore.add)(I, dot(T, m))), s = n.recurrentActivation.apply((0, _tfjsCore.add)(C, dot(R, y))), l = (0, _tfjsCore.add)((0, _tfjsCore.mul)(s, a), (0, _tfjsCore.mul)(o, n.activation.apply((0, _tfjsCore.add)(N, dot(O, v))))), u = n.recurrentActivation.apply((0, _tfjsCore.add)(E, dot(D, b)));\n      } else {\n        0 < n.dropout && n.dropout < 1 && (e = (0, _tfjsCore.mul)(e, c[0]));var M = dot(e, n.kernel.read());0 < n.recurrentDropout && n.recurrentDropout < 1 && (i = (0, _tfjsCore.mul)(i, p[0])), M = (0, _tfjsCore.add)(M, dot(i, n.recurrentKernel.read())), n.useBias && (M = biasAdd(M, n.bias.read()));var P = sliceAlongLastAxis(M, 0, n.units),\n            V = sliceAlongLastAxis(M, n.units, n.units),\n            F = sliceAlongLastAxis(M, 2 * n.units, n.units),\n            B = sliceAlongLastAxis(M, 3 * n.units, n.units);o = n.recurrentActivation.apply(P), s = n.recurrentActivation.apply(V), l = (0, _tfjsCore.add)((0, _tfjsCore.mul)(s, a), (0, _tfjsCore.mul)(o, n.activation.apply(F))), u = n.recurrentActivation.apply(B);\n      }var U = (0, _tfjsCore.mul)(u, n.activation.apply(l));return [U, U, l];\n    });\n  }, t.prototype.getConfig = function () {\n    var t = { units: this.units, activation: serializeActivation(this.activation), recurrentActivation: serializeActivation(this.recurrentActivation), useBias: this.useBias, kernelInitializer: serializeInitializer(this.kernelInitializer), recurrentInitializer: serializeInitializer(this.recurrentInitializer), biasInitializer: serializeInitializer(this.biasInitializer), unitForgetBias: this.unitForgetBias, kernelRegularizer: serializeRegularizer(this.kernelRegularizer), recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer), biasRegularizer: serializeRegularizer(this.biasRegularizer), activityRegularizer: serializeRegularizer(this.activityRegularizer), kernelConstraint: serializeConstraint(this.kernelConstraint), recurrentConstraint: serializeConstraint(this.recurrentConstraint), biasConstraint: serializeConstraint(this.biasConstraint), dropout: this.dropout, recurrentDropout: this.recurrentDropout, implementation: this.implementation },\n        n = e.prototype.getConfig.call(this);return Object.assign(t, n), t;\n  }, t.className = \"LSTMCell\", t;\n}(RNNCell);_tfjsCore.serialization.registerClass(LSTMCell);var LSTM = function (e) {\n  function t(t) {\n    return 0 === t.implementation && console.warn(\"`implementation=0` has been deprecated, and now defaults to `implementation=1`. Please update your layer call.\"), t.cell = new LSTMCell(t), e.call(this, t) || this;\n  }return __extends(t, e), t.prototype.call = function (t, n) {\n    var r = this;return (0, _tfjsCore.tidy)(function () {\n      null != r.cell.dropoutMask && ((0, _tfjsCore.dispose)(r.cell.dropoutMask), r.cell.dropoutMask = null), null != r.cell.recurrentDropoutMask && ((0, _tfjsCore.dispose)(r.cell.recurrentDropoutMask), r.cell.recurrentDropoutMask = null);var i = null == n ? null : n.mask,\n          a = null == n ? null : n.training,\n          o = null == n ? null : n.initialState;return e.prototype.call.call(r, t, { mask: i, training: a, initialState: o });\n    });\n  }, Object.defineProperty(t.prototype, \"units\", { get: function () {\n      return this.cell.units;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"activation\", { get: function () {\n      return this.cell.activation;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"recurrentActivation\", { get: function () {\n      return this.cell.recurrentActivation;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"useBias\", { get: function () {\n      return this.cell.useBias;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"kernelInitializer\", { get: function () {\n      return this.cell.kernelInitializer;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"recurrentInitializer\", { get: function () {\n      return this.cell.recurrentInitializer;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"biasInitializer\", { get: function () {\n      return this.cell.biasInitializer;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"unitForgetBias\", { get: function () {\n      return this.cell.unitForgetBias;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"kernelRegularizer\", { get: function () {\n      return this.cell.kernelRegularizer;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"recurrentRegularizer\", { get: function () {\n      return this.cell.recurrentRegularizer;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"biasRegularizer\", { get: function () {\n      return this.cell.biasRegularizer;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"kernelConstraint\", { get: function () {\n      return this.cell.kernelConstraint;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"recurrentConstraint\", { get: function () {\n      return this.cell.recurrentConstraint;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"biasConstraint\", { get: function () {\n      return this.cell.biasConstraint;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"dropout\", { get: function () {\n      return this.cell.dropout;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"recurrentDropout\", { get: function () {\n      return this.cell.recurrentDropout;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"implementation\", { get: function () {\n      return this.cell.implementation;\n    }, enumerable: !0, configurable: !0 }), t.prototype.getConfig = function () {\n    var t = { units: this.units, activation: serializeActivation(this.activation), recurrentActivation: serializeActivation(this.recurrentActivation), useBias: this.useBias, kernelInitializer: serializeInitializer(this.kernelInitializer), recurrentInitializer: serializeInitializer(this.recurrentInitializer), biasInitializer: serializeInitializer(this.biasInitializer), unitForgetBias: this.unitForgetBias, kernelRegularizer: serializeRegularizer(this.kernelRegularizer), recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer), biasRegularizer: serializeRegularizer(this.biasRegularizer), activityRegularizer: serializeRegularizer(this.activityRegularizer), kernelConstraint: serializeConstraint(this.kernelConstraint), recurrentConstraint: serializeConstraint(this.recurrentConstraint), biasConstraint: serializeConstraint(this.biasConstraint), dropout: this.dropout, recurrentDropout: this.recurrentDropout, implementation: this.implementation },\n        n = e.prototype.getConfig.call(this);return delete n.cell, Object.assign(t, n), t;\n  }, t.fromConfig = function (e, t) {\n    return 0 === t.implmentation && (t.implementation = 1), new e(t);\n  }, t.className = \"LSTM\", t;\n}(RNN);_tfjsCore.serialization.registerClass(LSTM);var StackedRNNCells = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this;return n.cells = t.cells, n;\n  }return __extends(t, e), Object.defineProperty(t.prototype, \"stateSize\", { get: function () {\n      for (var e = [], t = 0, n = this.cells.slice().reverse(); t < n.length; t++) {\n        var r = n[t];Array.isArray(r.stateSize) ? e.push.apply(e, r.stateSize) : e.push(r.stateSize);\n      }return e;\n    }, enumerable: !0, configurable: !0 }), t.prototype.call = function (e, t) {\n    var n = this;return (0, _tfjsCore.tidy)(function () {\n      for (var r = (e = e).slice(1), i = [], a = 0, o = n.cells.slice().reverse(); a < o.length; a++) {\n        var s = o[a];Array.isArray(s.stateSize) ? i.push(r.splice(0, s.stateSize.length)) : i.push(r.splice(0, 1));\n      }i.reverse();for (var l, u = [], c = 0; c < n.cells.length; ++c) {\n        s = n.cells[c];r = i[c], l = 0 === c ? [e[0]].concat(r) : [l[0]].concat(r), l = s.call(l, t), u.push(l.slice(1));\n      }r = [];for (var p = 0, h = u.slice().reverse(); p < h.length; p++) {\n        var d = h[p];r.push.apply(r, d);\n      }return [l[0]].concat(r);\n    });\n  }, t.prototype.build = function (e) {\n    var t;isArrayOfShapes(e) && (e = e[0]), e = e;for (var n = 0, r = this.cells; n < r.length; n++) {\n      var i = r[n];i.build(e), t = Array.isArray(i.stateSize) ? i.stateSize[0] : i.stateSize, e = [e[0], t];\n    }this.built = !0;\n  }, t.prototype.getConfig = function () {\n    for (var t = [], n = 0, r = this.cells; n < r.length; n++) {\n      var i = r[n];t.push({ className: this.getClassName(), config: i.getConfig() });\n    }var a = { cells: t },\n        o = e.prototype.getConfig.call(this);return Object.assign(a, o), a;\n  }, t.fromConfig = function (e, t, n) {\n    void 0 === n && (n = {});for (var r = [], i = 0, a = t.cells; i < a.length; i++) {\n      var o = a[i];r.push(deserialize(o, n));\n    }return new e({ cells: r });\n  }, Object.defineProperty(t.prototype, \"trainableWeights\", { get: function () {\n      if (!this.trainable) return [];for (var e = [], t = 0, n = this.cells; t < n.length; t++) {\n        var r = n[t];e.push.apply(e, r.trainableWeights);\n      }return e;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"nonTrainableWeights\", { get: function () {\n      for (var e = [], t = 0, n = this.cells; t < n.length; t++) {\n        var r = n[t];e.push.apply(e, r.nonTrainableWeights);\n      }if (!this.trainable) {\n        for (var i = [], a = 0, o = this.cells; a < o.length; a++) {\n          r = o[a];i.push.apply(i, r.trainableWeights);\n        }return i.concat(e);\n      }return e;\n    }, enumerable: !0, configurable: !0 }), t.prototype.getWeights = function () {\n    for (var e = [], t = 0, n = this.cells; t < n.length; t++) {\n      var r = n[t];e.push.apply(e, r.weights);\n    }return batchGetValue(e);\n  }, t.prototype.setWeights = function (e) {\n    for (var t = [], n = 0, r = this.cells; n < r.length; n++) for (var i = r[n], a = i.weights.length, o = e.splice(a), s = 0; s < i.weights.length; ++s) t.push([i.weights[s], o[s]]);batchSetValue(t);\n  }, t.className = \"StackedRNNCells\", t;\n}(RNNCell);function generateDropoutMask(e, t, n, r) {\n  function i() {\n    return dropout(e(), getScalar(t));\n  }if (void 0 === n && (n = null), void 0 === r && (r = 1), r > 1) {\n    for (var a = [], o = 0; o < r; o++) a.push(inTrainPhase(i, e, n));return a.forEach(function (e) {\n      return (0, _tfjsCore.keep)(e);\n    }), a;\n  }return (0, _tfjsCore.keep)(inTrainPhase(i, e, n));\n}_tfjsCore.serialization.registerClass(StackedRNNCells);var Wrapper = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this;return n.layer = t.layer, n;\n  }return __extends(t, e), t.prototype.build = function (e) {\n    this.built = !0;\n  }, Object.defineProperty(t.prototype, \"trainable\", { get: function () {\n      return null != this.layer && this.layer.trainable;\n    }, set: function (e) {\n      null != this.layer && (this.layer.trainable = e);\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"trainableWeights\", { get: function () {\n      return this.layer.trainableWeights;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"nonTrainableWeights\", { get: function () {\n      return this.layer.nonTrainableWeights;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"updates\", { get: function () {\n      return this.layer._updates;\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"losses\", { get: function () {\n      return this.layer.losses;\n    }, enumerable: !0, configurable: !0 }), t.prototype.getWeights = function () {\n    return this.layer.getWeights();\n  }, t.prototype.setWeights = function (e) {\n    this.layer.setWeights(e);\n  }, t.prototype.getConfig = function () {\n    var t = { layer: { className: this.layer.getClassName(), config: this.layer.getConfig() } },\n        n = e.prototype.getConfig.call(this);return Object.assign(t, n), t;\n  }, t.fromConfig = function (e, t, n) {\n    void 0 === n && (n = {});var r = deserialize(t.layer, n);delete t.layer;var i = { layer: r };return Object.assign(i, t), new e(i);\n  }, t;\n}(Layer),\n    TimeDistributed = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this;return n.supportsMasking = !0, n;\n  }return __extends(t, e), t.prototype.build = function (t) {\n    if ((t = getExactlyOneShape(t)).length < 3) throw new ValueError(\"TimeDistributed layer expects an input shape >= 3D, but received input shape \" + JSON.stringify(t));this.inputSpec = [{ shape: t }];var n = [t[0]].concat(t.slice(2));this.layer.built || (this.layer.build(n), this.layer.built = !0), e.prototype.build.call(this, t);\n  }, t.prototype.computeOutputShape = function (e) {\n    var t = [(e = getExactlyOneShape(e))[0]].concat(e.slice(2)),\n        n = this.layer.computeOutputShape(t),\n        r = e[1];return [n[0], r].concat(n.slice(1));\n  }, t.prototype.call = function (e, t) {\n    var n = this;return (0, _tfjsCore.tidy)(function () {\n      return rnn(function (e, r) {\n        return [getExactlyOneTensor(n.layer.call(e, t)), []];\n      }, e = getExactlyOneTensor(e), [], !1, null, null, !1, !0)[1];\n    });\n  }, t.className = \"TimeDistributed\", t;\n}(Wrapper);_tfjsCore.serialization.registerClass(TimeDistributed);var VALID_BIDIRECTIONAL_MERGE_MODES = [\"sum\", \"mul\", \"concat\", \"ave\"];function checkBidirectionalMergeMode(e) {\n  checkStringTypeUnionValue(VALID_BIDIRECTIONAL_MERGE_MODES, \"BidirectionalMergeMode\", e);\n}var Bidirectional = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this,\n        r = t.layer.getConfig();if (n.forwardLayer = deserialize({ className: t.layer.getClassName(), config: r }), r.goBackwards = !0 !== r.goBackwards, n.backwardLayer = deserialize({ className: t.layer.getClassName(), config: r }), n.forwardLayer.name = \"forward_\" + n.forwardLayer.name, n.backwardLayer.name = \"backward_\" + n.backwardLayer.name, checkBidirectionalMergeMode(t.mergeMode), n.mergeMode = t.mergeMode, t.weights) throw new NotImplementedError(\"weights support is not implemented for Bidirectional layer yet.\");return n._stateful = t.layer.stateful, n.returnSequences = t.layer.returnSequences, n.returnState = t.layer.returnState, n.supportsMasking = !0, n._trainable = !0, n.inputSpec = t.layer.inputSpec, n.numConstants = null, n;\n  }return __extends(t, e), Object.defineProperty(t.prototype, \"trainable\", { get: function () {\n      return this._trainable;\n    }, set: function (e) {\n      this._trainable = e, null != this.forwardLayer && (this.forwardLayer.trainable = e), null != this.backwardLayer && (this.backwardLayer.trainable = e);\n    }, enumerable: !0, configurable: !0 }), t.prototype.getWeights = function () {\n    return this.forwardLayer.getWeights().concat(this.backwardLayer.getWeights());\n  }, t.prototype.setWeights = function (e) {\n    var t = e.length,\n        n = Math.floor(t / 2);this.forwardLayer.setWeights(e.slice(0, n)), this.backwardLayer.setWeights(e.slice(n));\n  }, t.prototype.computeOutputShape = function (e) {\n    var t,\n        n,\n        r,\n        i = this.forwardLayer.computeOutputShape(e);return Array.isArray(i) && Array.isArray(i[0]) || (i = [i]), i = i, this.returnState ? (r = i.slice(1), t = i[0]) : t = i[0], t = t, \"concat\" === this.mergeMode ? (t[t.length - 1] *= 2, n = [t]) : n = null == this.mergeMode ? [t, t.slice()] : [t], this.returnState ? null == this.mergeMode ? n.concat(r).concat(r.slice()) : [t].concat(r).concat(r.slice()) : singletonOrArray(n);\n  }, t.prototype.apply = function (t, n) {\n    var r = null == n ? null : n.initialState,\n        i = null == n ? null : n.constants;null == n && (n = {});var a = standardizeArgs(t, r, i, this.numConstants);if (t = a.inputs, r = a.initialState, i = a.constants, Array.isArray(t) && (r = t.slice(1), t = t[0]), (null == r || 0 === r.length) && null == i) return e.prototype.apply.call(this, t, n);var o = [],\n        s = [];if (null != r) {\n      var l = r.length;if (l % 2 > 0) throw new ValueError(\"When passing `initialState` to a Bidrectional RNN, the state should be an Array containing the states of the underlying RNNs.\");n.initialState = r, o.push.apply(o, r);var u = r.map(function (e) {\n        return new InputSpec({ shape: e.shape });\n      });this.forwardLayer.stateSpec = u.slice(0, l / 2), this.backwardLayer.stateSpec = u.slice(l / 2), s.push.apply(s, u);\n    }if (null != i) throw new NotImplementedError(\"Support for constants in Bidirectional layers is not implemented yet.\");for (var c = o[0] instanceof SymbolicTensor, p = 0, h = o; p < h.length; p++) {\n      if (h[p] instanceof SymbolicTensor !== c) throw new ValueError(\"The initial state of a Bidirectional layer cannot be specified as a mix of symbolic and non-symbolic tensors\");\n    }if (c) {\n      var d = [t].concat(o),\n          g = this.inputSpec.concat(s),\n          f = this.inputSpec;this.inputSpec = g;var m = e.prototype.apply.call(this, d, n);return this.inputSpec = f, m;\n    }return e.prototype.apply.call(this, t, n);\n  }, t.prototype.call = function (e, t) {\n    var n = this;return (0, _tfjsCore.tidy)(function () {\n      if (null != t.mask) throw new NotImplementedError(\"The support for masking is not implemented for Bidirectional layers yet.\");var r,\n          i,\n          a,\n          o,\n          s = t.initialState;if (null == s) r = n.forwardLayer.call(e, t), i = n.backwardLayer.call(e, t);else {\n        var l = s.slice(0, s.length / 2),\n            u = s.slice(s.length / 2);r = n.forwardLayer.call(e, Object.assign(t, { initialState: l })), i = n.backwardLayer.call(e, Object.assign(t, { initialState: u }));\n      }return n.returnState && (Array.isArray(r) && (a = r.slice(1).concat(i.slice(1))), r = r[0], i = i[0]), n.returnSequences && (i = (0, _tfjsCore.reverse)(i, 1)), \"concat\" === n.mergeMode ? o = concatenate([r, i]) : \"sum\" === n.mergeMode ? o = (0, _tfjsCore.add)(r, i) : \"ave\" === n.mergeMode ? o = (0, _tfjsCore.mul)(getScalar(.5), (0, _tfjsCore.add)(r, i)) : \"mul\" === n.mergeMode ? o = (0, _tfjsCore.mul)(r, i) : null == n.mergeMode && (o = [r, i]), n.returnState ? null == n.mergeMode ? o.concat(a) : [o].concat(a) : o;\n    });\n  }, t.prototype.resetStates = function (e) {\n    this.forwardLayer.resetStates(), this.backwardLayer.resetStates();\n  }, t.prototype.build = function (e) {\n    var t = this;nameScope(this.forwardLayer.name, function () {\n      t.forwardLayer.build(e);\n    }), nameScope(this.backwardLayer.name, function () {\n      t.backwardLayer.build(e);\n    }), this.built = !0;\n  }, Object.defineProperty(t.prototype, \"trainableWeights\", { get: function () {\n      return this.forwardLayer.trainableWeights.concat(this.backwardLayer.trainableWeights);\n    }, enumerable: !0, configurable: !0 }), Object.defineProperty(t.prototype, \"nonTrainableWeights\", { get: function () {\n      return this.forwardLayer.nonTrainableWeights.concat(this.backwardLayer.nonTrainableWeights);\n    }, enumerable: !0, configurable: !0 }), t.prototype.getConfig = function () {\n    var t = { mergeMode: this.mergeMode },\n        n = e.prototype.getConfig.call(this);return Object.assign(t, n), t;\n  }, t.fromConfig = function (e, t) {\n    var n = deserialize(t.layer);if (delete t.layer, null != t.numConstants) throw new NotImplementedError(\"Deserialization of a Bidirectional layer with numConstants present is not supported yet.\");var r = t;return r.layer = n, new e(r);\n  }, t.className = \"Bidirectional\", t;\n}(Wrapper);function inputLayer(e) {\n  return new InputLayer(e);\n}function elu$2(e) {\n  return new ELU(e);\n}function reLU(e) {\n  return new ReLU(e);\n}function leakyReLU(e) {\n  return new LeakyReLU(e);\n}function softmax$1(e) {\n  return new Softmax$1(e);\n}function thresholdedReLU(e) {\n  return new ThresholdedReLU(e);\n}function conv1d$2(e) {\n  return new Conv1D(e);\n}function conv2d$2(e) {\n  return new Conv2D(e);\n}function conv2dTranspose$1(e) {\n  return new Conv2DTranspose(e);\n}function separableConv2d$1(e) {\n  return new SeparableConv2D(e);\n}function cropping2D(e) {\n  return new Cropping2D(e);\n}function upSampling2d(e) {\n  return new UpSampling2D(e);\n}function depthwiseConv2d$2(e) {\n  return new DepthwiseConv2D(e);\n}function activation(e) {\n  return new Activation$1(e);\n}function dense(e) {\n  return new Dense(e);\n}function dropout$1(e) {\n  return new Dropout(e);\n}function flatten$1(e) {\n  return new Flatten(e);\n}function repeatVector(e) {\n  return new RepeatVector(e);\n}function reshape(e) {\n  return new Reshape(e);\n}function permute(e) {\n  return new Permute(e);\n}function embedding(e) {\n  return new Embedding(e);\n}function add$2(e) {\n  return new Add(e);\n}function average$1(e) {\n  return new Average(e);\n}function concatenate$2(e) {\n  return new Concatenate(e);\n}function maximum$2(e) {\n  return new Maximum(e);\n}function minimum$2(e) {\n  return new Minimum(e);\n}function multiply$1(e) {\n  return new Multiply(e);\n}function batchNormalization$1(e) {\n  return new BatchNormalization(e);\n}function zeroPadding2d(e) {\n  return new ZeroPadding2D(e);\n}function averagePooling1d(e) {\n  return new AveragePooling1D(e);\n}function avgPool1d(e) {\n  return averagePooling1d(e);\n}function avgPooling1d(e) {\n  return averagePooling1d(e);\n}function averagePooling2d(e) {\n  return new AveragePooling2D(e);\n}function avgPool2d(e) {\n  return averagePooling2d(e);\n}function avgPooling2d(e) {\n  return averagePooling2d(e);\n}function globalAveragePooling1d(e) {\n  return new GlobalAveragePooling1D(e);\n}function globalAveragePooling2d(e) {\n  return new GlobalAveragePooling2D(e);\n}function globalMaxPooling1d(e) {\n  return new GlobalMaxPooling1D(e);\n}function globalMaxPooling2d(e) {\n  return new GlobalMaxPooling2D(e);\n}function maxPooling1d(e) {\n  return new MaxPooling1D(e);\n}function maxPooling2d(e) {\n  return new MaxPooling2D(e);\n}function gru(e) {\n  return new GRU(e);\n}function gruCell(e) {\n  return new GRUCell(e);\n}function lstm(e) {\n  return new LSTM(e);\n}function lstmCell(e) {\n  return new LSTMCell(e);\n}function simpleRNN(e) {\n  return new SimpleRNN(e);\n}function simpleRNNCell(e) {\n  return new SimpleRNNCell(e);\n}function rnn$1(e) {\n  return new RNN(e);\n}function stackedRNNCells(e) {\n  return new StackedRNNCells(e);\n}function bidirectional(e) {\n  return new Bidirectional(e);\n}function timeDistributed(e) {\n  return new TimeDistributed(e);\n}_tfjsCore.serialization.registerClass(Bidirectional);var globalMaxPool1d = globalMaxPooling1d,\n    globalMaxPool2d = globalMaxPooling2d,\n    maxPool1d = maxPooling1d,\n    maxPool2d = maxPooling2d,\n    exports_layers = Object.freeze({ inputLayer: inputLayer, elu: elu$2, reLU: reLU, leakyReLU: leakyReLU, softmax: softmax$1, thresholdedReLU: thresholdedReLU, conv1d: conv1d$2, conv2d: conv2d$2, conv2dTranspose: conv2dTranspose$1, separableConv2d: separableConv2d$1, cropping2D: cropping2D, upSampling2d: upSampling2d, depthwiseConv2d: depthwiseConv2d$2, activation: activation, dense: dense, dropout: dropout$1, flatten: flatten$1, repeatVector: repeatVector, reshape: reshape, permute: permute, embedding: embedding, add: add$2, average: average$1, concatenate: concatenate$2, maximum: maximum$2, minimum: minimum$2, multiply: multiply$1, batchNormalization: batchNormalization$1, zeroPadding2d: zeroPadding2d, averagePooling1d: averagePooling1d, avgPool1d: avgPool1d, avgPooling1d: avgPooling1d, averagePooling2d: averagePooling2d, avgPool2d: avgPool2d, avgPooling2d: avgPooling2d, globalAveragePooling1d: globalAveragePooling1d, globalAveragePooling2d: globalAveragePooling2d, globalMaxPooling1d: globalMaxPooling1d, globalMaxPooling2d: globalMaxPooling2d, maxPooling1d: maxPooling1d, maxPooling2d: maxPooling2d, gru: gru, gruCell: gruCell, lstm: lstm, lstmCell: lstmCell, simpleRNN: simpleRNN, simpleRNNCell: simpleRNNCell, rnn: rnn$1, stackedRNNCells: stackedRNNCells, bidirectional: bidirectional, timeDistributed: timeDistributed, globalMaxPool1d: globalMaxPool1d, globalMaxPool2d: globalMaxPool2d, maxPool1d: maxPool1d, maxPool2d: maxPool2d, Layer: Layer, RNN: RNN, RNNCell: RNNCell, input: input });function binaryAccuracy$1(e, t) {\n  return binaryAccuracy(e, t);\n}function binaryCrossentropy$2(e, t) {\n  return binaryCrossentropy$1(e, t);\n}function categoricalAccuracy$1(e, t) {\n  return categoricalAccuracy(e, t);\n}function categoricalCrossentropy$2(e, t) {\n  return categoricalCrossentropy$1(e, t);\n}function precision$1(e, t) {\n  return precision(e, t);\n}function recall$1(e, t) {\n  return recall(e, t);\n}function cosineProximity$1(e, t) {\n  return cosineProximity(e, t);\n}function meanAbsoluteError$1(e, t) {\n  return meanAbsoluteError(e, t);\n}function meanAbsolutePercentageError$1(e, t) {\n  return meanAbsolutePercentageError(e, t);\n}function MAPE$2(e, t) {\n  return meanAbsolutePercentageError(e, t);\n}function mape$2(e, t) {\n  return meanAbsolutePercentageError(e, t);\n}function meanSquaredError$1(e, t) {\n  return meanSquaredError(e, t);\n}function MSE$2(e, t) {\n  return meanSquaredError(e, t);\n}function mse$2(e, t) {\n  return meanSquaredError(e, t);\n}var exports_metrics = Object.freeze({ binaryAccuracy: binaryAccuracy$1, binaryCrossentropy: binaryCrossentropy$2, categoricalAccuracy: categoricalAccuracy$1, categoricalCrossentropy: categoricalCrossentropy$2, precision: precision$1, recall: recall$1, cosineProximity: cosineProximity$1, meanAbsoluteError: meanAbsoluteError$1, meanAbsolutePercentageError: meanAbsolutePercentageError$1, MAPE: MAPE$2, mape: mape$2, meanSquaredError: meanSquaredError$1, MSE: MSE$2, mse: mse$2 });function l1l2(e) {\n  return new L1L2(e);\n}function l1$1(e) {\n  return l1(e);\n}function l2$1(e) {\n  return l2(e);\n}var exports_regularizers = Object.freeze({ l1l2: l1l2, l1: l1$1, l2: l2$1 }),\n    Callback = function (e) {\n  function t() {\n    var t = null !== e && e.apply(this, arguments) || this;return t.model = null, t;\n  }return __extends(t, e), t.prototype.setModel = function (e) {\n    if (!(e instanceof Model)) throw new Error(\"model must be a Model, not some other Container\");this.model = e;\n  }, t;\n}(BaseCallback);exports.constraints = exports_constraints;\nexports.initializers = exports_initializers;\nexports.layers = exports_layers;\nexports.metrics = exports_metrics;\nexports.regularizers = exports_regularizers;\nexports.CallbackList = CallbackList;\nexports.CustomCallback = CustomCallback;\nexports.History = History;\nexports.Callback = Callback;\nexports.SymbolicTensor = SymbolicTensor;\nexports.Model = Model;\nexports.input = input;\nexports.loadModel = loadModel;\nexports.model = model;\nexports.registerCallbackConstructor = registerCallbackConstructor;\nexports.sequential = sequential;\nexports.RNN = RNN;\nexports.Sequential = Sequential;\nexports.LayerVariable = LayerVariable;\nexports.version_layers = version;\n//# sourceMappingURL=tf-layers.esm.js.map"},"hash":"6265f11e490fbf9298cf39f2bfe910c9","cacheData":{"env":{}}}