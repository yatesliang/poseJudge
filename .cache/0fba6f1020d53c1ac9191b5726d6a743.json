{"dependencies":[{"name":"/Users/LYC/Desktop/tf/TryIt/.babelrc","includedInParent":true,"mtime":1539397829000},{"name":"/Users/LYC/Desktop/tf/TryIt/package.json","includedInParent":true,"mtime":1539397829000},{"name":"babel-runtime/core-js/promise"},{"name":"@tensorflow-models/posenet","loc":{"line":1,"column":25}},{"name":"@tensorflow/tfjs","loc":{"line":2,"column":20}},{"name":"dat.gui","loc":{"line":3,"column":16}},{"name":"stats.js","loc":{"line":4,"column":18}},{"name":"./demo_util","loc":{"line":5,"column":61}}],"generated":{"js":"'use strict';\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\n\nvar _promise = require('babel-runtime/core-js/promise');\n\nvar _promise2 = _interopRequireDefault(_promise);\n\nexports.bindPage = bindPage;\n\nvar _posenet = require('@tensorflow-models/posenet');\n\nvar posenet = _interopRequireWildcard(_posenet);\n\nvar _tfjs = require('@tensorflow/tfjs');\n\nvar tf = _interopRequireWildcard(_tfjs);\n\nvar _dat = require('dat.gui');\n\nvar _dat2 = _interopRequireDefault(_dat);\n\nvar _stats = require('stats.js');\n\nvar _stats2 = _interopRequireDefault(_stats);\n\nvar _demo_util = require('./demo_util');\n\nfunction _interopRequireWildcard(obj) { if (obj && obj.__esModule) { return obj; } else { var newObj = {}; if (obj != null) { for (var key in obj) { if (Object.prototype.hasOwnProperty.call(obj, key)) newObj[key] = obj[key]; } } newObj.default = obj; return newObj; } }\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nconst videoWidth = 500;\nconst videoHeight = 600;\nconst stats = new _stats2.default();\nvar currentPoses = [];\n\nfunction isAndroid() {\n  return (/Android/i.test(navigator.userAgent)\n  );\n}\n\nfunction isiOS() {\n  return (/iPhone|iPad|iPod/i.test(navigator.userAgent)\n  );\n}\n\nfunction isMobile() {\n  return isAndroid() || isiOS();\n}\n\n/**\n * Loads a the camera to be used in the demo\n * 先加载个镜头用\n */\nasync function setupCamera() {\n  if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {\n    throw new Error('Browser API navigator.mediaDevices.getUserMedia not available');\n  }\n\n  const video = document.getElementById('video');\n  video.width = videoWidth;\n  video.height = videoHeight;\n\n  const mobile = isMobile();\n  const stream = await navigator.mediaDevices.getUserMedia({\n    'audio': false,\n    'video': {\n      facingMode: 'user',\n      width: mobile ? undefined : videoWidth,\n      height: mobile ? undefined : videoHeight\n    }\n  });\n  video.srcObject = stream;\n\n  return new _promise2.default(resolve => {\n    video.onloadedmetadata = () => {\n      resolve(video);\n    };\n  });\n}\n\nasync function loadVideo() {\n  const video = await setupCamera();\n  video.play();\n\n  return video;\n}\n\nconst guiState = {\n  algorithm: 'single-pose',\n  input: {\n    mobileNetArchitecture: isMobile() ? '0.50' : '0.75',\n    outputStride: 16,\n    imageScaleFactor: 0.5\n  },\n  singlePoseDetection: {\n    minPoseConfidence: 0.1,\n    minPartConfidence: 0.5\n  },\n  multiPoseDetection: {\n    maxPoseDetections: 5,\n    minPoseConfidence: 0.15,\n    minPartConfidence: 0.1,\n    nmsRadius: 30.0\n  },\n  output: {\n    showVideo: true,\n    showSkeleton: true,\n    showPoints: true,\n    showBoundingBox: false\n  },\n  net: null\n};\n\n/**\n * Sets up dat.gui controller on the top-right of the window\n * 设置网页右上角的控制面板\n */\nfunction setupGui(cameras, net) {\n  guiState.net = net;\n\n  if (cameras.length > 0) {\n    guiState.camera = cameras[0].deviceId;\n  }\n\n  // const gui = new dat.GUI({ width: 300 });\n\n  // The single-pose algorithm is faster and simpler but requires only one\n  // person to be in the frame or results will be innaccurate. Multi-pose works\n  // for more than 1 person\n  // const algorithmController =\n  //   gui.add(guiState, 'algorithm', ['single-pose', 'multi-pose']);\n\n  // The input parameters have the most effect on accuracy and speed of the\n  // network\n  // let input = gui.addFolder('Input');\n  // Architecture: there are a few PoseNet models varying in size and\n  // accuracy. 1.01 is the largest, but will be the slowest. 0.50 is the\n  // fastest, but least accurate.\n  // const architectureController = input.add(\n  //   guiState.input, 'mobileNetArchitecture',\n  //   ['1.01', '1.00', '0.75', '0.50']);\n  // Output stride:  Internally, this parameter affects the height and width of\n  // the layers in the neural network. The lower the value of the output stride\n  // the higher the accuracy but slower the speed, the higher the value the\n  // faster the speed but lower the accuracy.\n  /*\n  input.add(guiState.input, 'outputStride', [8, 16, 32]);\n  // Image scale factor: What to scale the image by before feeding it through\n  // the network.\n  input.add(guiState.input, 'imageScaleFactor').min(0.2).max(1.0);\n  //input.open();\n   // Pose confidence: the overall confidence in the estimation of a person's\n  // pose (i.e. a person detected in a frame)\n  // Min part confidence: the confidence that a particular estimated keypoint\n  // position is accurate (i.e. the elbow's position)\n  let single = gui.addFolder('Single Pose Detection');\n  single.add(guiState.singlePoseDetection, 'minPoseConfidence', 0.0, 1.0);\n  single.add(guiState.singlePoseDetection, 'minPartConfidence', 0.0, 1.0);\n   let multi = gui.addFolder('Multi Pose Detection');\n  multi.add(guiState.multiPoseDetection, 'maxPoseDetections')\n    .min(1)\n    .max(20)\n    .step(1);\n  multi.add(guiState.multiPoseDetection, 'minPoseConfidence', 0.0, 1.0);\n  multi.add(guiState.multiPoseDetection, 'minPartConfidence', 0.0, 1.0);\n  // nms Radius: controls the minimum distance between poses that are returned\n  // defaults to 20, which is probably fine for most use cases\n  multi.add(guiState.multiPoseDetection, 'nmsRadius').min(0.0).max(40.0);\n  //multi.open();\n   let output = gui.addFolder('Output');\n  output.add(guiState.output, 'showVideo');\n  output.add(guiState.output, 'showSkeleton');\n  output.add(guiState.output, 'showPoints');\n  output.add(guiState.output, 'showBoundingBox');\n  //output.open();\n  */\n\n  // architectureController.onChange(function (architecture) {\n  //   guiState.changeToArchitecture = architecture;\n  // });\n  guiState.changeToArchitecture = 0.5;\n\n  // algorithmController.onChange(function (value) {\n  //   switch (guiState.algorithm) {\n  //     case 'single-pose':\n  //       multi.close();\n  //       single.open();\n  //       break;\n  //     case 'multi-pose':\n  //       single.close();\n  //       multi.open();\n  //       break;\n  //   }\n  // });\n}\n\n/**\n * Sets up a frames per second panel on the top-left of the window\n * 设置网页左上角的面板（竟然是不断输出图像？？？）\n */\nfunction setupFPS() {\n  stats.showPanel(0); // 0: fps, 1: ms, 2: mb, 3+: custom\n  document.body.appendChild(stats.dom);\n}\n\n/**\n * Feeds an image to posenet to estimate poses - this is where the magic\n * happens. This function loops with a requestAnimationFrame method.\n * 传输一张图像进行处理识别，魔法发生的地方\n * video相当于图片，可以对video进行处理\n */\nfunction detectPoseInRealTime(video, net) {\n  // for test ouput the video\n  // console.log(video)\n  const canvas2 = document.getElementById('input');\n  const canvas = document.getElementById('output');\n  const ctx = canvas.getContext('2d');\n  const ctx2 = canvas2.getContext('2d');\n  // since images are being fed from a webcam\n  const flipHorizontal = true;\n\n  canvas.width = videoWidth;\n  canvas.height = videoHeight;\n  canvas2.width = videoWidth;\n  canvas2.height = videoHeight;\n\n  async function poseDetectionFrame() {\n    if (guiState.changeToArchitecture) {\n      // Important to purge variables and free up GPU memory\n      guiState.net.dispose();\n\n      // Load the PoseNet model weights for either the 0.50, 0.75, 1.00, or 1.01\n      // version\n      guiState.net = await posenet.load(+guiState.changeToArchitecture);\n\n      guiState.changeToArchitecture = null;\n    }\n\n    // Begin monitoring code for frames per second\n    stats.begin();\n\n    // Scale an image down to a certain factor. Too large of an image will slow\n    // down the GPU\n    const imageScaleFactor = guiState.input.imageScaleFactor;\n    const outputStride = +guiState.input.outputStride;\n    /**\n     * process the image\n     */\n\n    let images = document.getElementById('demo_pic1');\n    const imagePose = await guiState.net.estimateSinglePose(images, imageScaleFactor, flipHorizontal, outputStride);\n    // console.log(imagePose);\n    let imageFormattedPoints = formatKeypoints(imagePose.keypoints);\n    let imageBoundingPoints = posenet.getBoundingBoxPoints(imagePose.keypoints);\n    let imageVector = PoseVector(imageFormattedPoints, imageBoundingPoints);\n\n    let poses = [];\n    let minPoseConfidence;\n    let minPartConfidence;\n    switch (guiState.algorithm) {\n      case 'single-pose':\n        // test the video\n        // console.log(video);\n        const pose = await guiState.net.estimateSinglePose(video, imageScaleFactor, flipHorizontal, outputStride);\n        if (pose.hasOwnProperty('score')) {\n          poses.push(pose);\n        } else {\n          poses = pose;\n        }\n        if (poses[0]) {\n          currentPoses = poses;\n          var temppose = poses[0];\n          var formatedPoints = formatKeypoints(temppose.keypoints); // 获得的是一个17维的数组\n          var boundingBoxPoint = posenet.getBoundingBoxPoints(temppose.keypoints); // 获得四个点的坐标boundingbox\n          let poseVector = PoseVector(formatedPoints, boundingBoxPoint);\n          // console.log(typeof(poseVector));\n          let testSimilar = weightedDistanceMatching(poseVector, imageVector);\n          // console.log(testSimilar);\n          // console.log(formatedPoints);\n          // console.log(boundingBoxPoint);\n        }\n        minPoseConfidence = +guiState.singlePoseDetection.minPoseConfidence;\n        minPartConfidence = +guiState.singlePoseDetection.minPartConfidence;\n        break;\n      case 'multi-pose':\n        poses = await guiState.net.estimateMultiplePoses(video, imageScaleFactor, flipHorizontal, outputStride, guiState.multiPoseDetection.maxPoseDetections, guiState.multiPoseDetection.minPartConfidence, guiState.multiPoseDetection.nmsRadius);\n\n        minPoseConfidence = +guiState.multiPoseDetection.minPoseConfidence;\n        minPartConfidence = +guiState.multiPoseDetection.minPartConfidence;\n        break;\n    }\n\n    ctx.clearRect(0, 0, videoWidth, videoHeight);\n    // ctx2.clearRect(0, 0, videoWidth, videoHeight);\n\n    if (guiState.output.showVideo) {\n      ctx.save();\n      ctx.scale(-1, 1);\n      ctx.translate(-videoWidth, 0);\n      ctx.drawImage(video, 0, 0, videoWidth, videoHeight);\n      ctx.restore();\n      // ctx2.save();\n      // ctx2.scale(-1, 1);\n      // ctx2.translate(-videoWidth, 0);\n      // ctx2.drawImage(images, 0, 0, videoWidth, videoHeight);\n      // ctx2.restore();\n    }\n\n    // For each pose (i.e. person) detected in an image, loop through the poses\n    // and draw the resulting skeleton and keypoints if over certain confidence\n    // scores\n    poses.forEach(({ score, keypoints }) => {\n      if (score >= minPoseConfidence) {\n        if (guiState.output.showPoints) {\n          (0, _demo_util.drawKeypoints)(keypoints, minPartConfidence, ctx);\n        }\n        if (guiState.output.showSkeleton) {\n          (0, _demo_util.drawSkeleton)(keypoints, minPartConfidence, ctx);\n        }\n        if (guiState.output.showBoundingBox) {\n          (0, _demo_util.drawBoundingBox)(keypoints, ctx);\n        }\n      }\n    });\n\n    // End monitoring code for frames per second\n    stats.end();\n\n    requestAnimationFrame(poseDetectionFrame);\n  }\n\n  poseDetectionFrame();\n}\n\n/**\n * Kicks off the demo by loading the posenet model, finding and loading\n * available camera devices, and setting off the detectPoseInRealTime function.\n * 加载模型，设置网页显示的内容\n */\nasync function bindPage() {\n  // Load the PoseNet model weights with architecture 0.75\n  const net = await posenet.load(0.75);\n\n  document.getElementById('loading').style.display = 'none';\n  document.getElementById('main').style.display = 'block';\n\n  let video;\n\n  try {\n    video = await loadVideo();\n  } catch (e) {\n    let info = document.getElementById('info');\n    info.textContent = 'this browser does not support video capture,' + 'or this device does not have a camera';\n    info.style.display = 'block';\n    throw e;\n  }\n  // Magic\n  setupGui([], net);\n  // setupFPS();\n  detectPoseInRealTime(video, net);\n}\n\nnavigator.getUserMedia = navigator.getUserMedia || navigator.webkitGetUserMedia || navigator.mozGetUserMedia;\n// kick off the demo\nbindPage();\n\nfunction formatKeypoints(keypoints) {\n  // sort keypoints by part name\n  keypoints.sort(function (a, b) {\n    var x = a.part.toLowerCase();\n    var y = b.part.toLowerCase();\n    return x < y ? -1 : x > y ? 1 : 0;\n  });\n\n  return keypoints;\n};\n\nfunction weightedDistanceMatching(poseVector1, poseVector2) {\n  typeof poseVector1;\n  // let vector1 = tf.tensor1d(poseVector1);\n  // let vector2 = tf.tensor1d(poseVector2);\n  let vector1PoseXY = poseVector1.slice(0, 34);\n  let vector1Confidences = poseVector1.slice(34, 51);\n  let vector1ConfidenceSum = poseVector1.slice(51, 52);\n  let vector2PoseXY = poseVector2.slice(0, 34);\n\n  // console.log(vector1PoseXY);\n  // First summation\n  let summation1 = 1 / vector1ConfidenceSum;\n\n  // Second summation\n  let summation2 = 0;\n  for (let i = 0; i < vector1PoseXY.length; i++) {\n    let tempConf = Math.floor(i / 2);\n    let tempSum = vector1Confidences[tempConf] * Math.abs(vector1PoseXY[i] - vector2PoseXY[i]);\n    summation2 = summation2 + tempSum;\n  }\n  console.log(summation1 * summation2);\n  return summation1 * summation2;\n}\n\nfunction PoseVector(keypoints, boudingBoxpoints) {\n  let boxWidth = boudingBoxpoints[1].x - boudingBoxpoints[0].x;\n  let boxHeight = boudingBoxpoints[3].y - boudingBoxpoints[0].y;\n  let x0 = boudingBoxpoints[0].x;\n  let y0 = boudingBoxpoints[0].y;\n  var vectorTemp = [];\n  // console.log(x0);\n  for (let i = 0; i < keypoints.length; i++) {\n    const { x, y } = keypoints[i].position;\n\n    let x1 = (x - x0) * 400 / boxWidth;\n    // console.log(x1);\n    vectorTemp.push(x1);\n    let y1 = (y - y0) * 400 / boxHeight;\n    vectorTemp.push(y1);\n  }\n\n  // let PoseVector = tf.Math.l2_normalize(vectorTemp, dim = 0);\n  // let PoseVector = tf.nn.l2_normalize(vectorTemp, dim = 0);\n  // console.log(vectorTemp);\n  let PoseVector = l2_normalize(vectorTemp);\n  var sum = 0;\n  for (let j = 0; j < keypoints.length; j++) {\n    sum += keypoints[j].score;\n    PoseVector.push(keypoints[j].score);\n  }\n  PoseVector.push(sum);\n  // console.log(PoseVector);\n  return PoseVector;\n}\n\nfunction l2_normalize(vectorList, dim) {\n  for (let i = 0; i < vectorList.length; i++) {\n    vectorList[i] = vectorList[i] * vectorList[i];\n  }\n  const data = tf.tensor1d(vectorList);\n  const sum = tf.variable(tf.sum(data).toFloat()).get();\n  // console.log(sum.get());\n  // sum.print();\n  for (let i = 0; i < vectorList.length; i++) {\n    vectorList[i] = tf.variable(tf.sqrt(vectorList[i] / sum)).get();\n  }\n  // console.log(vectorList);\n  return vectorList;\n}"},"hash":"42f1dccf3fe1e517019ebf3e5a94fa87","cacheData":{"env":{}}}